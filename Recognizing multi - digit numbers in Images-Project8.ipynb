{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Street View House Numbers, Recognizing multi-digit numbers in Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: KeysView(<HDF5 file \"SVHN_single_grey1.h5\" (mode r)>)\n",
      "Keys: X_test\n",
      "Keys: X_train\n",
      "Keys: X_val\n",
      "Keys: y_test\n",
      "Keys: y_train\n",
      "Keys: y_val\n"
     ]
    }
   ],
   "source": [
    "#Fetching the dataset from the file\n",
    "\n",
    "with h5py.File('SVHN_single_grey1.h5', 'r') as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    print(\"Keys: %s\" % list(f.keys())[0])\n",
    "    print(\"Keys: %s\" % list(f.keys())[1])\n",
    "    print(\"Keys: %s\" % list(f.keys())[2])\n",
    "    print(\"Keys: %s\" % list(f.keys())[3])\n",
    "    print(\"Keys: %s\" % list(f.keys())[4])\n",
    "    print(\"Keys: %s\" % list(f.keys())[5])\n",
    "    X_test = list(f.keys())[0]\n",
    "    X_train = list(f.keys())[1]\n",
    "    X_val = list(f.keys())[2]\n",
    "    y_test = list(f.keys())[3]\n",
    "    y_train = list(f.keys())[4]\n",
    "    y_val = list(f.keys())[5]\n",
    "\n",
    "    # Get the data\n",
    "    X_test = list(f[X_test])\n",
    "    X_train = list(f[X_train])\n",
    "    X_val = list(f[X_val])\n",
    "    y_test = list(f[y_test])\n",
    "    y_train = list(f[y_train])\n",
    "    y_val = list(f[y_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DataSet is already devided into 3 subsets\n",
    "    - Training set - This dataset is usually used to Train the model\n",
    "    - Validation Set - This dataset is usually used to run our trained model , and then improve the performance by tuning hyperparameters\n",
    "    - Test Set - This is pure Test dataset, final model should be run on this to predict the result and performance of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of X_train is 42000\n",
      "size of y_train is 42000\n",
      "size of X_val is 60000\n",
      "size of y_val is 60000\n",
      "size of X_test is 18000\n",
      "size of y_test is 18000\n"
     ]
    }
   ],
   "source": [
    "#we can see that the dataset has the split values already, lets check the size of each\n",
    "\n",
    "print('size of X_train is %d'%(len(X_train)))\n",
    "print('size of y_train is %d'%(len(y_train)))\n",
    "\n",
    "print('size of X_val is %d'%(len(X_val)))\n",
    "print('size of y_val is %d'%(len(y_val)))\n",
    "\n",
    "print('size of X_test is %d'%(len(X_test)))\n",
    "print('size of y_test is %d'%(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check One of the training value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 33.0704,  30.2601,  26.852 , ...,  71.4471,  58.2204,  42.9939],\n",
       "       [ 25.2283,  25.5533,  29.9765, ..., 113.0209, 103.3639,  84.2949],\n",
       "       [ 26.2775,  22.6137,  40.4763, ..., 113.3028, 121.775 , 115.4228],\n",
       "       ...,\n",
       "       [ 28.5502,  36.212 ,  45.0801, ...,  24.1359,  25.0927,  26.0603],\n",
       "       [ 38.4352,  26.4733,  23.2717, ...,  28.1094,  29.4683,  30.0661],\n",
       "       [ 50.2984,  26.0773,  24.0389, ...,  49.6682,  50.853 ,  53.0377]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape\n",
    "# We can see that every element is n * 32 * 32 metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets convert the list variables to Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "X_val=np.array(X_val)\n",
    "y_val=np.array(y_val)\n",
    "X_test=np.array(X_test)\n",
    "y_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets plot one of the data to understand how the image looks and what is its corresponding correct classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "i=600\n",
    "\n",
    "plt.imshow(X_train[i,:],cmap = matplotlib.cm.binary)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(y_train[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 32, 32) (60000, 32, 32) (18000, 32, 32) (42000,) (60000,) (18000,)\n"
     ]
    }
   ],
   "source": [
    "# Lets print the shape of each dataset\n",
    "\n",
    "print(X_train.shape, X_val.shape ,X_test.shape , y_train.shape, y_val.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have 3-D data and models wont accept it we need to conver it to 2-D\n",
    " \n",
    "#Reshaping the data ( n * 32 * 32) to (n * 1024)\n",
    "\n",
    "X_train_new = X_train.reshape((X_train.shape[0],-1))\n",
    "X_val_new = X_val.reshape((X_val.shape[0],-1))\n",
    "X_test_new = X_test.reshape((X_test.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 1024) (60000, 1024) (18000, 1024) (42000,) (60000,) (18000,)\n"
     ]
    }
   ],
   "source": [
    "# Lets print the shape now\n",
    "\n",
    "print(X_train_new.shape, X_val_new.shape ,X_test_new.shape , y_train.shape, y_val.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'encoder = OneHotEncoder()\\n\\ny_train_new = encoder.fit_transform(y_train_new).toarray()\\ny_val_new = encoder.fit_transform(y_val_new).toarray()\\ny_test_new = encoder.fit_transform(y_test_new).toarray()'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Currently the target variables hold the classification value like values ranging from 0 to 9, \n",
    "#we have to convert them to binary values \n",
    "# example if value is 2 then we have to conver it to 0 0 1 0 0 0 0 0 0 0\n",
    "# We can either use OneHotEncoding technique or Keras builtin to_categorical \n",
    "\n",
    "y_train_new = to_categorical(y_train)\n",
    "y_val_new = to_categorical(y_val)\n",
    "y_test_new = to_categorical(y_test)\n",
    "\n",
    "#we can also use below technique\n",
    "\n",
    "'''encoder = OneHotEncoder()\n",
    "\n",
    "y_train_new = encoder.fit_transform(y_train_new).toarray()\n",
    "y_val_new = encoder.fit_transform(y_val_new).toarray()\n",
    "y_test_new = encoder.fit_transform(y_test_new).toarray()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 1024) (60000, 1024) (18000, 1024) (42000, 10) (60000, 10) (18000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Lets print the shape now\n",
    "\n",
    "print(X_train_new.shape, X_val_new.shape ,X_test_new.shape , y_train_new.shape, y_val_new.shape,y_test_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_new[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize the train ,validate and test data\n",
    "This is important as all values spread in same range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train_new = sc.fit_transform(X_train_new)\n",
    "X_val_new = sc.fit_transform(X_val_new)\n",
    "X_test_new = sc.fit_transform(X_test_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor(KNN) classifier Apporach\n",
    "##### Lets build the model using KNN classifier and see how the model behaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=2)\n",
    "\n",
    "#Fit the model\n",
    "knn_model.fit(X_train_new,y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = knn_model.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN has been executed from googleColab and result of it is attached separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Technique ( Neural Network Classifier)\n",
    "##### Lets build the model using keras framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.framework.ops.reset_default_graph>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reset Default graph - Needed as we build the graph at every stage and may cause issue when run in sequence/repeated\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create method to evaluate the model on Test data set\n",
    "\n",
    "def evaluateModel(model):\n",
    "    print('\\n# Evaluate on test data')\n",
    "    results = model.evaluate(X_test_new, y_test_new, batch_size=500)\n",
    "    print('test acc:', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few points to be noted before we build the Neural Network\n",
    "- We have 3 set of data, training data, validate data and test data\n",
    "- we have classification scenario with target value ranging from 0 to 9\n",
    "- Lets start building Neural Network with only 1 hidden node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Neural Network -1\n",
    "#### Lets start building Neural Network with only 1 hidden node\n",
    "#### use Activation funtion 'relu'\n",
    "#### kernelinitializer 'uniform'\n",
    "#### Optimizer 'adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets Initialize & build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the Neural Network\n",
    "model1_keras = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We have input feature of 1024 and output 10 categorical values , hence input shape will be 1024 and output node will be 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create input layer\n",
    "model1_keras.add(Dense(units=512,kernel_initializer='uniform',input_shape=(1024,)))\n",
    "#Add activation\n",
    "model1_keras.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding 1st hidden layer\n",
    "model1_keras.add(Dense(200, kernel_initializer='uniform'))\n",
    "#Add activation\n",
    "model1_keras.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "# we have an output of 10 node, which is the the desired dimensions of our output\n",
    "model1_keras.add(Dense(10, kernel_initializer='uniform')) \n",
    "#Add activation\n",
    "# We use the softmax because we have multiclass classification\n",
    "model1_keras.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now compile the network\n",
    "#Using loss function as categorical_crossentropy as we have multiclass classification\n",
    "model1_keras.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/20\n",
      "42000/42000 [==============================] - 20s 464us/step - loss: 1.1821 - accuracy: 0.6202 - val_loss: 0.9281 - val_accuracy: 0.7076\n",
      "Epoch 2/20\n",
      "42000/42000 [==============================] - 18s 418us/step - loss: 0.7911 - accuracy: 0.7568 - val_loss: 0.7257 - val_accuracy: 0.7778\n",
      "Epoch 3/20\n",
      "42000/42000 [==============================] - 18s 420us/step - loss: 0.6774 - accuracy: 0.7904 - val_loss: 0.6129 - val_accuracy: 0.8130\n",
      "Epoch 4/20\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.6027 - accuracy: 0.8149 - val_loss: 0.5681 - val_accuracy: 0.8295\n",
      "Epoch 5/20\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.5559 - accuracy: 0.8277 - val_loss: 0.5994 - val_accuracy: 0.8173\n",
      "Epoch 6/20\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.5134 - accuracy: 0.8409 - val_loss: 0.5711 - val_accuracy: 0.8236\n",
      "Epoch 7/20\n",
      "42000/42000 [==============================] - 18s 438us/step - loss: 0.4899 - accuracy: 0.8450 - val_loss: 0.5227 - val_accuracy: 0.8441\n",
      "Epoch 8/20\n",
      "42000/42000 [==============================] - 19s 442us/step - loss: 0.4639 - accuracy: 0.8545 - val_loss: 0.4633 - val_accuracy: 0.8627\n",
      "Epoch 9/20\n",
      "42000/42000 [==============================] - 20s 465us/step - loss: 0.4307 - accuracy: 0.8654 - val_loss: 0.4701 - val_accuracy: 0.8597\n",
      "Epoch 10/20\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 0.4065 - accuracy: 0.8713 - val_loss: 0.4677 - val_accuracy: 0.8599\n",
      "Epoch 11/20\n",
      "42000/42000 [==============================] - 23s 536us/step - loss: 0.4009 - accuracy: 0.8726 - val_loss: 0.4375 - val_accuracy: 0.8713\n",
      "Epoch 12/20\n",
      "42000/42000 [==============================] - 22s 530us/step - loss: 0.3878 - accuracy: 0.8784 - val_loss: 0.4535 - val_accuracy: 0.8715\n",
      "Epoch 13/20\n",
      "42000/42000 [==============================] - 21s 496us/step - loss: 0.3588 - accuracy: 0.8850 - val_loss: 0.4521 - val_accuracy: 0.8734\n",
      "Epoch 14/20\n",
      "42000/42000 [==============================] - 23s 538us/step - loss: 0.3492 - accuracy: 0.8883 - val_loss: 0.4760 - val_accuracy: 0.8682\n",
      "Epoch 15/20\n",
      "42000/42000 [==============================] - 22s 528us/step - loss: 0.3389 - accuracy: 0.8921 - val_loss: 0.4120 - val_accuracy: 0.8906\n",
      "Epoch 16/20\n",
      "42000/42000 [==============================] - 21s 492us/step - loss: 0.3268 - accuracy: 0.8946 - val_loss: 0.4527 - val_accuracy: 0.8737\n",
      "Epoch 17/20\n",
      "42000/42000 [==============================] - 21s 499us/step - loss: 0.3108 - accuracy: 0.9001 - val_loss: 0.4415 - val_accuracy: 0.8826\n",
      "Epoch 18/20\n",
      "42000/42000 [==============================] - 21s 496us/step - loss: 0.3001 - accuracy: 0.9032 - val_loss: 0.4442 - val_accuracy: 0.8819\n",
      "Epoch 19/20\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 0.2953 - accuracy: 0.9047 - val_loss: 0.4064 - val_accuracy: 0.8949\n",
      "Epoch 20/20\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.2820 - accuracy: 0.9100 - val_loss: 0.4465 - val_accuracy: 0.8866\n"
     ]
    }
   ],
   "source": [
    "# now fit the model\n",
    "model1_keras_result = model1_keras.fit(X_train_new, y_train_new,           \n",
    "          validation_data=(X_val_new,y_val_new),\n",
    "          epochs=20,batch_size=50,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to visualize training and validation acccuracy\n",
    "\n",
    "def plotModelTrainingValidationLossAccuracy(model_result):\n",
    "    plt.plot(model_result.history['accuracy'])\n",
    "    plt.plot(model_result.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(model_result.history['loss'])\n",
    "    plt.plot(model_result.history['val_loss']) \n",
    "    plt.title('Model loss') \n",
    "    plt.ylabel('Loss') \n",
    "    plt.xlabel('Epoch') \n",
    "    plt.legend(['Train', 'Val'], loc='upper left') \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotModelTrainingValidationLossAccuracy(model1_keras_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that Training Accuracy is 91% and Validation Accuracy is 88%, We can tune the model to improve the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Neural Network -2\n",
    "#### Lets add 2 hidden node\n",
    "#### use Activation funtion 'relu'\n",
    "#### kernelinitializer 'uniform'\n",
    "#### Optimizer 'adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the Neural Network\n",
    "model2_keras = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/20\n",
      "42000/42000 [==============================] - 20s 482us/step - loss: 1.2153 - accuracy: 0.5970 - val_loss: 0.9040 - val_accuracy: 0.7145\n",
      "Epoch 2/20\n",
      "42000/42000 [==============================] - 24s 579us/step - loss: 0.8079 - accuracy: 0.7474 - val_loss: 0.7307 - val_accuracy: 0.7724\n",
      "Epoch 3/20\n",
      "42000/42000 [==============================] - 25s 588us/step - loss: 0.6899 - accuracy: 0.7837 - val_loss: 0.6469 - val_accuracy: 0.8005\n",
      "Epoch 4/20\n",
      "42000/42000 [==============================] - 21s 490us/step - loss: 0.6166 - accuracy: 0.8073 - val_loss: 0.5945 - val_accuracy: 0.8150\n",
      "Epoch 5/20\n",
      "42000/42000 [==============================] - 20s 466us/step - loss: 0.5593 - accuracy: 0.8245 - val_loss: 0.5463 - val_accuracy: 0.8300\n",
      "Epoch 6/20\n",
      "42000/42000 [==============================] - 22s 517us/step - loss: 0.5212 - accuracy: 0.8359 - val_loss: 0.5158 - val_accuracy: 0.8421\n",
      "Epoch 7/20\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 0.4876 - accuracy: 0.8460 - val_loss: 0.5017 - val_accuracy: 0.8475\n",
      "Epoch 8/20\n",
      "42000/42000 [==============================] - 23s 541us/step - loss: 0.4553 - accuracy: 0.8552 - val_loss: 0.4664 - val_accuracy: 0.8580\n",
      "Epoch 9/20\n",
      "42000/42000 [==============================] - 20s 487us/step - loss: 0.4328 - accuracy: 0.8627 - val_loss: 0.5565 - val_accuracy: 0.8345\n",
      "Epoch 10/20\n",
      "42000/42000 [==============================] - 23s 556us/step - loss: 0.4065 - accuracy: 0.8695 - val_loss: 0.4499 - val_accuracy: 0.8669\n",
      "Epoch 11/20\n",
      "42000/42000 [==============================] - 23s 537us/step - loss: 0.3883 - accuracy: 0.8762 - val_loss: 0.4433 - val_accuracy: 0.8692\n",
      "Epoch 12/20\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.3704 - accuracy: 0.8807 - val_loss: 0.4495 - val_accuracy: 0.8682\n",
      "Epoch 13/20\n",
      "42000/42000 [==============================] - 19s 443us/step - loss: 0.3577 - accuracy: 0.8853 - val_loss: 0.4472 - val_accuracy: 0.8724\n",
      "Epoch 14/20\n",
      "42000/42000 [==============================] - 20s 473us/step - loss: 0.3358 - accuracy: 0.8916 - val_loss: 0.4476 - val_accuracy: 0.8722\n",
      "Epoch 15/20\n",
      "42000/42000 [==============================] - 22s 520us/step - loss: 0.3264 - accuracy: 0.8925 - val_loss: 0.4075 - val_accuracy: 0.8881\n",
      "Epoch 16/20\n",
      "42000/42000 [==============================] - 21s 493us/step - loss: 0.3095 - accuracy: 0.8998 - val_loss: 0.4582 - val_accuracy: 0.8781\n",
      "Epoch 17/20\n",
      "42000/42000 [==============================] - 22s 522us/step - loss: 0.3004 - accuracy: 0.9026 - val_loss: 0.4741 - val_accuracy: 0.8751\n",
      "Epoch 18/20\n",
      "42000/42000 [==============================] - 21s 508us/step - loss: 0.2984 - accuracy: 0.9041 - val_loss: 0.4136 - val_accuracy: 0.8897\n",
      "Epoch 19/20\n",
      "42000/42000 [==============================] - 18s 433us/step - loss: 0.2774 - accuracy: 0.9096 - val_loss: 0.4222 - val_accuracy: 0.8877\n",
      "Epoch 20/20\n",
      "42000/42000 [==============================] - 21s 506us/step - loss: 0.2772 - accuracy: 0.9114 - val_loss: 0.4318 - val_accuracy: 0.8905\n"
     ]
    }
   ],
   "source": [
    "#Lets create input layer\n",
    "model2_keras.add(Dense(units=512,kernel_initializer='uniform',input_shape=(1024,)))\n",
    "#Add activation\n",
    "model2_keras.add(Activation('relu'))\n",
    "\n",
    "#Adding 1st hidden layer\n",
    "model2_keras.add(Dense(256, kernel_initializer='uniform'))\n",
    "#Add activation\n",
    "model2_keras.add(Activation('relu'))\n",
    "\n",
    "#Adding 2nd hidden layer\n",
    "model2_keras.add(Dense(256, kernel_initializer='uniform'))\n",
    "#Add activation\n",
    "model2_keras.add(Activation('relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "# we have an output of 10 node, which is the the desired dimensions of our output\n",
    "model2_keras.add(Dense(10, kernel_initializer='uniform')) \n",
    "#Add activation\n",
    "# We use the softmax because we have multiclass classification\n",
    "model2_keras.add(Activation('softmax'))\n",
    "\n",
    "# now compile the network\n",
    "model2_keras.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# now fit the model\n",
    "model2_keras_result=model2_keras.fit(X_train_new, y_train_new,           \n",
    "          validation_data=(X_val_new,y_val_new),\n",
    "          epochs=20,batch_size=50,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotModelTrainingValidationLossAccuracy(model2_keras_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that Training Accuracy is 91% and Validation Accuracy is 89%, We can tune the model to improve the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Neural Network -3\n",
    "#### Lets add 2 hidden node\n",
    "#### use Activation funtion 'relu'\n",
    "#### kernelinitializer 'uniform'\n",
    "#### Optimizer 'adam', with learning rate 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the Neural Network\n",
    "model3_keras = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/20\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 2.3354 - accuracy: 0.1070 - val_loss: 2.2867 - val_accuracy: 0.1083\n",
      "Epoch 2/20\n",
      "42000/42000 [==============================] - 21s 494us/step - loss: 2.2999 - accuracy: 0.1050 - val_loss: 2.3075 - val_accuracy: 0.1024\n",
      "Epoch 3/20\n",
      "42000/42000 [==============================] - 21s 497us/step - loss: 2.3341 - accuracy: 0.1015 - val_loss: 2.3039 - val_accuracy: 0.1000\n",
      "Epoch 4/20\n",
      "42000/42000 [==============================] - 19s 449us/step - loss: 2.3037 - accuracy: 0.0980 - val_loss: 2.3047 - val_accuracy: 0.1000\n",
      "Epoch 5/20\n",
      "42000/42000 [==============================] - 22s 522us/step - loss: 2.3038 - accuracy: 0.0978 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 6/20\n",
      "42000/42000 [==============================] - 24s 564us/step - loss: 2.3037 - accuracy: 0.1019 - val_loss: 2.3039 - val_accuracy: 0.1000\n",
      "Epoch 7/20\n",
      "42000/42000 [==============================] - 20s 466us/step - loss: 2.3036 - accuracy: 0.1005 - val_loss: 2.3035 - val_accuracy: 0.1000\n",
      "Epoch 8/20\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 2.3037 - accuracy: 0.1013 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 9/20\n",
      "42000/42000 [==============================] - 21s 489us/step - loss: 2.3037 - accuracy: 0.0999 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "Epoch 10/20\n",
      "42000/42000 [==============================] - 21s 489us/step - loss: 2.3037 - accuracy: 0.0993 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "42000/42000 [==============================] - 19s 455us/step - loss: 2.3038 - accuracy: 0.0995 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 12/20\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 2.3037 - accuracy: 0.1000 - val_loss: 2.3034 - val_accuracy: 0.1000\n",
      "Epoch 13/20\n",
      "42000/42000 [==============================] - 23s 558us/step - loss: 2.3039 - accuracy: 0.0974 - val_loss: 2.3029 - val_accuracy: 0.1000\n",
      "Epoch 14/20\n",
      "42000/42000 [==============================] - 24s 564us/step - loss: 2.3038 - accuracy: 0.0989 - val_loss: 2.3033 - val_accuracy: 0.1000\n",
      "Epoch 15/20\n",
      "42000/42000 [==============================] - 21s 497us/step - loss: 2.3037 - accuracy: 0.1011 - val_loss: 2.3036 - val_accuracy: 0.1000\n",
      "Epoch 16/20\n",
      "42000/42000 [==============================] - 20s 487us/step - loss: 2.3037 - accuracy: 0.0985 - val_loss: 2.3031 - val_accuracy: 0.1000\n",
      "Epoch 17/20\n",
      "42000/42000 [==============================] - 20s 478us/step - loss: 2.3038 - accuracy: 0.0997 - val_loss: 2.3033 - val_accuracy: 0.1000\n",
      "Epoch 18/20\n",
      "42000/42000 [==============================] - 17s 396us/step - loss: 2.3038 - accuracy: 0.0987 - val_loss: 2.3028 - val_accuracy: 0.1000\n",
      "Epoch 19/20\n",
      "42000/42000 [==============================] - 17s 398us/step - loss: 2.3036 - accuracy: 0.0996 - val_loss: 2.3032 - val_accuracy: 0.1000\n",
      "Epoch 20/20\n",
      "42000/42000 [==============================] - 17s 397us/step - loss: 2.3037 - accuracy: 0.0998 - val_loss: 2.3035 - val_accuracy: 0.1000\n"
     ]
    }
   ],
   "source": [
    "#Lets create input layer\n",
    "model3_keras.add(Dense(units=512,kernel_initializer='uniform',input_shape=(1024,)))\n",
    "#Add activation\n",
    "model3_keras.add(Activation('relu'))\n",
    "\n",
    "#Adding 1st hidden layer\n",
    "model3_keras.add(Dense(256, kernel_initializer='uniform'))\n",
    "#Add activation\n",
    "model3_keras.add(Activation('relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "# we have an output of 10 node, which is the the desired dimensions of our output\n",
    "model3_keras.add(Dense(10, kernel_initializer='uniform')) \n",
    "#Add activation\n",
    "# We use the softmax because we have multiclass classification\n",
    "model3_keras.add(Activation('softmax'))\n",
    "\n",
    "# Lets create adam optimizer \n",
    "adam = optimizers.adam(learning_rate=0.01)\n",
    "\n",
    "# now compile the network\n",
    "model3_keras.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# now fit the model\n",
    "model3_keras_result=model3_keras.fit(X_train_new, y_train_new,           \n",
    "          validation_data=(X_val_new,y_val_new),\n",
    "          epochs=20,batch_size=50,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotModelTrainingValidationLossAccuracy(model3_keras_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that with learning rate 0.01 model performance is very poor .Training Accuracy is 10% and Validation Accuracy is 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network -4\n",
    "#### Lets add 2 hidden node\n",
    "#### use Activation funtion 'relu'\n",
    "#### kernelinitializer 'uniform'\n",
    "#### Optimizer 'SGD' with learning rate 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the Neural Network\n",
    "model4_keras = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/20\n",
      "42000/42000 [==============================] - 13s 299us/step - loss: 2.2134 - accuracy: 0.2517 - val_loss: 2.0176 - val_accuracy: 0.4147\n",
      "Epoch 2/20\n",
      "42000/42000 [==============================] - 12s 282us/step - loss: 1.6493 - accuracy: 0.5269 - val_loss: 1.3362 - val_accuracy: 0.6195\n",
      "Epoch 3/20\n",
      "42000/42000 [==============================] - 11s 265us/step - loss: 1.1766 - accuracy: 0.6610 - val_loss: 1.0377 - val_accuracy: 0.6993\n",
      "Epoch 4/20\n",
      "42000/42000 [==============================] - 11s 266us/step - loss: 0.9629 - accuracy: 0.7204 - val_loss: 0.8923 - val_accuracy: 0.7367\n",
      "Epoch 5/20\n",
      "42000/42000 [==============================] - 11s 268us/step - loss: 0.8360 - accuracy: 0.7566 - val_loss: 0.7796 - val_accuracy: 0.7735\n",
      "Epoch 6/20\n",
      "42000/42000 [==============================] - 11s 268us/step - loss: 0.7509 - accuracy: 0.7810 - val_loss: 0.7307 - val_accuracy: 0.7819\n",
      "Epoch 7/20\n",
      "42000/42000 [==============================] - 11s 267us/step - loss: 0.6851 - accuracy: 0.7993 - val_loss: 0.6696 - val_accuracy: 0.8041\n",
      "Epoch 8/20\n",
      "42000/42000 [==============================] - 11s 262us/step - loss: 0.6349 - accuracy: 0.8139 - val_loss: 0.6366 - val_accuracy: 0.8134\n",
      "Epoch 9/20\n",
      "42000/42000 [==============================] - 11s 264us/step - loss: 0.5955 - accuracy: 0.8262 - val_loss: 0.5949 - val_accuracy: 0.8259\n",
      "Epoch 10/20\n",
      "42000/42000 [==============================] - 13s 317us/step - loss: 0.5603 - accuracy: 0.8359 - val_loss: 0.5513 - val_accuracy: 0.8428\n",
      "Epoch 11/20\n",
      "42000/42000 [==============================] - 15s 354us/step - loss: 0.5331 - accuracy: 0.8430 - val_loss: 0.5391 - val_accuracy: 0.8451\n",
      "Epoch 12/20\n",
      "42000/42000 [==============================] - 15s 362us/step - loss: 0.5085 - accuracy: 0.8506 - val_loss: 0.5261 - val_accuracy: 0.8455\n",
      "Epoch 13/20\n",
      "42000/42000 [==============================] - 15s 353us/step - loss: 0.4865 - accuracy: 0.8567 - val_loss: 0.5278 - val_accuracy: 0.8451\n",
      "Epoch 14/20\n",
      "42000/42000 [==============================] - 15s 353us/step - loss: 0.4651 - accuracy: 0.8632 - val_loss: 0.4714 - val_accuracy: 0.8654\n",
      "Epoch 15/20\n",
      "42000/42000 [==============================] - 16s 383us/step - loss: 0.4462 - accuracy: 0.8692 - val_loss: 0.4743 - val_accuracy: 0.8629\n",
      "Epoch 16/20\n",
      "42000/42000 [==============================] - 17s 404us/step - loss: 0.4307 - accuracy: 0.8737 - val_loss: 0.4394 - val_accuracy: 0.8772\n",
      "Epoch 17/20\n",
      "42000/42000 [==============================] - 16s 370us/step - loss: 0.4179 - accuracy: 0.8762 - val_loss: 0.4467 - val_accuracy: 0.8725\n",
      "Epoch 18/20\n",
      "42000/42000 [==============================] - 15s 365us/step - loss: 0.4014 - accuracy: 0.8827 - val_loss: 0.4253 - val_accuracy: 0.8801\n",
      "Epoch 19/20\n",
      "42000/42000 [==============================] - 16s 378us/step - loss: 0.3866 - accuracy: 0.8845 - val_loss: 0.4342 - val_accuracy: 0.8756\n",
      "Epoch 20/20\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 0.3721 - accuracy: 0.8905 - val_loss: 0.4541 - val_accuracy: 0.8710\n"
     ]
    }
   ],
   "source": [
    "#Lets create input layer\n",
    "model4_keras.add(Dense(units=512,kernel_initializer='uniform',input_shape=(1024,)))\n",
    "#Add activation\n",
    "model4_keras.add(Activation('relu'))\n",
    "\n",
    "#Adding 1st hidden layer\n",
    "model4_keras.add(Dense(256, kernel_initializer='uniform'))\n",
    "#Add activation\n",
    "model4_keras.add(Activation('relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "# we have an output of 10 node, which is the the desired dimensions of our output\n",
    "model4_keras.add(Dense(10, kernel_initializer='uniform')) \n",
    "#Add activation\n",
    "# We use the softmax because we have multiclass classification\n",
    "model4_keras.add(Activation('softmax'))\n",
    "\n",
    "# Lets create adam optimizer \n",
    "sgd = optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "# now compile the network\n",
    "model4_keras.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# now fit the model\n",
    "model4_keras_result=model4_keras.fit(X_train_new, y_train_new,           \n",
    "          validation_data=(X_val_new,y_val_new),\n",
    "          epochs=20,batch_size=50,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that with learning rate 0.01 model performance is not great .Training Accuracy is 89% and Validation Accuracy is 87%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network -5\n",
    "#### Lets add 2 hidden node\n",
    "#### use Activation funtion 'relu'\n",
    "#### kernelinitializer 'uniform'\n",
    "#### Optimizer 'SGD' with learning rate 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the Neural Network\n",
    "model5_keras = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/20\n",
      "42000/42000 [==============================] - 17s 415us/step - loss: 2.2995 - accuracy: 0.1110 - val_loss: 2.2909 - val_accuracy: 0.1367\n",
      "Epoch 2/20\n",
      "42000/42000 [==============================] - 19s 447us/step - loss: 2.2844 - accuracy: 0.1587 - val_loss: 2.2772 - val_accuracy: 0.1854\n",
      "Epoch 3/20\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 2.2702 - accuracy: 0.2056 - val_loss: 2.2623 - val_accuracy: 0.2271\n",
      "Epoch 4/20\n",
      "42000/42000 [==============================] - 16s 372us/step - loss: 2.2543 - accuracy: 0.2428 - val_loss: 2.2451 - val_accuracy: 0.2680\n",
      "Epoch 5/20\n",
      "42000/42000 [==============================] - 14s 338us/step - loss: 2.2352 - accuracy: 0.2822 - val_loss: 2.2239 - val_accuracy: 0.2979\n",
      "Epoch 6/20\n",
      "42000/42000 [==============================] - 16s 371us/step - loss: 2.2115 - accuracy: 0.3103 - val_loss: 2.1975 - val_accuracy: 0.3302\n",
      "Epoch 7/20\n",
      "42000/42000 [==============================] - 16s 374us/step - loss: 2.1815 - accuracy: 0.3421 - val_loss: 2.1635 - val_accuracy: 0.3521\n",
      "Epoch 8/20\n",
      "42000/42000 [==============================] - 15s 349us/step - loss: 2.1429 - accuracy: 0.3656 - val_loss: 2.1198 - val_accuracy: 0.3766\n",
      "Epoch 9/20\n",
      "42000/42000 [==============================] - 15s 359us/step - loss: 2.0935 - accuracy: 0.3916 - val_loss: 2.0642 - val_accuracy: 0.4002\n",
      "Epoch 10/20\n",
      "42000/42000 [==============================] - 16s 392us/step - loss: 2.0317 - accuracy: 0.4165 - val_loss: 1.9959 - val_accuracy: 0.4320\n",
      "Epoch 11/20\n",
      "42000/42000 [==============================] - 17s 416us/step - loss: 1.9579 - accuracy: 0.4451 - val_loss: 1.9167 - val_accuracy: 0.4629\n",
      "Epoch 12/20\n",
      "42000/42000 [==============================] - 16s 378us/step - loss: 1.8751 - accuracy: 0.4783 - val_loss: 1.8310 - val_accuracy: 0.4944\n",
      "Epoch 13/20\n",
      "42000/42000 [==============================] - 15s 361us/step - loss: 1.7888 - accuracy: 0.5075 - val_loss: 1.7444 - val_accuracy: 0.5227\n",
      "Epoch 14/20\n",
      "42000/42000 [==============================] - 16s 384us/step - loss: 1.7039 - accuracy: 0.5360 - val_loss: 1.6614 - val_accuracy: 0.5463\n",
      "Epoch 15/20\n",
      "42000/42000 [==============================] - 15s 355us/step - loss: 1.6241 - accuracy: 0.5564 - val_loss: 1.5847 - val_accuracy: 0.5674\n",
      "Epoch 16/20\n",
      "42000/42000 [==============================] - 16s 380us/step - loss: 1.5511 - accuracy: 0.5770 - val_loss: 1.5159 - val_accuracy: 0.5808\n",
      "Epoch 17/20\n",
      "42000/42000 [==============================] - 16s 377us/step - loss: 1.4851 - accuracy: 0.5926 - val_loss: 1.4526 - val_accuracy: 0.6008\n",
      "Epoch 18/20\n",
      "42000/42000 [==============================] - 16s 385us/step - loss: 1.4260 - accuracy: 0.6085 - val_loss: 1.3964 - val_accuracy: 0.6154\n",
      "Epoch 19/20\n",
      "42000/42000 [==============================] - 16s 382us/step - loss: 1.3725 - accuracy: 0.6216 - val_loss: 1.3460 - val_accuracy: 0.6287\n",
      "Epoch 20/20\n",
      "42000/42000 [==============================] - 16s 376us/step - loss: 1.3245 - accuracy: 0.6337 - val_loss: 1.3014 - val_accuracy: 0.6388\n"
     ]
    }
   ],
   "source": [
    "#Lets create input layer\n",
    "model5_keras.add(Dense(units=512,kernel_initializer='uniform',input_shape=(1024,)))\n",
    "#Add activation\n",
    "model5_keras.add(Activation('relu'))\n",
    "\n",
    "#Adding 1st hidden layer\n",
    "model5_keras.add(Dense(256, kernel_initializer='uniform'))\n",
    "#Add activation\n",
    "model5_keras.add(Activation('relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "# we have an output of 10 node, which is the the desired dimensions of our output\n",
    "model5_keras.add(Dense(10, kernel_initializer='uniform')) \n",
    "#Add activation\n",
    "# We use the softmax because we have multiclass classification\n",
    "model5_keras.add(Activation('softmax'))\n",
    "\n",
    "# Lets create adam optimizer \n",
    "sgd = optimizers.SGD(learning_rate=0.001)\n",
    "\n",
    "# now compile the network\n",
    "model5_keras.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# now fit the model\n",
    "model5_keras_result=model5_keras.fit(X_train_new, y_train_new,           \n",
    "          validation_data=(X_val_new,y_val_new),\n",
    "          epochs=20,batch_size=50,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that with learning rate 0.001 model performance is very poor .Training Accuracy is 63% and Validation Accuracy is 63%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets create a method to create model with given inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(kernelInitializer,activation,learningRate):\n",
    "       \n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=512,kernel_initializer=kernelInitializer,input_shape=(1024,)))\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    model.add(Dense(256, kernel_initializer=kernelInitializer))\n",
    "    model.add(Activation(activation))\n",
    "    \n",
    "    model.add(Dense(10, kernel_initializer=kernelInitializer)) \n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    optimizer = optimizers.SGD(learning_rate=learningRate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network -6\n",
    "#### use Activation funtion 'relu'\n",
    "#### kernelinitializer ''glorot_uniform\"   (xavier)\n",
    "#### Optimizer 'SGD' with learning rate 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6_keras = buildModel('glorot_uniform','relu',0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/20\n",
      "42000/42000 [==============================] - 16s 388us/step - loss: 1.6945 - accuracy: 0.4590 - val_loss: 1.2050 - val_accuracy: 0.6494\n",
      "Epoch 2/20\n",
      "42000/42000 [==============================] - 17s 398us/step - loss: 1.0572 - accuracy: 0.6866 - val_loss: 0.9393 - val_accuracy: 0.7233\n",
      "Epoch 3/20\n",
      "42000/42000 [==============================] - 16s 381us/step - loss: 0.8665 - accuracy: 0.7446 - val_loss: 0.7939 - val_accuracy: 0.7678\n",
      "Epoch 4/20\n",
      "42000/42000 [==============================] - 16s 391us/step - loss: 0.7582 - accuracy: 0.7784 - val_loss: 0.7211 - val_accuracy: 0.7893\n",
      "Epoch 5/20\n",
      "42000/42000 [==============================] - 17s 408us/step - loss: 0.6838 - accuracy: 0.7999 - val_loss: 0.6757 - val_accuracy: 0.7996\n",
      "Epoch 6/20\n",
      "42000/42000 [==============================] - 20s 484us/step - loss: 0.6288 - accuracy: 0.8147 - val_loss: 0.6119 - val_accuracy: 0.8239\n",
      "Epoch 7/20\n",
      "42000/42000 [==============================] - 19s 454us/step - loss: 0.5843 - accuracy: 0.8283 - val_loss: 0.5721 - val_accuracy: 0.8337\n",
      "Epoch 8/20\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.5465 - accuracy: 0.8401 - val_loss: 0.5438 - val_accuracy: 0.8429\n",
      "Epoch 9/20\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.5142 - accuracy: 0.8490 - val_loss: 0.5209 - val_accuracy: 0.8488\n",
      "Epoch 10/20\n",
      "42000/42000 [==============================] - 18s 432us/step - loss: 0.4862 - accuracy: 0.8555 - val_loss: 0.5046 - val_accuracy: 0.8548\n",
      "Epoch 11/20\n",
      "42000/42000 [==============================] - 17s 400us/step - loss: 0.4597 - accuracy: 0.8642 - val_loss: 0.4834 - val_accuracy: 0.8623\n",
      "Epoch 12/20\n",
      "42000/42000 [==============================] - 18s 417us/step - loss: 0.4368 - accuracy: 0.8712 - val_loss: 0.4493 - val_accuracy: 0.8721\n",
      "Epoch 13/20\n",
      "42000/42000 [==============================] - 16s 386us/step - loss: 0.4145 - accuracy: 0.8787 - val_loss: 0.4419 - val_accuracy: 0.8731\n",
      "Epoch 14/20\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 0.3980 - accuracy: 0.8826 - val_loss: 0.4321 - val_accuracy: 0.8755\n",
      "Epoch 15/20\n",
      "42000/42000 [==============================] - 19s 445us/step - loss: 0.3835 - accuracy: 0.8870 - val_loss: 0.4209 - val_accuracy: 0.8815\n",
      "Epoch 16/20\n",
      "42000/42000 [==============================] - 18s 423us/step - loss: 0.3680 - accuracy: 0.8910 - val_loss: 0.4078 - val_accuracy: 0.8834\n",
      "Epoch 17/20\n",
      "42000/42000 [==============================] - 17s 412us/step - loss: 0.3546 - accuracy: 0.8963 - val_loss: 0.4306 - val_accuracy: 0.8758\n",
      "Epoch 18/20\n",
      "42000/42000 [==============================] - 20s 479us/step - loss: 0.3407 - accuracy: 0.8989 - val_loss: 0.3984 - val_accuracy: 0.8866\n",
      "Epoch 19/20\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 0.3259 - accuracy: 0.9050 - val_loss: 0.4153 - val_accuracy: 0.8808\n",
      "Epoch 20/20\n",
      "42000/42000 [==============================] - 15s 360us/step - loss: 0.3160 - accuracy: 0.9065 - val_loss: 0.3669 - val_accuracy: 0.8979\n"
     ]
    }
   ],
   "source": [
    "# now fit the model\n",
    "model6_keras_result=model6_keras.fit(X_train_new, y_train_new,           \n",
    "          validation_data=(X_val_new,y_val_new),\n",
    "          epochs=20,batch_size=50,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that Training Accuracy is 90% and Validation Accuracy is 89%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network -7\n",
    "#### use Activation funtion 'relu'\n",
    "#### kernelinitializer ''glorot_uniform\"   (xavier)\n",
    "#### Optimizer 'SGD' with learning rate 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/20\n",
      "42000/42000 [==============================] - 17s 413us/step - loss: 2.2915 - accuracy: 0.1632 - val_loss: 2.1753 - val_accuracy: 0.2351\n",
      "Epoch 2/20\n",
      "42000/42000 [==============================] - 18s 423us/step - loss: 2.0957 - accuracy: 0.2945 - val_loss: 2.0119 - val_accuracy: 0.3501\n",
      "Epoch 3/20\n",
      "42000/42000 [==============================] - 22s 535us/step - loss: 1.9354 - accuracy: 0.3931 - val_loss: 1.8545 - val_accuracy: 0.4329\n",
      "Epoch 4/20\n",
      "42000/42000 [==============================] - 16s 384us/step - loss: 1.7816 - accuracy: 0.4676 - val_loss: 1.7050 - val_accuracy: 0.5021\n",
      "Epoch 5/20\n",
      "42000/42000 [==============================] - 16s 393us/step - loss: 1.6382 - accuracy: 0.5274 - val_loss: 1.5706 - val_accuracy: 0.5498\n",
      "Epoch 6/20\n",
      "42000/42000 [==============================] - 17s 410us/step - loss: 1.5127 - accuracy: 0.5704 - val_loss: 1.4560 - val_accuracy: 0.5894\n",
      "Epoch 7/20\n",
      "42000/42000 [==============================] - 18s 431us/step - loss: 1.4068 - accuracy: 0.6055 - val_loss: 1.3608 - val_accuracy: 0.6193\n",
      "Epoch 8/20\n",
      "42000/42000 [==============================] - 20s 470us/step - loss: 1.3190 - accuracy: 0.6336 - val_loss: 1.2815 - val_accuracy: 0.6434\n",
      "Epoch 9/20\n",
      "42000/42000 [==============================] - 18s 440us/step - loss: 1.2464 - accuracy: 0.6545 - val_loss: 1.2149 - val_accuracy: 0.6617\n",
      "Epoch 10/20\n",
      "42000/42000 [==============================] - 19s 456us/step - loss: 1.1858 - accuracy: 0.6699 - val_loss: 1.1617 - val_accuracy: 0.6750\n",
      "Epoch 11/20\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 1.1348 - accuracy: 0.6834 - val_loss: 1.1141 - val_accuracy: 0.6887\n",
      "Epoch 12/20\n",
      "42000/42000 [==============================] - 17s 414us/step - loss: 1.0906 - accuracy: 0.6943 - val_loss: 1.0755 - val_accuracy: 0.6970\n",
      "Epoch 13/20\n",
      "42000/42000 [==============================] - 19s 460us/step - loss: 1.0526 - accuracy: 0.7047 - val_loss: 1.0405 - val_accuracy: 0.7061\n",
      "Epoch 14/20\n",
      "42000/42000 [==============================] - 17s 409us/step - loss: 1.0189 - accuracy: 0.7127 - val_loss: 1.0082 - val_accuracy: 0.7153\n",
      "Epoch 15/20\n",
      "42000/42000 [==============================] - 17s 413us/step - loss: 0.9886 - accuracy: 0.7218 - val_loss: 0.9811 - val_accuracy: 0.7229\n",
      "Epoch 16/20\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 0.9617 - accuracy: 0.7289 - val_loss: 0.9560 - val_accuracy: 0.7289\n",
      "Epoch 17/20\n",
      "42000/42000 [==============================] - 18s 424us/step - loss: 0.9366 - accuracy: 0.7347 - val_loss: 0.9342 - val_accuracy: 0.7357\n",
      "Epoch 18/20\n",
      "42000/42000 [==============================] - 18s 436us/step - loss: 0.9140 - accuracy: 0.7421 - val_loss: 0.9122 - val_accuracy: 0.7410\n",
      "Epoch 19/20\n",
      "42000/42000 [==============================] - 19s 446us/step - loss: 0.8932 - accuracy: 0.7483 - val_loss: 0.8929 - val_accuracy: 0.7469\n",
      "Epoch 20/20\n",
      "42000/42000 [==============================] - 17s 407us/step - loss: 0.8729 - accuracy: 0.7544 - val_loss: 0.8745 - val_accuracy: 0.7509\n"
     ]
    }
   ],
   "source": [
    "model7_keras = buildModel('glorot_uniform','relu',0.001)\n",
    "# now fit the model\n",
    "model7_keras_result=model7_keras.fit(X_train_new, y_train_new,           \n",
    "          validation_data=(X_val_new,y_val_new),\n",
    "          epochs=20,batch_size=50,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that Training Accuracy is 87% and Validation Accuracy is 75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network -8\n",
    "#### use Activation funtion 'relu'\n",
    "#### kernelinitializer ''he_uniform\"   \n",
    "#### Optimizer 'SGD' with learning rate 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/20\n",
      "42000/42000 [==============================] - 19s 459us/step - loss: 1.6433 - accuracy: 0.4643 - val_loss: 1.1658 - val_accuracy: 0.6523\n",
      "Epoch 2/20\n",
      "42000/42000 [==============================] - 19s 453us/step - loss: 1.0324 - accuracy: 0.6913 - val_loss: 0.9044 - val_accuracy: 0.7333\n",
      "Epoch 3/20\n",
      "42000/42000 [==============================] - 17s 400us/step - loss: 0.8498 - accuracy: 0.7476 - val_loss: 0.7833 - val_accuracy: 0.7700\n",
      "Epoch 4/20\n",
      "42000/42000 [==============================] - 17s 401us/step - loss: 0.7456 - accuracy: 0.7785 - val_loss: 0.7011 - val_accuracy: 0.7932\n",
      "Epoch 5/20\n",
      "42000/42000 [==============================] - 17s 404us/step - loss: 0.6731 - accuracy: 0.8006 - val_loss: 0.6405 - val_accuracy: 0.8145\n",
      "Epoch 6/20\n",
      "42000/42000 [==============================] - 16s 372us/step - loss: 0.6155 - accuracy: 0.8168 - val_loss: 0.5982 - val_accuracy: 0.8271\n",
      "Epoch 7/20\n",
      "42000/42000 [==============================] - 18s 439us/step - loss: 0.5703 - accuracy: 0.8307 - val_loss: 0.5762 - val_accuracy: 0.8334\n",
      "Epoch 8/20\n",
      "42000/42000 [==============================] - 17s 405us/step - loss: 0.5331 - accuracy: 0.8432 - val_loss: 0.5333 - val_accuracy: 0.8483\n",
      "Epoch 9/20\n",
      "42000/42000 [==============================] - 16s 373us/step - loss: 0.5010 - accuracy: 0.8511 - val_loss: 0.5304 - val_accuracy: 0.8437\n",
      "Epoch 10/20\n",
      "42000/42000 [==============================] - 15s 363us/step - loss: 0.4731 - accuracy: 0.8600 - val_loss: 0.5090 - val_accuracy: 0.8528\n",
      "Epoch 11/20\n",
      "42000/42000 [==============================] - 16s 377us/step - loss: 0.4490 - accuracy: 0.8669 - val_loss: 0.4696 - val_accuracy: 0.8649\n",
      "Epoch 12/20\n",
      "42000/42000 [==============================] - 16s 373us/step - loss: 0.4287 - accuracy: 0.8722 - val_loss: 0.4496 - val_accuracy: 0.8726\n",
      "Epoch 13/20\n",
      "42000/42000 [==============================] - 15s 359us/step - loss: 0.4068 - accuracy: 0.8787 - val_loss: 0.4336 - val_accuracy: 0.8787\n",
      "Epoch 14/20\n",
      "42000/42000 [==============================] - 15s 362us/step - loss: 0.3871 - accuracy: 0.8854 - val_loss: 0.4276 - val_accuracy: 0.8797\n",
      "Epoch 15/20\n",
      "42000/42000 [==============================] - 16s 378us/step - loss: 0.3720 - accuracy: 0.8902 - val_loss: 0.4403 - val_accuracy: 0.8716\n",
      "Epoch 16/20\n",
      "42000/42000 [==============================] - 16s 372us/step - loss: 0.3552 - accuracy: 0.8953 - val_loss: 0.3932 - val_accuracy: 0.8916\n",
      "Epoch 17/20\n",
      "42000/42000 [==============================] - 15s 351us/step - loss: 0.3417 - accuracy: 0.9016 - val_loss: 0.3925 - val_accuracy: 0.8920\n",
      "Epoch 18/20\n",
      "42000/42000 [==============================] - 15s 363us/step - loss: 0.3279 - accuracy: 0.9037 - val_loss: 0.3827 - val_accuracy: 0.8954\n",
      "Epoch 19/20\n",
      "42000/42000 [==============================] - 18s 426us/step - loss: 0.3139 - accuracy: 0.9087 - val_loss: 0.3896 - val_accuracy: 0.8927\n",
      "Epoch 20/20\n",
      "42000/42000 [==============================] - 17s 409us/step - loss: 0.3031 - accuracy: 0.9110 - val_loss: 0.3647 - val_accuracy: 0.9008\n"
     ]
    }
   ],
   "source": [
    "model8_keras = buildModel('he_uniform','relu',0.01)\n",
    "# now fit the model\n",
    "model8_keras_result=model8_keras.fit(X_train_new, y_train_new,           \n",
    "          validation_data=(X_val_new,y_val_new),\n",
    "          epochs=20,batch_size=50,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that Training Accuracy is 91% and Validation Accuracy is 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network -9\n",
    "#### use Activation funtion 'sigmoid'\n",
    "#### kernelinitializer ''he_uniform\"   \n",
    "#### Optimizer 'adam' with learning rate 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/20\n",
      "42000/42000 [==============================] - 17s 416us/step - loss: 2.3068 - accuracy: 0.1077 - val_loss: 2.3010 - val_accuracy: 0.1067\n",
      "Epoch 2/20\n",
      "42000/42000 [==============================] - 18s 421us/step - loss: 2.2990 - accuracy: 0.1193 - val_loss: 2.2933 - val_accuracy: 0.1381\n",
      "Epoch 3/20\n",
      "42000/42000 [==============================] - 17s 402us/step - loss: 2.2926 - accuracy: 0.1360 - val_loss: 2.2923 - val_accuracy: 0.1310\n",
      "Epoch 4/20\n",
      "42000/42000 [==============================] - 18s 422us/step - loss: 2.2864 - accuracy: 0.1487 - val_loss: 2.2831 - val_accuracy: 0.1427\n",
      "Epoch 5/20\n",
      "42000/42000 [==============================] - 22s 518us/step - loss: 2.2801 - accuracy: 0.1627 - val_loss: 2.2771 - val_accuracy: 0.1717\n",
      "Epoch 6/20\n",
      "42000/42000 [==============================] - 21s 496us/step - loss: 2.2732 - accuracy: 0.1741 - val_loss: 2.2685 - val_accuracy: 0.1964\n",
      "Epoch 7/20\n",
      "42000/42000 [==============================] - 23s 541us/step - loss: 2.2642 - accuracy: 0.1911 - val_loss: 2.2615 - val_accuracy: 0.1834\n",
      "Epoch 8/20\n",
      "42000/42000 [==============================] - 21s 499us/step - loss: 2.2533 - accuracy: 0.2041 - val_loss: 2.2455 - val_accuracy: 0.2207\n",
      "Epoch 9/20\n",
      "42000/42000 [==============================] - 20s 471us/step - loss: 2.2383 - accuracy: 0.2233 - val_loss: 2.2276 - val_accuracy: 0.2176\n",
      "Epoch 10/20\n",
      "42000/42000 [==============================] - 21s 505us/step - loss: 2.2160 - accuracy: 0.2446 - val_loss: 2.2011 - val_accuracy: 0.2473\n",
      "Epoch 11/20\n",
      "42000/42000 [==============================] - 19s 451us/step - loss: 2.1828 - accuracy: 0.2698 - val_loss: 2.1589 - val_accuracy: 0.2901\n",
      "Epoch 12/20\n",
      "42000/42000 [==============================] - 18s 425us/step - loss: 2.1336 - accuracy: 0.3030 - val_loss: 2.1029 - val_accuracy: 0.3272\n",
      "Epoch 13/20\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 2.0649 - accuracy: 0.3454 - val_loss: 2.0224 - val_accuracy: 0.3750\n",
      "Epoch 14/20\n",
      "42000/42000 [==============================] - 17s 407us/step - loss: 1.9786 - accuracy: 0.3858 - val_loss: 1.9284 - val_accuracy: 0.4012\n",
      "Epoch 15/20\n",
      "42000/42000 [==============================] - 16s 392us/step - loss: 1.8819 - accuracy: 0.4198 - val_loss: 1.8308 - val_accuracy: 0.4372\n",
      "Epoch 16/20\n",
      "42000/42000 [==============================] - 17s 404us/step - loss: 1.7853 - accuracy: 0.4475 - val_loss: 1.7380 - val_accuracy: 0.4698\n",
      "Epoch 17/20\n",
      "42000/42000 [==============================] - 18s 419us/step - loss: 1.6979 - accuracy: 0.4740 - val_loss: 1.6563 - val_accuracy: 0.4765\n",
      "Epoch 18/20\n",
      "42000/42000 [==============================] - 18s 428us/step - loss: 1.6195 - accuracy: 0.5005 - val_loss: 1.5812 - val_accuracy: 0.5243\n",
      "Epoch 19/20\n",
      "42000/42000 [==============================] - 18s 427us/step - loss: 1.5502 - accuracy: 0.5296 - val_loss: 1.5147 - val_accuracy: 0.5469\n",
      "Epoch 20/20\n",
      "42000/42000 [==============================] - 19s 448us/step - loss: 1.4856 - accuracy: 0.5564 - val_loss: 1.4567 - val_accuracy: 0.5748\n"
     ]
    }
   ],
   "source": [
    "model9_keras = buildModel('he_uniform','sigmoid',0.01)\n",
    "# now fit the model\n",
    "model9_keras_result=model9_keras.fit(X_train_new, y_train_new,           \n",
    "          validation_data=(X_val_new,y_val_new),\n",
    "          epochs=20,batch_size=40,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that Training Accuracy is 55% and Validation Accuracy is 57%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV to find best values for batch_size and epoch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets use gridSearchCV to find optimal hyperparamters for better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # default values\n",
    "    activation='relu' # or linear\n",
    "    dropout_rate=0.2 # or 0.2\n",
    "    init_mode='uniform'\n",
    "    weight_constraint=0 # or  4\n",
    "    optimizer='adam' # or SGD\n",
    "    lr = 0.01\n",
    "    momemntum=0\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, \n",
    "                    input_dim=1024, kernel_initializer=init_mode, \n",
    "                    activation=activation))\n",
    "        #Adding 1st hidden layer\n",
    "    model.add(Dense(50, kernel_initializer=init_mode))\n",
    "    #Add activation\n",
    "    model.add(Activation(activation))\n",
    "    # Adding the output layer\n",
    "    # we have an output of 10 node, which is the the desired dimensions of our output\n",
    "    model.add(Dense(10, kernel_initializer=init_mode)) \n",
    "    #Add activation\n",
    "    # We use the softmax because we have multiclass classification\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, batch_size=1000, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search epochs, batch size\n",
    "epochs = [10,300] # add 50, 100, 150 etc\n",
    "batch_size = [500,1000] # add 5, 10, 20, 40, 60, 80, 100 etc\n",
    "param_grid = dict(epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "42000/42000 [==============================] - 1s 35us/step - loss: 1.8574 - accuracy: 0.3891\n",
      "Epoch 2/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 1.1618 - accuracy: 0.6461\n",
      "Epoch 3/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.9623 - accuracy: 0.7100\n",
      "Epoch 4/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.8325 - accuracy: 0.7517\n",
      "Epoch 5/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.7598 - accuracy: 0.7740\n",
      "Epoch 6/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.7026 - accuracy: 0.7922\n",
      "Epoch 7/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.6598 - accuracy: 0.8028\n",
      "Epoch 8/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.6244 - accuracy: 0.8152\n",
      "Epoch 9/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.5909 - accuracy: 0.8269\n",
      "Epoch 10/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.5695 - accuracy: 0.8331\n",
      "Epoch 11/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.5430 - accuracy: 0.8392\n",
      "Epoch 12/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.5253 - accuracy: 0.8455\n",
      "Epoch 13/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.5061 - accuracy: 0.8507\n",
      "Epoch 14/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.4965 - accuracy: 0.8543\n",
      "Epoch 15/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.4849 - accuracy: 0.8573\n",
      "Epoch 16/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.4653 - accuracy: 0.8620\n",
      "Epoch 17/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.4520 - accuracy: 0.8668\n",
      "Epoch 18/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.4442 - accuracy: 0.8679\n",
      "Epoch 19/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.4361 - accuracy: 0.8701\n",
      "Epoch 20/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.4253 - accuracy: 0.8724\n",
      "Epoch 21/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.4149 - accuracy: 0.8763\n",
      "Epoch 22/300\n",
      "42000/42000 [==============================] - 1s 26us/step - loss: 0.4052 - accuracy: 0.8797\n",
      "Epoch 23/300\n",
      "42000/42000 [==============================] - 1s 26us/step - loss: 0.3992 - accuracy: 0.8806\n",
      "Epoch 24/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.3877 - accuracy: 0.8847\n",
      "Epoch 25/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.3813 - accuracy: 0.8862\n",
      "Epoch 26/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.3823 - accuracy: 0.8851\n",
      "Epoch 27/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.3773 - accuracy: 0.8865\n",
      "Epoch 28/300\n",
      "42000/42000 [==============================] - 1s 31us/step - loss: 0.3782 - accuracy: 0.8869\n",
      "Epoch 29/300\n",
      "42000/42000 [==============================] - 1s 31us/step - loss: 0.3589 - accuracy: 0.8926\n",
      "Epoch 30/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.3560 - accuracy: 0.8927\n",
      "Epoch 31/300\n",
      "42000/42000 [==============================] - 1s 31us/step - loss: 0.3474 - accuracy: 0.8946\n",
      "Epoch 32/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.3418 - accuracy: 0.8972\n",
      "Epoch 33/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.3358 - accuracy: 0.8987\n",
      "Epoch 34/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.3517 - accuracy: 0.8933\n",
      "Epoch 35/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.3322 - accuracy: 0.9002\n",
      "Epoch 36/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.3225 - accuracy: 0.9024\n",
      "Epoch 37/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.3215 - accuracy: 0.9019\n",
      "Epoch 38/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.3147 - accuracy: 0.9049\n",
      "Epoch 39/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.3043 - accuracy: 0.9094\n",
      "Epoch 40/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.3050 - accuracy: 0.9075\n",
      "Epoch 41/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.3083 - accuracy: 0.9065\n",
      "Epoch 42/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.3154 - accuracy: 0.9040\n",
      "Epoch 43/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.3000 - accuracy: 0.9085\n",
      "Epoch 44/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.2895 - accuracy: 0.9125\n",
      "Epoch 45/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.2930 - accuracy: 0.9113\n",
      "Epoch 46/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.2858 - accuracy: 0.9128\n",
      "Epoch 47/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.2876 - accuracy: 0.9137\n",
      "Epoch 48/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.2886 - accuracy: 0.9122\n",
      "Epoch 49/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.2792 - accuracy: 0.9149\n",
      "Epoch 50/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.2749 - accuracy: 0.9157\n",
      "Epoch 51/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.2648 - accuracy: 0.9199\n",
      "Epoch 52/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.2692 - accuracy: 0.9180\n",
      "Epoch 53/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.2627 - accuracy: 0.9211\n",
      "Epoch 54/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.2526 - accuracy: 0.9226\n",
      "Epoch 55/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.2573 - accuracy: 0.9220\n",
      "Epoch 56/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.2609 - accuracy: 0.9195\n",
      "Epoch 57/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.2564 - accuracy: 0.9233\n",
      "Epoch 58/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.2534 - accuracy: 0.9235\n",
      "Epoch 59/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.2433 - accuracy: 0.9259\n",
      "Epoch 60/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.2534 - accuracy: 0.9235\n",
      "Epoch 61/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.2474 - accuracy: 0.9246\n",
      "Epoch 62/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.2388 - accuracy: 0.9268\n",
      "Epoch 63/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.2341 - accuracy: 0.9295\n",
      "Epoch 64/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.2479 - accuracy: 0.9244\n",
      "Epoch 65/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.2436 - accuracy: 0.9260\n",
      "Epoch 66/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.2264 - accuracy: 0.9313\n",
      "Epoch 67/300\n",
      "42000/42000 [==============================] - 2s 43us/step - loss: 0.2245 - accuracy: 0.9323\n",
      "Epoch 68/300\n",
      "42000/42000 [==============================] - 2s 41us/step - loss: 0.2258 - accuracy: 0.9307\n",
      "Epoch 69/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.2240 - accuracy: 0.9323\n",
      "Epoch 70/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.2226 - accuracy: 0.9332\n",
      "Epoch 71/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.2348 - accuracy: 0.9282\n",
      "Epoch 72/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.2193 - accuracy: 0.9335\n",
      "Epoch 73/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.2153 - accuracy: 0.9346\n",
      "Epoch 74/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.2097 - accuracy: 0.9365\n",
      "Epoch 75/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.2150 - accuracy: 0.9354\n",
      "Epoch 76/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.2178 - accuracy: 0.9338\n",
      "Epoch 77/300\n",
      "42000/42000 [==============================] - 1s 32us/step - loss: 0.2088 - accuracy: 0.9358\n",
      "Epoch 78/300\n",
      "42000/42000 [==============================] - 1s 31us/step - loss: 0.2182 - accuracy: 0.9337\n",
      "Epoch 79/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.2366 - accuracy: 0.9288\n",
      "Epoch 80/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.2028 - accuracy: 0.9385\n",
      "Epoch 81/300\n",
      "42000/42000 [==============================] - 1s 32us/step - loss: 0.1988 - accuracy: 0.9386\n",
      "Epoch 82/300\n",
      "42000/42000 [==============================] - 1s 31us/step - loss: 0.1935 - accuracy: 0.9416\n",
      "Epoch 83/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.1968 - accuracy: 0.9414\n",
      "Epoch 84/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.2026 - accuracy: 0.9390\n",
      "Epoch 85/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1975 - accuracy: 0.9396\n",
      "Epoch 86/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1927 - accuracy: 0.9410\n",
      "Epoch 87/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1981 - accuracy: 0.9392\n",
      "Epoch 88/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1946 - accuracy: 0.9401\n",
      "Epoch 89/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1935 - accuracy: 0.9410\n",
      "Epoch 90/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1949 - accuracy: 0.9402\n",
      "Epoch 91/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1909 - accuracy: 0.9421\n",
      "Epoch 92/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1838 - accuracy: 0.9448\n",
      "Epoch 93/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1869 - accuracy: 0.9432\n",
      "Epoch 94/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1899 - accuracy: 0.9412\n",
      "Epoch 95/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1850 - accuracy: 0.9443\n",
      "Epoch 96/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1830 - accuracy: 0.9435\n",
      "Epoch 97/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1852 - accuracy: 0.9444\n",
      "Epoch 98/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1824 - accuracy: 0.9452\n",
      "Epoch 99/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1764 - accuracy: 0.9469\n",
      "Epoch 100/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1693 - accuracy: 0.9488\n",
      "Epoch 101/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1686 - accuracy: 0.9477\n",
      "Epoch 102/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1829 - accuracy: 0.9438\n",
      "Epoch 103/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1771 - accuracy: 0.9456\n",
      "Epoch 104/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1744 - accuracy: 0.9470\n",
      "Epoch 105/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1677 - accuracy: 0.9496\n",
      "Epoch 106/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1680 - accuracy: 0.9484\n",
      "Epoch 107/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1738 - accuracy: 0.9464\n",
      "Epoch 108/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.2032 - accuracy: 0.9365\n",
      "Epoch 109/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.1700 - accuracy: 0.9491\n",
      "Epoch 110/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1699 - accuracy: 0.9478\n",
      "Epoch 111/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.1524 - accuracy: 0.9546\n",
      "Epoch 112/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1578 - accuracy: 0.9525\n",
      "Epoch 113/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1543 - accuracy: 0.9534\n",
      "Epoch 114/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1619 - accuracy: 0.9511\n",
      "Epoch 115/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1549 - accuracy: 0.9521\n",
      "Epoch 116/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1509 - accuracy: 0.9537\n",
      "Epoch 117/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1678 - accuracy: 0.9491\n",
      "Epoch 118/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1473 - accuracy: 0.9555\n",
      "Epoch 119/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1528 - accuracy: 0.9543\n",
      "Epoch 120/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1462 - accuracy: 0.9555\n",
      "Epoch 121/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1469 - accuracy: 0.9561\n",
      "Epoch 122/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1408 - accuracy: 0.9573\n",
      "Epoch 123/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1592 - accuracy: 0.9508\n",
      "Epoch 124/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1536 - accuracy: 0.9529\n",
      "Epoch 125/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1445 - accuracy: 0.9552\n",
      "Epoch 126/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1448 - accuracy: 0.9568\n",
      "Epoch 127/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1592 - accuracy: 0.9497\n",
      "Epoch 128/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1485 - accuracy: 0.9534\n",
      "Epoch 129/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1462 - accuracy: 0.9547\n",
      "Epoch 130/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1399 - accuracy: 0.9572\n",
      "Epoch 131/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1321 - accuracy: 0.9604\n",
      "Epoch 132/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1383 - accuracy: 0.9575\n",
      "Epoch 133/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1486 - accuracy: 0.9546\n",
      "Epoch 134/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1597 - accuracy: 0.9511\n",
      "Epoch 135/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1628 - accuracy: 0.9495\n",
      "Epoch 136/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1298 - accuracy: 0.9600\n",
      "Epoch 137/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1204 - accuracy: 0.9654\n",
      "Epoch 138/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1355 - accuracy: 0.9584\n",
      "Epoch 139/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1343 - accuracy: 0.9598\n",
      "Epoch 140/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1368 - accuracy: 0.9585\n",
      "Epoch 141/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1312 - accuracy: 0.9590\n",
      "Epoch 142/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.1310 - accuracy: 0.9596\n",
      "Epoch 143/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.1242 - accuracy: 0.9627\n",
      "Epoch 144/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.1322 - accuracy: 0.9582\n",
      "Epoch 145/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1445 - accuracy: 0.9555\n",
      "Epoch 146/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1363 - accuracy: 0.9586\n",
      "Epoch 147/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1333 - accuracy: 0.9589\n",
      "Epoch 148/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1200 - accuracy: 0.9629\n",
      "Epoch 149/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1340 - accuracy: 0.9593\n",
      "Epoch 150/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1383 - accuracy: 0.9571\n",
      "Epoch 151/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1154 - accuracy: 0.9645\n",
      "Epoch 152/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1210 - accuracy: 0.9641\n",
      "Epoch 153/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1276 - accuracy: 0.9610\n",
      "Epoch 154/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1344 - accuracy: 0.9581\n",
      "Epoch 155/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1229 - accuracy: 0.9621\n",
      "Epoch 156/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1234 - accuracy: 0.9632\n",
      "Epoch 157/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1177 - accuracy: 0.9644\n",
      "Epoch 158/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1085 - accuracy: 0.9662\n",
      "Epoch 159/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1332 - accuracy: 0.9587\n",
      "Epoch 160/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1371 - accuracy: 0.9568\n",
      "Epoch 161/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1201 - accuracy: 0.9629\n",
      "Epoch 162/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1101 - accuracy: 0.9663\n",
      "Epoch 163/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1141 - accuracy: 0.9649\n",
      "Epoch 164/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1172 - accuracy: 0.9636\n",
      "Epoch 165/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1074 - accuracy: 0.9674\n",
      "Epoch 166/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1102 - accuracy: 0.9660\n",
      "Epoch 167/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1211 - accuracy: 0.9625\n",
      "Epoch 168/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1249 - accuracy: 0.9619\n",
      "Epoch 169/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1086 - accuracy: 0.9665\n",
      "Epoch 170/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1061 - accuracy: 0.9675\n",
      "Epoch 171/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1124 - accuracy: 0.9665\n",
      "Epoch 172/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1183 - accuracy: 0.9639\n",
      "Epoch 173/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1147 - accuracy: 0.9642\n",
      "Epoch 174/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1102 - accuracy: 0.9655\n",
      "Epoch 175/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1222 - accuracy: 0.9624\n",
      "Epoch 176/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1228 - accuracy: 0.9626\n",
      "Epoch 177/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1132 - accuracy: 0.9648\n",
      "Epoch 178/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1085 - accuracy: 0.9678\n",
      "Epoch 179/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1195 - accuracy: 0.9631\n",
      "Epoch 180/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1133 - accuracy: 0.9652\n",
      "Epoch 181/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1036 - accuracy: 0.9683\n",
      "Epoch 182/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0995 - accuracy: 0.9697\n",
      "Epoch 183/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0963 - accuracy: 0.9703\n",
      "Epoch 184/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1063 - accuracy: 0.9668\n",
      "Epoch 185/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1042 - accuracy: 0.9679\n",
      "Epoch 186/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0961 - accuracy: 0.9709\n",
      "Epoch 187/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0964 - accuracy: 0.9710\n",
      "Epoch 188/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1009 - accuracy: 0.9698\n",
      "Epoch 189/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1091 - accuracy: 0.9671\n",
      "Epoch 190/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1146 - accuracy: 0.9646\n",
      "Epoch 191/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1017 - accuracy: 0.9684\n",
      "Epoch 192/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1056 - accuracy: 0.9684\n",
      "Epoch 193/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1124 - accuracy: 0.9648\n",
      "Epoch 194/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.1133 - accuracy: 0.9653\n",
      "Epoch 195/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0983 - accuracy: 0.9696\n",
      "Epoch 196/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1170 - accuracy: 0.9640\n",
      "Epoch 197/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.0937 - accuracy: 0.9715\n",
      "Epoch 198/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.0900 - accuracy: 0.9732\n",
      "Epoch 199/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1008 - accuracy: 0.9693\n",
      "Epoch 200/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0910 - accuracy: 0.9718\n",
      "Epoch 201/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0987 - accuracy: 0.9694\n",
      "Epoch 202/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1079 - accuracy: 0.9676\n",
      "Epoch 203/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1055 - accuracy: 0.9673\n",
      "Epoch 204/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1014 - accuracy: 0.9688\n",
      "Epoch 205/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1182 - accuracy: 0.9625\n",
      "Epoch 206/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0912 - accuracy: 0.9724\n",
      "Epoch 207/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0956 - accuracy: 0.9713\n",
      "Epoch 208/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0823 - accuracy: 0.9756\n",
      "Epoch 209/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0844 - accuracy: 0.9755\n",
      "Epoch 210/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0992 - accuracy: 0.9699\n",
      "Epoch 211/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1046 - accuracy: 0.9679\n",
      "Epoch 212/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0974 - accuracy: 0.9700\n",
      "Epoch 213/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0851 - accuracy: 0.9742\n",
      "Epoch 214/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.0797 - accuracy: 0.9763\n",
      "Epoch 215/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1012 - accuracy: 0.9687\n",
      "Epoch 216/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.1071 - accuracy: 0.9670\n",
      "Epoch 217/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0927 - accuracy: 0.9710\n",
      "Epoch 218/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0845 - accuracy: 0.9750\n",
      "Epoch 219/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0946 - accuracy: 0.9697\n",
      "Epoch 220/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0886 - accuracy: 0.9727\n",
      "Epoch 221/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.0817 - accuracy: 0.9753\n",
      "Epoch 222/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0923 - accuracy: 0.9720\n",
      "Epoch 223/300\n",
      "42000/42000 [==============================] - 1s 32us/step - loss: 0.0867 - accuracy: 0.9732\n",
      "Epoch 224/300\n",
      "42000/42000 [==============================] - 1s 33us/step - loss: 0.0915 - accuracy: 0.9721\n",
      "Epoch 225/300\n",
      "42000/42000 [==============================] - 1s 32us/step - loss: 0.0781 - accuracy: 0.9762\n",
      "Epoch 226/300\n",
      "42000/42000 [==============================] - 1s 33us/step - loss: 0.0740 - accuracy: 0.9783\n",
      "Epoch 227/300\n",
      "42000/42000 [==============================] - 1s 34us/step - loss: 0.0787 - accuracy: 0.9762\n",
      "Epoch 228/300\n",
      "42000/42000 [==============================] - 1s 33us/step - loss: 0.0803 - accuracy: 0.9761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/300\n",
      "42000/42000 [==============================] - 1s 32us/step - loss: 0.1213 - accuracy: 0.9625\n",
      "Epoch 230/300\n",
      "42000/42000 [==============================] - 1s 32us/step - loss: 0.0975 - accuracy: 0.9708\n",
      "Epoch 231/300\n",
      "42000/42000 [==============================] - 1s 34us/step - loss: 0.1037 - accuracy: 0.9680\n",
      "Epoch 232/300\n",
      "42000/42000 [==============================] - 1s 34us/step - loss: 0.0907 - accuracy: 0.9712\n",
      "Epoch 233/300\n",
      "42000/42000 [==============================] - 1s 32us/step - loss: 0.0717 - accuracy: 0.9788\n",
      "Epoch 234/300\n",
      "42000/42000 [==============================] - 1s 32us/step - loss: 0.0674 - accuracy: 0.9807\n",
      "Epoch 235/300\n",
      "42000/42000 [==============================] - 1s 32us/step - loss: 0.0714 - accuracy: 0.9785\n",
      "Epoch 236/300\n",
      "42000/42000 [==============================] - 1s 31us/step - loss: 0.0716 - accuracy: 0.9784\n",
      "Epoch 237/300\n",
      "42000/42000 [==============================] - 1s 31us/step - loss: 0.0826 - accuracy: 0.9756\n",
      "Epoch 238/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.1058 - accuracy: 0.9686\n",
      "Epoch 239/300\n",
      "42000/42000 [==============================] - 1s 26us/step - loss: 0.0940 - accuracy: 0.9707\n",
      "Epoch 240/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0832 - accuracy: 0.9748\n",
      "Epoch 241/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0822 - accuracy: 0.9747\n",
      "Epoch 242/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0812 - accuracy: 0.9744\n",
      "Epoch 243/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0694 - accuracy: 0.9790\n",
      "Epoch 244/300\n",
      "42000/42000 [==============================] - 1s 26us/step - loss: 0.0743 - accuracy: 0.9777\n",
      "Epoch 245/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0620 - accuracy: 0.9818\n",
      "Epoch 246/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0804 - accuracy: 0.9754\n",
      "Epoch 247/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0774 - accuracy: 0.9761\n",
      "Epoch 248/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0816 - accuracy: 0.9749\n",
      "Epoch 249/300\n",
      "42000/42000 [==============================] - ETA: 0s - loss: 0.0748 - accuracy: 0.97 - 1s 28us/step - loss: 0.0747 - accuracy: 0.9768\n",
      "Epoch 250/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0869 - accuracy: 0.9729\n",
      "Epoch 251/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0979 - accuracy: 0.9693\n",
      "Epoch 252/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0701 - accuracy: 0.9780\n",
      "Epoch 253/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0671 - accuracy: 0.9796\n",
      "Epoch 254/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0885 - accuracy: 0.9732\n",
      "Epoch 255/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0687 - accuracy: 0.9789\n",
      "Epoch 256/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0688 - accuracy: 0.9797\n",
      "Epoch 257/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0778 - accuracy: 0.9760\n",
      "Epoch 258/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0690 - accuracy: 0.9789\n",
      "Epoch 259/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0895 - accuracy: 0.9729\n",
      "Epoch 260/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0869 - accuracy: 0.9738\n",
      "Epoch 261/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0776 - accuracy: 0.9760\n",
      "Epoch 262/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0814 - accuracy: 0.9743\n",
      "Epoch 263/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0715 - accuracy: 0.9786\n",
      "Epoch 264/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0728 - accuracy: 0.9775\n",
      "Epoch 265/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0843 - accuracy: 0.9750\n",
      "Epoch 266/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0911 - accuracy: 0.9722\n",
      "Epoch 267/300\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.0692 - accuracy: 0.9794\n",
      "Epoch 268/300\n",
      "42000/42000 [==============================] - 1s 35us/step - loss: 0.0672 - accuracy: 0.9801\n",
      "Epoch 269/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.0783 - accuracy: 0.9760\n",
      "Epoch 270/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0875 - accuracy: 0.9724\n",
      "Epoch 271/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0700 - accuracy: 0.9781\n",
      "Epoch 272/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.0807 - accuracy: 0.9757\n",
      "Epoch 273/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.0673 - accuracy: 0.9795\n",
      "Epoch 274/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.0824 - accuracy: 0.9753\n",
      "Epoch 275/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.0650 - accuracy: 0.9812\n",
      "Epoch 276/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0604 - accuracy: 0.9823\n",
      "Epoch 277/300\n",
      "42000/42000 [==============================] - 1s 34us/step - loss: 0.0656 - accuracy: 0.9802\n",
      "Epoch 278/300\n",
      "42000/42000 [==============================] - 2s 49us/step - loss: 0.0627 - accuracy: 0.9812\n",
      "Epoch 279/300\n",
      "42000/42000 [==============================] - 2s 38us/step - loss: 0.0675 - accuracy: 0.9795\n",
      "Epoch 280/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.0707 - accuracy: 0.9779\n",
      "Epoch 281/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.0649 - accuracy: 0.9802\n",
      "Epoch 282/300\n",
      "42000/42000 [==============================] - 1s 33us/step - loss: 0.0778 - accuracy: 0.9763\n",
      "Epoch 283/300\n",
      "42000/42000 [==============================] - 2s 36us/step - loss: 0.0965 - accuracy: 0.9701\n",
      "Epoch 284/300\n",
      "42000/42000 [==============================] - 1s 35us/step - loss: 0.0825 - accuracy: 0.9738\n",
      "Epoch 285/300\n",
      "42000/42000 [==============================] - 1s 35us/step - loss: 0.0686 - accuracy: 0.9789\n",
      "Epoch 286/300\n",
      "42000/42000 [==============================] - 2s 36us/step - loss: 0.0545 - accuracy: 0.9839\n",
      "Epoch 287/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.0643 - accuracy: 0.9800\n",
      "Epoch 288/300\n",
      "42000/42000 [==============================] - 2s 40us/step - loss: 0.0923 - accuracy: 0.9717\n",
      "Epoch 289/300\n",
      "42000/42000 [==============================] - 1s 33us/step - loss: 0.0739 - accuracy: 0.9771\n",
      "Epoch 290/300\n",
      "42000/42000 [==============================] - 1s 33us/step - loss: 0.0572 - accuracy: 0.9832\n",
      "Epoch 291/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.0772 - accuracy: 0.9759\n",
      "Epoch 292/300\n",
      "42000/42000 [==============================] - 1s 27us/step - loss: 0.0689 - accuracy: 0.9794\n",
      "Epoch 293/300\n",
      "42000/42000 [==============================] - 1s 28us/step - loss: 0.1001 - accuracy: 0.9682\n",
      "Epoch 294/300\n",
      "42000/42000 [==============================] - 1s 29us/step - loss: 0.0638 - accuracy: 0.9808\n",
      "Epoch 295/300\n",
      "42000/42000 [==============================] - 1s 32us/step - loss: 0.0658 - accuracy: 0.9804\n",
      "Epoch 296/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.0513 - accuracy: 0.9851\n",
      "Epoch 297/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.0671 - accuracy: 0.9798\n",
      "Epoch 298/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.0759 - accuracy: 0.9768\n",
      "Epoch 299/300\n",
      "42000/42000 [==============================] - 1s 33us/step - loss: 0.0531 - accuracy: 0.9837\n",
      "Epoch 300/300\n",
      "42000/42000 [==============================] - 1s 30us/step - loss: 0.0511 - accuracy: 0.9854\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train_new, y_train_new) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.814595 using {'batch_size': 500, 'epochs': 300}\n"
     ]
    }
   ],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best values as per grid search batch_size=500 and epochs=300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Best models so far we found are as below with validation accuracy more than 90%\n",
    "- Neural Network -2\n",
    "- Neural Network -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/300\n",
      "42000/42000 [==============================] - 7s 164us/step - loss: 0.1538 - accuracy: 0.9496 - val_loss: 0.3057 - val_accuracy: 0.9338\n",
      "Epoch 2/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.0976 - accuracy: 0.9695 - val_loss: 0.2958 - val_accuracy: 0.9394\n",
      "Epoch 3/300\n",
      "42000/42000 [==============================] - 7s 162us/step - loss: 0.0822 - accuracy: 0.9749 - val_loss: 0.2927 - val_accuracy: 0.9434\n",
      "Epoch 4/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 0.0721 - accuracy: 0.9786 - val_loss: 0.2939 - val_accuracy: 0.9443.0716 - ac\n",
      "Epoch 5/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 0.0649 - accuracy: 0.9814 - val_loss: 0.2955 - val_accuracy: 0.9471\n",
      "Epoch 6/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.0583 - accuracy: 0.9838 - val_loss: 0.2982 - val_accuracy: 0.9484\n",
      "Epoch 7/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 0.0532 - accuracy: 0.9855 - val_loss: 0.2997 - val_accuracy: 0.9496\n",
      "Epoch 8/300\n",
      "42000/42000 [==============================] - 7s 170us/step - loss: 0.0476 - accuracy: 0.9881 - val_loss: 0.3037 - val_accuracy: 0.9503\n",
      "Epoch 9/300\n",
      "42000/42000 [==============================] - 7s 166us/step - loss: 0.0436 - accuracy: 0.9890 - val_loss: 0.3062 - val_accuracy: 0.9517\n",
      "Epoch 10/300\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.0399 - accuracy: 0.9906 - val_loss: 0.3104 - val_accuracy: 0.9520\n",
      "Epoch 11/300\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 0.0364 - accuracy: 0.9919 - val_loss: 0.3141 - val_accuracy: 0.9528\n",
      "Epoch 12/300\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 0.0329 - accuracy: 0.9929 - val_loss: 0.3192 - val_accuracy: 0.9539\n",
      "Epoch 13/300\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 0.0297 - accuracy: 0.9942 - val_loss: 0.3225 - val_accuracy: 0.9543\n",
      "Epoch 14/300\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 0.0270 - accuracy: 0.9947 - val_loss: 0.3260 - val_accuracy: 0.9549\n",
      "Epoch 15/300\n",
      "42000/42000 [==============================] - 7s 170us/step - loss: 0.0245 - accuracy: 0.9955 - val_loss: 0.3332 - val_accuracy: 0.9552\n",
      "Epoch 16/300\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 0.0219 - accuracy: 0.9965 - val_loss: 0.3384 - val_accuracy: 0.9558\n",
      "Epoch 17/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 0.0201 - accuracy: 0.9968 - val_loss: 0.3426 - val_accuracy: 0.9559\n",
      "Epoch 18/300\n",
      "42000/42000 [==============================] - 6s 153us/step - loss: 0.0180 - accuracy: 0.9973 - val_loss: 0.3488 - val_accuracy: 0.9559\n",
      "Epoch 19/300\n",
      "42000/42000 [==============================] - 7s 162us/step - loss: 0.0170 - accuracy: 0.9976 - val_loss: 0.3559 - val_accuracy: 0.9561\n",
      "Epoch 20/300\n",
      "42000/42000 [==============================] - 6s 155us/step - loss: 0.0155 - accuracy: 0.9979 - val_loss: 0.3602 - val_accuracy: 0.9564\n",
      "Epoch 21/300\n",
      "42000/42000 [==============================] - 7s 157us/step - loss: 0.0146 - accuracy: 0.9979 - val_loss: 0.3643 - val_accuracy: 0.9564\n",
      "Epoch 22/300\n",
      "42000/42000 [==============================] - 7s 156us/step - loss: 0.0129 - accuracy: 0.9985 - val_loss: 0.3724 - val_accuracy: 0.9565\n",
      "Epoch 23/300\n",
      "42000/42000 [==============================] - 6s 152us/step - loss: 0.0112 - accuracy: 0.9986 - val_loss: 0.3773 - val_accuracy: 0.9568\n",
      "Epoch 24/300\n",
      "42000/42000 [==============================] - 6s 151us/step - loss: 0.0105 - accuracy: 0.9988 - val_loss: 0.3874 - val_accuracy: 0.9559\n",
      "Epoch 25/300\n",
      "42000/42000 [==============================] - 7s 164us/step - loss: 0.0096 - accuracy: 0.9990 - val_loss: 0.3899 - val_accuracy: 0.9571\n",
      "Epoch 26/300\n",
      "42000/42000 [==============================] - 6s 153us/step - loss: 0.0086 - accuracy: 0.9990 - val_loss: 0.3959 - val_accuracy: 0.9568\n",
      "Epoch 27/300\n",
      "42000/42000 [==============================] - 7s 158us/step - loss: 0.0079 - accuracy: 0.9992 - val_loss: 0.4016 - val_accuracy: 0.9570\n",
      "Epoch 28/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.0093 - accuracy: 0.9986 - val_loss: 0.4046 - val_accuracy: 0.9561\n",
      "Epoch 29/300\n",
      "42000/42000 [==============================] - 10s 235us/step - loss: 0.0123 - accuracy: 0.9977 - val_loss: 0.4081 - val_accuracy: 0.9561\n",
      "Epoch 30/300\n",
      "42000/42000 [==============================] - 7s 169us/step - loss: 0.0102 - accuracy: 0.9984 - val_loss: 0.4148 - val_accuracy: 0.9566\n",
      "Epoch 31/300\n",
      "42000/42000 [==============================] - 10s 230us/step - loss: 0.0092 - accuracy: 0.9986 - val_loss: 0.4253 - val_accuracy: 0.9547\n",
      "Epoch 32/300\n",
      "42000/42000 [==============================] - 9s 207us/step - loss: 0.0267 - accuracy: 0.9922 - val_loss: 0.4280 - val_accuracy: 0.9492\n",
      "Epoch 33/300\n",
      "42000/42000 [==============================] - 9s 203us/step - loss: 0.0328 - accuracy: 0.9901 - val_loss: 0.4277 - val_accuracy: 0.9503\n",
      "Epoch 34/300\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0277 - accuracy: 0.9914 - val_loss: 0.4332 - val_accuracy: 0.9515\n",
      "Epoch 35/300\n",
      "42000/42000 [==============================] - 8s 192us/step - loss: 0.0350 - accuracy: 0.9896 - val_loss: 0.4578 - val_accuracy: 0.9401\n",
      "Epoch 36/300\n",
      "42000/42000 [==============================] - 7s 156us/step - loss: 0.0422 - accuracy: 0.9871 - val_loss: 0.4371 - val_accuracy: 0.9459\n",
      "Epoch 37/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 0.0337 - accuracy: 0.9897 - val_loss: 0.4191 - val_accuracy: 0.9530\n",
      "Epoch 38/300\n",
      "42000/42000 [==============================] - 8s 202us/step - loss: 0.0272 - accuracy: 0.9922 - val_loss: 0.4376 - val_accuracy: 0.9493\n",
      "Epoch 39/300\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 0.0275 - accuracy: 0.9918 - val_loss: 0.4436 - val_accuracy: 0.9470\n",
      "Epoch 40/300\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 0.0417 - accuracy: 0.9870 - val_loss: 0.4483 - val_accuracy: 0.9479\n",
      "Epoch 41/300\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 0.0289 - accuracy: 0.9915 - val_loss: 0.4662 - val_accuracy: 0.9483\n",
      "Epoch 42/300\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 0.0277 - accuracy: 0.9915 - val_loss: 0.4559 - val_accuracy: 0.9503\n",
      "Epoch 43/300\n",
      "42000/42000 [==============================] - 8s 197us/step - loss: 0.0219 - accuracy: 0.9936 - val_loss: 0.4537 - val_accuracy: 0.9502\n",
      "Epoch 44/300\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 0.0215 - accuracy: 0.9936 - val_loss: 0.4559 - val_accuracy: 0.9518\n",
      "Epoch 45/300\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0231 - accuracy: 0.9930 - val_loss: 0.4557 - val_accuracy: 0.9501\n",
      "Epoch 46/300\n",
      "42000/42000 [==============================] - 9s 203us/step - loss: 0.0313 - accuracy: 0.9906 - val_loss: 0.4791 - val_accuracy: 0.9463\n",
      "Epoch 47/300\n",
      "42000/42000 [==============================] - 7s 158us/step - loss: 0.0470 - accuracy: 0.9855 - val_loss: 0.5053 - val_accuracy: 0.9419\n",
      "Epoch 48/300\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 0.0461 - accuracy: 0.9855 - val_loss: 0.4557 - val_accuracy: 0.9494\n",
      "Epoch 49/300\n",
      "42000/42000 [==============================] - 7s 158us/step - loss: 0.0354 - accuracy: 0.9891 - val_loss: 0.4759 - val_accuracy: 0.9509\n",
      "Epoch 50/300\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 0.0372 - accuracy: 0.9882 - val_loss: 0.4763 - val_accuracy: 0.9481\n",
      "Epoch 51/300\n",
      "42000/42000 [==============================] - 9s 209us/step - loss: 0.0283 - accuracy: 0.9917 - val_loss: 0.4750 - val_accuracy: 0.9512\n",
      "Epoch 52/300\n",
      "42000/42000 [==============================] - 10s 234us/step - loss: 0.0309 - accuracy: 0.9901 - val_loss: 0.4971 - val_accuracy: 0.9463\n",
      "Epoch 53/300\n",
      "42000/42000 [==============================] - 10s 249us/step - loss: 0.0327 - accuracy: 0.9896 - val_loss: 0.5117 - val_accuracy: 0.9450\n",
      "Epoch 54/300\n",
      "42000/42000 [==============================] - 8s 194us/step - loss: 0.0210 - accuracy: 0.9939 - val_loss: 0.4867 - val_accuracy: 0.9520\n",
      "Epoch 55/300\n",
      "42000/42000 [==============================] - 6s 154us/step - loss: 0.0170 - accuracy: 0.9950 - val_loss: 0.4863 - val_accuracy: 0.9527\n",
      "Epoch 56/300\n",
      "42000/42000 [==============================] - 7s 170us/step - loss: 0.0128 - accuracy: 0.9966 - val_loss: 0.5025 - val_accuracy: 0.9518\n",
      "Epoch 57/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 0.0159 - accuracy: 0.9948 - val_loss: 0.5126 - val_accuracy: 0.9495 - l\n",
      "Epoch 58/300\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.0433 - accuracy: 0.9872 - val_loss: 0.5484 - val_accuracy: 0.9408\n",
      "Epoch 59/300\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 0.0517 - accuracy: 0.9849 - val_loss: 0.5277 - val_accuracy: 0.9426\n",
      "Epoch 60/300\n",
      "42000/42000 [==============================] - 7s 165us/step - loss: 0.0480 - accuracy: 0.9857 - val_loss: 0.5284 - val_accuracy: 0.9438\n",
      "Epoch 61/300\n",
      "42000/42000 [==============================] - 11s 261us/step - loss: 0.0340 - accuracy: 0.9891 - val_loss: 0.5138 - val_accuracy: 0.9481\n",
      "Epoch 62/300\n",
      "42000/42000 [==============================] - 10s 243us/step - loss: 0.0245 - accuracy: 0.9926 - val_loss: 0.5112 - val_accuracy: 0.9507\n",
      "Epoch 63/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0254 - accuracy: 0.9926 - val_loss: 0.5089 - val_accuracy: 0.9500\n",
      "Epoch 64/300\n",
      "42000/42000 [==============================] - 9s 223us/step - loss: 0.0275 - accuracy: 0.9914 - val_loss: 0.5151 - val_accuracy: 0.9497\n",
      "Epoch 65/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0175 - accuracy: 0.9948 - val_loss: 0.5179 - val_accuracy: 0.9512\n",
      "Epoch 66/300\n",
      "42000/42000 [==============================] - 9s 224us/step - loss: 0.0168 - accuracy: 0.9955 - val_loss: 0.5292 - val_accuracy: 0.9505\n",
      "Epoch 67/300\n",
      "42000/42000 [==============================] - 10s 232us/step - loss: 0.0323 - accuracy: 0.9910 - val_loss: 0.5421 - val_accuracy: 0.9463\n",
      "Epoch 68/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0217 - accuracy: 0.9930 - val_loss: 0.5424 - val_accuracy: 0.9496\n",
      "Epoch 69/300\n",
      "42000/42000 [==============================] - 10s 235us/step - loss: 0.0182 - accuracy: 0.9942 - val_loss: 0.5380 - val_accuracy: 0.9496\n",
      "Epoch 70/300\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 0.0178 - accuracy: 0.9942 - val_loss: 0.5348 - val_accuracy: 0.9512\n",
      "Epoch 71/300\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 0.0144 - accuracy: 0.9955 - val_loss: 0.5376 - val_accuracy: 0.9525\n",
      "Epoch 72/300\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0127 - accuracy: 0.9958 - val_loss: 0.5699 - val_accuracy: 0.9493\n",
      "Epoch 73/300\n",
      "42000/42000 [==============================] - 9s 224us/step - loss: 0.0289 - accuracy: 0.9915 - val_loss: 0.5927 - val_accuracy: 0.9445\n",
      "Epoch 74/300\n",
      "42000/42000 [==============================] - 10s 230us/step - loss: 0.0434 - accuracy: 0.9864 - val_loss: 0.5799 - val_accuracy: 0.9420\n",
      "Epoch 75/300\n",
      "42000/42000 [==============================] - 10s 236us/step - loss: 0.0481 - accuracy: 0.9863 - val_loss: 0.5756 - val_accuracy: 0.9468\n",
      "Epoch 76/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0395 - accuracy: 0.9887 - val_loss: 0.5797 - val_accuracy: 0.9464\n",
      "Epoch 77/300\n",
      "42000/42000 [==============================] - 9s 223us/step - loss: 0.0391 - accuracy: 0.9881 - val_loss: 0.5937 - val_accuracy: 0.9475\n",
      "Epoch 78/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0287 - accuracy: 0.9918 - val_loss: 0.5857 - val_accuracy: 0.9486\n",
      "Epoch 79/300\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 0.0262 - accuracy: 0.9921 - val_loss: 0.5596 - val_accuracy: 0.9501\n",
      "Epoch 80/300\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 0.0248 - accuracy: 0.9923 - val_loss: 0.5694 - val_accuracy: 0.9498\n",
      "Epoch 81/300\n",
      "42000/42000 [==============================] - 10s 229us/step - loss: 0.0203 - accuracy: 0.9937 - val_loss: 0.5732 - val_accuracy: 0.9499\n",
      "Epoch 82/300\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 0.0230 - accuracy: 0.9932 - val_loss: 0.5729 - val_accuracy: 0.9506\n",
      "Epoch 83/300\n",
      "42000/42000 [==============================] - 9s 225us/step - loss: 0.0131 - accuracy: 0.9957 - val_loss: 0.5752 - val_accuracy: 0.9525\n",
      "Epoch 84/300\n",
      "42000/42000 [==============================] - 10s 233us/step - loss: 0.0233 - accuracy: 0.9937 - val_loss: 0.5874 - val_accuracy: 0.9484\n",
      "Epoch 85/300\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0189 - accuracy: 0.9942 - val_loss: 0.5954 - val_accuracy: 0.9517\n",
      "Epoch 86/300\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 0.0159 - accuracy: 0.9949 - val_loss: 0.5795 - val_accuracy: 0.9507\n",
      "Epoch 87/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0216 - accuracy: 0.9935 - val_loss: 0.6014 - val_accuracy: 0.9501\n",
      "Epoch 88/300\n",
      "42000/42000 [==============================] - 9s 222us/step - loss: 0.0242 - accuracy: 0.9929 - val_loss: 0.6060 - val_accuracy: 0.9495\n",
      "Epoch 89/300\n",
      "42000/42000 [==============================] - 10s 232us/step - loss: 0.0356 - accuracy: 0.9900 - val_loss: 0.6315 - val_accuracy: 0.9425\n",
      "Epoch 90/300\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 0.0391 - accuracy: 0.9884 - val_loss: 0.5878 - val_accuracy: 0.9486\n",
      "Epoch 91/300\n",
      "42000/42000 [==============================] - 10s 234us/step - loss: 0.0326 - accuracy: 0.9902 - val_loss: 0.6283 - val_accuracy: 0.9465\n",
      "Epoch 92/300\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0469 - accuracy: 0.9874 - val_loss: 0.6149 - val_accuracy: 0.9461\n",
      "Epoch 93/300\n",
      "42000/42000 [==============================] - 9s 224us/step - loss: 0.0505 - accuracy: 0.9858 - val_loss: 0.6272 - val_accuracy: 0.9437\n",
      "Epoch 94/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0280 - accuracy: 0.9915 - val_loss: 0.6190 - val_accuracy: 0.9500\n",
      "Epoch 95/300\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 0.0127 - accuracy: 0.9960 - val_loss: 0.5939 - val_accuracy: 0.9526\n",
      "Epoch 96/300\n",
      "42000/42000 [==============================] - 9s 222us/step - loss: 0.0106 - accuracy: 0.9967 - val_loss: 0.6157 - val_accuracy: 0.9513\n",
      "Epoch 97/300\n",
      "42000/42000 [==============================] - 9s 226us/step - loss: 0.0237 - accuracy: 0.9933 - val_loss: 0.6475 - val_accuracy: 0.9471\n",
      "Epoch 98/300\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.6182 - val_accuracy: 0.9532\n",
      "Epoch 99/300\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 0.0113 - accuracy: 0.9967 - val_loss: 0.6295 - val_accuracy: 0.9508\n",
      "Epoch 100/300\n",
      "42000/42000 [==============================] - 9s 223us/step - loss: 0.0200 - accuracy: 0.9940 - val_loss: 0.6303 - val_accuracy: 0.9507\n",
      "Epoch 101/300\n",
      "42000/42000 [==============================] - 10s 228us/step - loss: 0.0126 - accuracy: 0.9961 - val_loss: 0.6363 - val_accuracy: 0.9502\n",
      "Epoch 102/300\n",
      "42000/42000 [==============================] - 10s 235us/step - loss: 0.0102 - accuracy: 0.9971 - val_loss: 0.6492 - val_accuracy: 0.9524\n",
      "Epoch 103/300\n",
      "42000/42000 [==============================] - 11s 263us/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.6488 - val_accuracy: 0.9508\n",
      "Epoch 104/300\n",
      "42000/42000 [==============================] - 10s 245us/step - loss: 0.0252 - accuracy: 0.9930 - val_loss: 0.6793 - val_accuracy: 0.9466\n",
      "Epoch 105/300\n",
      "42000/42000 [==============================] - 10s 227us/step - loss: 0.0372 - accuracy: 0.9889 - val_loss: 0.6478 - val_accuracy: 0.9481\n",
      "Epoch 106/300\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0334 - accuracy: 0.9903 - val_loss: 0.6446 - val_accuracy: 0.9482\n",
      "Epoch 107/300\n",
      "42000/42000 [==============================] - 9s 223us/step - loss: 0.0361 - accuracy: 0.9898 - val_loss: 0.6520 - val_accuracy: 0.9450\n",
      "Epoch 108/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0271 - accuracy: 0.9917 - val_loss: 0.6442 - val_accuracy: 0.9498\n",
      "Epoch 109/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0243 - accuracy: 0.9930 - val_loss: 0.6535 - val_accuracy: 0.9515\n",
      "Epoch 110/300\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 0.0177 - accuracy: 0.9944 - val_loss: 0.6439 - val_accuracy: 0.9492\n",
      "Epoch 111/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0164 - accuracy: 0.9953 - val_loss: 0.6645 - val_accuracy: 0.9489\n",
      "Epoch 112/300\n",
      "42000/42000 [==============================] - 9s 209us/step - loss: 0.0264 - accuracy: 0.9925 - val_loss: 0.6587 - val_accuracy: 0.9494\n",
      "Epoch 113/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0242 - accuracy: 0.9933 - val_loss: 0.6951 - val_accuracy: 0.9467\n",
      "Epoch 114/300\n",
      "42000/42000 [==============================] - 9s 208us/step - loss: 0.0272 - accuracy: 0.9918 - val_loss: 0.6570 - val_accuracy: 0.9501\n",
      "Epoch 115/300\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 0.0184 - accuracy: 0.9945 - val_loss: 0.6623 - val_accuracy: 0.9513\n",
      "Epoch 116/300\n",
      "42000/42000 [==============================] - 10s 229us/step - loss: 0.0290 - accuracy: 0.9920 - val_loss: 0.6715 - val_accuracy: 0.9483\n",
      "Epoch 117/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0397 - accuracy: 0.9890 - val_loss: 0.6745 - val_accuracy: 0.9492\n",
      "Epoch 118/300\n",
      "42000/42000 [==============================] - 9s 209us/step - loss: 0.0202 - accuracy: 0.9934 - val_loss: 0.6788 - val_accuracy: 0.9508\n",
      "Epoch 119/300\n",
      "42000/42000 [==============================] - 9s 205us/step - loss: 0.0160 - accuracy: 0.9955 - val_loss: 0.7188 - val_accuracy: 0.9484\n",
      "Epoch 120/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0258 - accuracy: 0.9925 - val_loss: 0.6645 - val_accuracy: 0.9483\n",
      "Epoch 121/300\n",
      "42000/42000 [==============================] - 9s 206us/step - loss: 0.0170 - accuracy: 0.9947 - val_loss: 0.6878 - val_accuracy: 0.9516\n",
      "Epoch 122/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0227 - accuracy: 0.9942 - val_loss: 0.6800 - val_accuracy: 0.9508\n",
      "Epoch 123/300\n",
      "42000/42000 [==============================] - 9s 208us/step - loss: 0.0231 - accuracy: 0.9932 - val_loss: 0.6859 - val_accuracy: 0.9496\n",
      "Epoch 124/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0140 - accuracy: 0.9958 - val_loss: 0.6866 - val_accuracy: 0.9508\n",
      "Epoch 125/300\n",
      "42000/42000 [==============================] - 9s 218us/step - loss: 0.0249 - accuracy: 0.9934 - val_loss: 0.6967 - val_accuracy: 0.9463\n",
      "Epoch 126/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0163 - accuracy: 0.9946 - val_loss: 0.6775 - val_accuracy: 0.9525\n",
      "Epoch 127/300\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 0.0182 - accuracy: 0.9948 - val_loss: 0.7140 - val_accuracy: 0.9471\n",
      "Epoch 128/300\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.0198 - accuracy: 0.9940 - val_loss: 0.7319 - val_accuracy: 0.9467\n",
      "Epoch 129/300\n",
      "42000/42000 [==============================] - 9s 218us/step - loss: 0.0364 - accuracy: 0.9904 - val_loss: 0.7326 - val_accuracy: 0.9413\n",
      "Epoch 130/300\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 0.0570 - accuracy: 0.9853 - val_loss: 0.7478 - val_accuracy: 0.9388\n",
      "Epoch 131/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0321 - accuracy: 0.9903 - val_loss: 0.7057 - val_accuracy: 0.9479\n",
      "Epoch 132/300\n",
      "42000/42000 [==============================] - 9s 206us/step - loss: 0.0197 - accuracy: 0.9946 - val_loss: 0.6806 - val_accuracy: 0.9514\n",
      "Epoch 133/300\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0089 - accuracy: 0.9974 - val_loss: 0.6772 - val_accuracy: 0.9530\n",
      "Epoch 134/300\n",
      "42000/42000 [==============================] - 9s 208us/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.6738 - val_accuracy: 0.9538\n",
      "Epoch 135/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.6789 - val_accuracy: 0.9542\n",
      "Epoch 136/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.6838 - val_accuracy: 0.9552\n",
      "Epoch 137/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0013 - accuracy: 0.9996 - val_loss: 0.6983 - val_accuracy: 0.9545\n",
      "Epoch 138/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.6934 - val_accuracy: 0.9553\n",
      "Epoch 139/300\n",
      "42000/42000 [==============================] - 9s 209us/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.7012 - val_accuracy: 0.9548\n",
      "Epoch 140/300\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.0051 - accuracy: 0.9986 - val_loss: 0.7287 - val_accuracy: 0.9527\n",
      "Epoch 141/300\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 0.0264 - accuracy: 0.9941 - val_loss: 0.7973 - val_accuracy: 0.9390\n",
      "Epoch 142/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.1011 - accuracy: 0.9757 - val_loss: 0.7385 - val_accuracy: 0.9356\n",
      "Epoch 143/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0839 - accuracy: 0.9780 - val_loss: 0.7282 - val_accuracy: 0.9421\n",
      "Epoch 144/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0459 - accuracy: 0.9868 - val_loss: 0.6855 - val_accuracy: 0.9474\n",
      "Epoch 145/300\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.0252 - accuracy: 0.9930 - val_loss: 0.6999 - val_accuracy: 0.9484\n",
      "Epoch 146/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0161 - accuracy: 0.9951 - val_loss: 0.6806 - val_accuracy: 0.9511\n",
      "Epoch 147/300\n",
      "42000/42000 [==============================] - 9s 209us/step - loss: 0.0147 - accuracy: 0.9961 - val_loss: 0.6881 - val_accuracy: 0.9516\n",
      "Epoch 148/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0089 - accuracy: 0.9971 - val_loss: 0.7003 - val_accuracy: 0.9530\n",
      "Epoch 149/300\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.7033 - val_accuracy: 0.9526\n",
      "Epoch 150/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.7129 - val_accuracy: 0.9533\n",
      "Epoch 151/300\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.0106 - accuracy: 0.9967 - val_loss: 0.7140 - val_accuracy: 0.9517\n",
      "Epoch 152/300\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.0139 - accuracy: 0.9959 - val_loss: 0.7438 - val_accuracy: 0.9508\n",
      "Epoch 153/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0289 - accuracy: 0.9926 - val_loss: 0.7441 - val_accuracy: 0.9450\n",
      "Epoch 154/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0397 - accuracy: 0.9890 - val_loss: 0.7595 - val_accuracy: 0.9456\n",
      "Epoch 155/300\n",
      "42000/42000 [==============================] - 10s 226us/step - loss: 0.0491 - accuracy: 0.9869 - val_loss: 0.7709 - val_accuracy: 0.9419\n",
      "Epoch 156/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0475 - accuracy: 0.9871 - val_loss: 0.6920 - val_accuracy: 0.9503\n",
      "Epoch 157/300\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 0.0156 - accuracy: 0.9950 - val_loss: 0.7154 - val_accuracy: 0.9509\n",
      "Epoch 158/300\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.7096 - val_accuracy: 0.9521\n",
      "Epoch 159/300\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0136 - accuracy: 0.9958 - val_loss: 0.7351 - val_accuracy: 0.9504\n",
      "Epoch 160/300\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 0.0210 - accuracy: 0.9938 - val_loss: 0.7211 - val_accuracy: 0.9507\n",
      "Epoch 161/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0142 - accuracy: 0.9959 - val_loss: 0.7599 - val_accuracy: 0.9491\n",
      "Epoch 162/300\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.0244 - accuracy: 0.9934 - val_loss: 0.7518 - val_accuracy: 0.9486\n",
      "Epoch 163/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0157 - accuracy: 0.9955 - val_loss: 0.7490 - val_accuracy: 0.9495\n",
      "Epoch 164/300\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 0.0167 - accuracy: 0.9954 - val_loss: 0.7645 - val_accuracy: 0.9497\n",
      "Epoch 165/300\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 0.0224 - accuracy: 0.9931 - val_loss: 0.7762 - val_accuracy: 0.9487\n",
      "Epoch 166/300\n",
      "42000/42000 [==============================] - 9s 209us/step - loss: 0.0311 - accuracy: 0.9919 - val_loss: 0.7727 - val_accuracy: 0.9470\n",
      "Epoch 167/300\n",
      "42000/42000 [==============================] - 9s 208us/step - loss: 0.0298 - accuracy: 0.9915 - val_loss: 0.7933 - val_accuracy: 0.9462\n",
      "Epoch 168/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0213 - accuracy: 0.9940 - val_loss: 0.7536 - val_accuracy: 0.9502\n",
      "Epoch 169/300\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 0.0222 - accuracy: 0.9933 - val_loss: 0.7567 - val_accuracy: 0.9500\n",
      "Epoch 170/300\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 0.0105 - accuracy: 0.9970 - val_loss: 0.7585 - val_accuracy: 0.9537\n",
      "Epoch 171/300\n",
      "42000/42000 [==============================] - 9s 206us/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.7468 - val_accuracy: 0.9536\n",
      "Epoch 172/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.7545 - val_accuracy: 0.9534\n",
      "Epoch 173/300\n",
      "42000/42000 [==============================] - 9s 207us/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.8047 - val_accuracy: 0.9493\n",
      "Epoch 174/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0222 - accuracy: 0.9937 - val_loss: 0.8099 - val_accuracy: 0.9453\n",
      "Epoch 175/300\n",
      "42000/42000 [==============================] - 9s 206us/step - loss: 0.0459 - accuracy: 0.9881 - val_loss: 0.8023 - val_accuracy: 0.9457\n",
      "Epoch 176/300\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0550 - accuracy: 0.9861 - val_loss: 0.7963 - val_accuracy: 0.9443\n",
      "Epoch 177/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0284 - accuracy: 0.9922 - val_loss: 0.7674 - val_accuracy: 0.9513\n",
      "Epoch 178/300\n",
      "42000/42000 [==============================] - 9s 223us/step - loss: 0.0144 - accuracy: 0.9960 - val_loss: 0.7547 - val_accuracy: 0.9524\n",
      "Epoch 179/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0141 - accuracy: 0.9965 - val_loss: 0.7809 - val_accuracy: 0.9498\n",
      "Epoch 180/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0097 - accuracy: 0.9972 - val_loss: 0.7688 - val_accuracy: 0.9523\n",
      "Epoch 181/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.7847 - val_accuracy: 0.9534\n",
      "Epoch 182/300\n",
      "42000/42000 [==============================] - 9s 209us/step - loss: 0.0180 - accuracy: 0.9952 - val_loss: 0.7889 - val_accuracy: 0.9498\n",
      "Epoch 183/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0217 - accuracy: 0.9941 - val_loss: 0.8095 - val_accuracy: 0.9467\n",
      "Epoch 184/300\n",
      "42000/42000 [==============================] - 9s 207us/step - loss: 0.0278 - accuracy: 0.9923 - val_loss: 0.7921 - val_accuracy: 0.9475\n",
      "Epoch 185/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0369 - accuracy: 0.9909 - val_loss: 0.8365 - val_accuracy: 0.9457\n",
      "Epoch 186/300\n",
      "42000/42000 [==============================] - 9s 208us/step - loss: 0.0414 - accuracy: 0.9893 - val_loss: 0.8158 - val_accuracy: 0.9473\n",
      "Epoch 187/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0300 - accuracy: 0.9923 - val_loss: 0.8142 - val_accuracy: 0.9484\n",
      "Epoch 188/300\n",
      "42000/42000 [==============================] - 9s 206us/step - loss: 0.0132 - accuracy: 0.9960 - val_loss: 0.7914 - val_accuracy: 0.9512\n",
      "Epoch 189/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.7811 - val_accuracy: 0.9529\n",
      "Epoch 190/300\n",
      "42000/42000 [==============================] - 9s 209us/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.8060 - val_accuracy: 0.9521\n",
      "Epoch 191/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.8293 - val_accuracy: 0.9517\n",
      "Epoch 192/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.8296 - val_accuracy: 0.9525\n",
      "Epoch 193/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0102 - accuracy: 0.9970 - val_loss: 0.8366 - val_accuracy: 0.9493\n",
      "Epoch 194/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0162 - accuracy: 0.9956 - val_loss: 0.8415 - val_accuracy: 0.9477\n",
      "Epoch 195/300\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 0.0382 - accuracy: 0.9917 - val_loss: 0.8369 - val_accuracy: 0.9436\n",
      "Epoch 196/300\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0582 - accuracy: 0.9857 - val_loss: 0.8251 - val_accuracy: 0.9452\n",
      "Epoch 197/300\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0355 - accuracy: 0.9904 - val_loss: 0.8031 - val_accuracy: 0.9478\n",
      "Epoch 198/300\n",
      "42000/42000 [==============================] - 9s 218us/step - loss: 0.0227 - accuracy: 0.9939 - val_loss: 0.7878 - val_accuracy: 0.9512\n",
      "Epoch 199/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0207 - accuracy: 0.9943 - val_loss: 0.8336 - val_accuracy: 0.9500\n",
      "Epoch 200/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0179 - accuracy: 0.9953 - val_loss: 0.8160 - val_accuracy: 0.9503\n",
      "Epoch 201/300\n",
      "42000/42000 [==============================] - 9s 209us/step - loss: 0.0233 - accuracy: 0.9937 - val_loss: 0.8138 - val_accuracy: 0.9492\n",
      "Epoch 202/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0255 - accuracy: 0.9925 - val_loss: 0.8416 - val_accuracy: 0.9477\n",
      "Epoch 203/300\n",
      "42000/42000 [==============================] - 9s 223us/step - loss: 0.0280 - accuracy: 0.9923 - val_loss: 0.8408 - val_accuracy: 0.9487\n",
      "Epoch 204/300\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 0.0219 - accuracy: 0.9942 - val_loss: 0.8160 - val_accuracy: 0.9516\n",
      "Epoch 205/300\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0180 - accuracy: 0.9952 - val_loss: 0.8066 - val_accuracy: 0.9513\n",
      "Epoch 206/300\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0157 - accuracy: 0.9960 - val_loss: 0.8330 - val_accuracy: 0.9507\n",
      "Epoch 207/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0114 - accuracy: 0.9964 - val_loss: 0.8192 - val_accuracy: 0.9527\n",
      "Epoch 208/300\n",
      "42000/42000 [==============================] - 9s 209us/step - loss: 0.0138 - accuracy: 0.9964 - val_loss: 0.8328 - val_accuracy: 0.9501\n",
      "Epoch 209/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0159 - accuracy: 0.9956 - val_loss: 0.8209 - val_accuracy: 0.9501\n",
      "Epoch 210/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0098 - accuracy: 0.9972 - val_loss: 0.8125 - val_accuracy: 0.9525\n",
      "Epoch 211/300\n",
      "42000/42000 [==============================] - 9s 222us/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.8366 - val_accuracy: 0.9495\n",
      "Epoch 212/300\n",
      "42000/42000 [==============================] - 9s 223us/step - loss: 0.0211 - accuracy: 0.9946 - val_loss: 0.8391 - val_accuracy: 0.9501\n",
      "Epoch 213/300\n",
      "42000/42000 [==============================] - 10s 239us/step - loss: 0.0191 - accuracy: 0.9951 - val_loss: 0.8572 - val_accuracy: 0.9480\n",
      "Epoch 214/300\n",
      "42000/42000 [==============================] - 10s 242us/step - loss: 0.0304 - accuracy: 0.9929 - val_loss: 0.9546 - val_accuracy: 0.9399\n",
      "Epoch 215/300\n",
      "42000/42000 [==============================] - 10s 235us/step - loss: 0.0700 - accuracy: 0.9842 - val_loss: 0.8309 - val_accuracy: 0.9445\n",
      "Epoch 216/300\n",
      "42000/42000 [==============================] - 12s 280us/step - loss: 0.0304 - accuracy: 0.9914 - val_loss: 0.8351 - val_accuracy: 0.9494\n",
      "Epoch 217/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 10s 235us/step - loss: 0.0130 - accuracy: 0.9960 - val_loss: 0.8329 - val_accuracy: 0.9527\n",
      "Epoch 218/300\n",
      "42000/42000 [==============================] - 10s 232us/step - loss: 0.0093 - accuracy: 0.9972 - val_loss: 0.8182 - val_accuracy: 0.9521\n",
      "Epoch 219/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0119 - accuracy: 0.9965 - val_loss: 0.8158 - val_accuracy: 0.9533\n",
      "Epoch 220/300\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0067 - accuracy: 0.9978 - val_loss: 0.8393 - val_accuracy: 0.9533\n",
      "Epoch 221/300\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 0.0038 - accuracy: 0.9989 - val_loss: 0.8510 - val_accuracy: 0.9541\n",
      "Epoch 222/300\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.8425 - val_accuracy: 0.9545\n",
      "Epoch 223/300\n",
      "42000/42000 [==============================] - 9s 208us/step - loss: 6.3239e-04 - accuracy: 0.9999 - val_loss: 0.8464 - val_accuracy: 0.9545\n",
      "Epoch 224/300\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 2.2635e-04 - accuracy: 1.0000 - val_loss: 0.8493 - val_accuracy: 0.9551\n",
      "Epoch 225/300\n",
      "42000/42000 [==============================] - 9s 206us/step - loss: 1.5599e-04 - accuracy: 1.0000 - val_loss: 0.8508 - val_accuracy: 0.9552\n",
      "Epoch 226/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 1.2290e-04 - accuracy: 1.0000 - val_loss: 0.8526 - val_accuracy: 0.9552\n",
      "Epoch 227/300\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 9.8943e-05 - accuracy: 1.0000 - val_loss: 0.8552 - val_accuracy: 0.9552\n",
      "Epoch 228/300\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 8.8969e-05 - accuracy: 1.0000 - val_loss: 0.8571 - val_accuracy: 0.9554\n",
      "Epoch 229/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 7.8707e-05 - accuracy: 1.0000 - val_loss: 0.8592 - val_accuracy: 0.9553\n",
      "Epoch 230/300\n",
      "42000/42000 [==============================] - 9s 209us/step - loss: 6.7436e-05 - accuracy: 1.0000 - val_loss: 0.8605 - val_accuracy: 0.9554\n",
      "Epoch 231/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 6.3529e-05 - accuracy: 1.0000 - val_loss: 0.8623 - val_accuracy: 0.9554\n",
      "Epoch 232/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 5.7556e-05 - accuracy: 1.0000 - val_loss: 0.8640 - val_accuracy: 0.9554\n",
      "Epoch 233/300\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 5.3200e-05 - accuracy: 1.0000 - val_loss: 0.8652 - val_accuracy: 0.9555\n",
      "Epoch 234/300\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 5.0278e-05 - accuracy: 1.0000 - val_loss: 0.8669 - val_accuracy: 0.9555\n",
      "Epoch 235/300\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 4.9928e-05 - accuracy: 1.0000 - val_loss: 0.8682 - val_accuracy: 0.9555\n",
      "Epoch 236/300\n",
      "42000/42000 [==============================] - 10s 227us/step - loss: 4.4975e-05 - accuracy: 1.0000 - val_loss: 0.8696 - val_accuracy: 0.9555\n",
      "Epoch 237/300\n",
      "42000/42000 [==============================] - 10s 236us/step - loss: 4.2158e-05 - accuracy: 1.0000 - val_loss: 0.8709 - val_accuracy: 0.9554\n",
      "Epoch 238/300\n",
      "42000/42000 [==============================] - 10s 233us/step - loss: 3.9877e-05 - accuracy: 1.0000 - val_loss: 0.8721 - val_accuracy: 0.9554\n",
      "Epoch 239/300\n",
      "42000/42000 [==============================] - 10s 241us/step - loss: 3.7856e-05 - accuracy: 1.0000 - val_loss: 0.8737 - val_accuracy: 0.9555\n",
      "Epoch 240/300\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 3.5313e-05 - accuracy: 1.0000 - val_loss: 0.8748 - val_accuracy: 0.9556\n",
      "Epoch 241/300\n",
      "42000/42000 [==============================] - 9s 218us/step - loss: 3.4414e-05 - accuracy: 1.0000 - val_loss: 0.8763 - val_accuracy: 0.9555\n",
      "Epoch 242/300\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 3.3437e-05 - accuracy: 1.0000 - val_loss: 0.8772 - val_accuracy: 0.9556\n",
      "Epoch 243/300\n",
      "42000/42000 [==============================] - 10s 227us/step - loss: 3.0902e-05 - accuracy: 1.0000 - val_loss: 0.8785 - val_accuracy: 0.9556\n",
      "Epoch 244/300\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 2.9728e-05 - accuracy: 1.0000 - val_loss: 0.8800 - val_accuracy: 0.9557\n",
      "Epoch 245/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 2.8470e-05 - accuracy: 1.0000 - val_loss: 0.8808 - val_accuracy: 0.9555\n",
      "Epoch 246/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 2.6723e-05 - accuracy: 1.0000 - val_loss: 0.8819 - val_accuracy: 0.9556\n",
      "Epoch 247/300\n",
      "42000/42000 [==============================] - 9s 225us/step - loss: 2.5782e-05 - accuracy: 1.0000 - val_loss: 0.8833 - val_accuracy: 0.9556\n",
      "Epoch 248/300\n",
      "42000/42000 [==============================] - 9s 218us/step - loss: 2.5930e-05 - accuracy: 1.0000 - val_loss: 0.8843 - val_accuracy: 0.9556\n",
      "Epoch 249/300\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 2.3964e-05 - accuracy: 1.0000 - val_loss: 0.8857 - val_accuracy: 0.9556\n",
      "Epoch 250/300\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 2.2350e-05 - accuracy: 1.0000 - val_loss: 0.8868 - val_accuracy: 0.9557\n",
      "Epoch 251/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 2.1336e-05 - accuracy: 1.0000 - val_loss: 0.8881 - val_accuracy: 0.9558\n",
      "Epoch 252/300\n",
      "42000/42000 [==============================] - 9s 222us/step - loss: 2.0701e-05 - accuracy: 1.0000 - val_loss: 0.8891 - val_accuracy: 0.9557\n",
      "Epoch 253/300\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 1.9648e-05 - accuracy: 1.0000 - val_loss: 0.8901 - val_accuracy: 0.9557\n",
      "Epoch 254/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 1.9567e-05 - accuracy: 1.0000 - val_loss: 0.8920 - val_accuracy: 0.9558\n",
      "Epoch 255/300\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 2.5655e-05 - accuracy: 1.0000 - val_loss: 0.8957 - val_accuracy: 0.9558\n",
      "Epoch 256/300\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.3837 - accuracy: 0.9364 - val_loss: 0.9047 - val_accuracy: 0.8913\n",
      "Epoch 257/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.2413 - accuracy: 0.9454 - val_loss: 0.6934 - val_accuracy: 0.9366\n",
      "Epoch 258/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0640 - accuracy: 0.9824 - val_loss: 0.6607 - val_accuracy: 0.9441\n",
      "Epoch 259/300\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0273 - accuracy: 0.9916 - val_loss: 0.6806 - val_accuracy: 0.9499\n",
      "Epoch 260/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0123 - accuracy: 0.9964 - val_loss: 0.7053 - val_accuracy: 0.9538\n",
      "Epoch 261/300\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 0.7172 - val_accuracy: 0.9537\n",
      "Epoch 262/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.7297 - val_accuracy: 0.9541\n",
      "Epoch 263/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.7307 - val_accuracy: 0.9547\n",
      "Epoch 264/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.7391 - val_accuracy: 0.9552\n",
      "Epoch 265/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 9.9636e-04 - accuracy: 0.9998 - val_loss: 0.7317 - val_accuracy: 0.9545\n",
      "Epoch 266/300\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 9.0578e-04 - accuracy: 0.9999 - val_loss: 0.7468 - val_accuracy: 0.9551\n",
      "Epoch 267/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.7550 - val_accuracy: 0.9545\n",
      "Epoch 268/300\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.7686 - val_accuracy: 0.9532\n",
      "Epoch 269/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.7933 - val_accuracy: 0.9501\n",
      "Epoch 270/300\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 0.0344 - accuracy: 0.9911 - val_loss: 0.7832 - val_accuracy: 0.9431\n",
      "Epoch 271/300\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 0.0487 - accuracy: 0.9865 - val_loss: 0.7793 - val_accuracy: 0.9426\n",
      "Epoch 272/300\n",
      "42000/42000 [==============================] - 10s 231us/step - loss: 0.0440 - accuracy: 0.9874 - val_loss: 0.7754 - val_accuracy: 0.9445\n",
      "Epoch 273/300\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 0.0428 - accuracy: 0.9887 - val_loss: 0.7813 - val_accuracy: 0.9456\n",
      "Epoch 274/300\n",
      "42000/42000 [==============================] - 12s 279us/step - loss: 0.0275 - accuracy: 0.9924 - val_loss: 0.7998 - val_accuracy: 0.9482\n",
      "Epoch 275/300\n",
      "42000/42000 [==============================] - 10s 237us/step - loss: 0.0361 - accuracy: 0.9905 - val_loss: 0.7631 - val_accuracy: 0.9468\n",
      "Epoch 276/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0268 - accuracy: 0.9923 - val_loss: 0.7770 - val_accuracy: 0.9513\n",
      "Epoch 277/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0147 - accuracy: 0.9954 - val_loss: 0.7952 - val_accuracy: 0.9502\n",
      "Epoch 278/300\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.8160 - val_accuracy: 0.9511\n",
      "Epoch 279/300\n",
      "42000/42000 [==============================] - 9s 225us/step - loss: 0.0143 - accuracy: 0.9959 - val_loss: 0.8132 - val_accuracy: 0.9511\n",
      "Epoch 280/300\n",
      "42000/42000 [==============================] - 9s 222us/step - loss: 0.0115 - accuracy: 0.9963 - val_loss: 0.8058 - val_accuracy: 0.9506\n",
      "Epoch 281/300\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0097 - accuracy: 0.9974 - val_loss: 0.8192 - val_accuracy: 0.9533\n",
      "Epoch 282/300\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.8215 - val_accuracy: 0.9531\n",
      "Epoch 283/300\n",
      "42000/42000 [==============================] - 9s 224us/step - loss: 0.0073 - accuracy: 0.9980 - val_loss: 0.8322 - val_accuracy: 0.9523\n",
      "Epoch 284/300\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.8398 - val_accuracy: 0.9527\n",
      "Epoch 285/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0134 - accuracy: 0.9963 - val_loss: 0.8416 - val_accuracy: 0.9496\n",
      "Epoch 286/300\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.0265 - accuracy: 0.9932 - val_loss: 0.8385 - val_accuracy: 0.9467\n",
      "Epoch 287/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0274 - accuracy: 0.9926 - val_loss: 0.8497 - val_accuracy: 0.9466\n",
      "Epoch 288/300\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.0261 - accuracy: 0.9931 - val_loss: 0.8136 - val_accuracy: 0.9495\n",
      "Epoch 289/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0343 - accuracy: 0.9910 - val_loss: 0.8398 - val_accuracy: 0.9464\n",
      "Epoch 290/300\n",
      "42000/42000 [==============================] - 9s 218us/step - loss: 0.0367 - accuracy: 0.9910 - val_loss: 0.8509 - val_accuracy: 0.9467\n",
      "Epoch 291/300\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0203 - accuracy: 0.9945 - val_loss: 0.8504 - val_accuracy: 0.9517\n",
      "Epoch 292/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0125 - accuracy: 0.9967 - val_loss: 0.8158 - val_accuracy: 0.9524\n",
      "Epoch 293/300\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0106 - accuracy: 0.9972 - val_loss: 0.8712 - val_accuracy: 0.9517\n",
      "Epoch 294/300\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0161 - accuracy: 0.9958 - val_loss: 0.8602 - val_accuracy: 0.9514\n",
      "Epoch 295/300\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0144 - accuracy: 0.9956 - val_loss: 0.8463 - val_accuracy: 0.9517\n",
      "Epoch 296/300\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0139 - accuracy: 0.9964 - val_loss: 0.8520 - val_accuracy: 0.9509\n",
      "Epoch 297/300\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.0225 - accuracy: 0.9939 - val_loss: 0.8729 - val_accuracy: 0.9485\n",
      "Epoch 298/300\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0243 - accuracy: 0.9934 - val_loss: 0.8663 - val_accuracy: 0.9515\n",
      "Epoch 299/300\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.0259 - accuracy: 0.9933 - val_loss: 0.8805 - val_accuracy: 0.9495\n",
      "Epoch 300/300\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0283 - accuracy: 0.9929 - val_loss: 0.9245 - val_accuracy: 0.9453\n"
     ]
    }
   ],
   "source": [
    "# now fit the model\n",
    "model2_keras_result_1=model2_keras.fit(X_train_new, y_train_new,           \n",
    "          validation_data=(X_val_new,y_val_new),\n",
    "          epochs=300,batch_size=500,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "18000/18000 [==============================] - 1s 35us/step\n",
      "test acc: 0.8378333449363708\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model  on test data\n",
    "\n",
    "evaluateModel(model2_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/300\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 1.4483 - accuracy: 0.5776 - val_loss: 1.4443 - val_accuracy: 0.5778\n",
      "Epoch 2/300\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.4427 - accuracy: 0.5788 - val_loss: 1.4396 - val_accuracy: 0.5822\n",
      "Epoch 3/300\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.4380 - accuracy: 0.5820 - val_loss: 1.4350 - val_accuracy: 0.5801\n",
      "Epoch 4/300\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.4333 - accuracy: 0.5831 - val_loss: 1.4301 - val_accuracy: 0.5843\n",
      "Epoch 5/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.4285 - accuracy: 0.5849 - val_loss: 1.4254 - val_accuracy: 0.5859\n",
      "Epoch 6/300\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.4239 - accuracy: 0.5864 - val_loss: 1.4208 - val_accuracy: 0.5900\n",
      "Epoch 7/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.4194 - accuracy: 0.5895 - val_loss: 1.4163 - val_accuracy: 0.5884\n",
      "Epoch 8/300\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.4147 - accuracy: 0.5893 - val_loss: 1.4116 - val_accuracy: 0.5938\n",
      "Epoch 9/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.4102 - accuracy: 0.5925 - val_loss: 1.4071 - val_accuracy: 0.5932\n",
      "Epoch 10/300\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.4056 - accuracy: 0.5939 - val_loss: 1.4024 - val_accuracy: 0.5952\n",
      "Epoch 11/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.4011 - accuracy: 0.5949 - val_loss: 1.3980 - val_accuracy: 0.5980\n",
      "Epoch 12/300\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.3966 - accuracy: 0.5967 - val_loss: 1.3936 - val_accuracy: 0.5954\n",
      "Epoch 13/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.3920 - accuracy: 0.5980 - val_loss: 1.3893 - val_accuracy: 0.6000\n",
      "Epoch 14/300\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.3877 - accuracy: 0.5987 - val_loss: 1.3846 - val_accuracy: 0.6022\n",
      "Epoch 15/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.3832 - accuracy: 0.6003 - val_loss: 1.3804 - val_accuracy: 0.6037\n",
      "Epoch 16/300\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.3788 - accuracy: 0.6020 - val_loss: 1.3761 - val_accuracy: 0.6068\n",
      "Epoch 17/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.3745 - accuracy: 0.6049 - val_loss: 1.3717 - val_accuracy: 0.6051\n",
      "Epoch 18/300\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.3701 - accuracy: 0.6049 - val_loss: 1.3675 - val_accuracy: 0.6068\n",
      "Epoch 19/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.3659 - accuracy: 0.6069 - val_loss: 1.3631 - val_accuracy: 0.6067\n",
      "Epoch 20/300\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.3616 - accuracy: 0.6078 - val_loss: 1.3588 - val_accuracy: 0.6096\n",
      "Epoch 21/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.3573 - accuracy: 0.6100 - val_loss: 1.3547 - val_accuracy: 0.6091\n",
      "Epoch 22/300\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.3531 - accuracy: 0.6113 - val_loss: 1.3505 - val_accuracy: 0.6106\n",
      "Epoch 23/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.3489 - accuracy: 0.6114 - val_loss: 1.3462 - val_accuracy: 0.6123\n",
      "Epoch 24/300\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 1.3447 - accuracy: 0.6134 - val_loss: 1.3421 - val_accuracy: 0.6133\n",
      "Epoch 25/300\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 1.3406 - accuracy: 0.6137 - val_loss: 1.3380 - val_accuracy: 0.6162\n",
      "Epoch 26/300\n",
      "42000/42000 [==============================] - 8s 201us/step - loss: 1.3364 - accuracy: 0.6164 - val_loss: 1.3338 - val_accuracy: 0.6147\n",
      "Epoch 27/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.3324 - accuracy: 0.6165 - val_loss: 1.3299 - val_accuracy: 0.6170\n",
      "Epoch 28/300\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.3284 - accuracy: 0.6177 - val_loss: 1.3258 - val_accuracy: 0.6197\n",
      "Epoch 29/300\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.3242 - accuracy: 0.6195 - val_loss: 1.3220 - val_accuracy: 0.6201\n",
      "Epoch 30/300\n",
      "42000/42000 [==============================] - 8s 186us/step - loss: 1.3203 - accuracy: 0.6199 - val_loss: 1.3180 - val_accuracy: 0.6184\n",
      "Epoch 31/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.3164 - accuracy: 0.6211 - val_loss: 1.3140 - val_accuracy: 0.6221\n",
      "Epoch 32/300\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.3124 - accuracy: 0.6223 - val_loss: 1.3101 - val_accuracy: 0.6217\n",
      "Epoch 33/300\n",
      "42000/42000 [==============================] - 8s 195us/step - loss: 1.3086 - accuracy: 0.6226 - val_loss: 1.3063 - val_accuracy: 0.6240\n",
      "Epoch 34/300\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.3047 - accuracy: 0.6247 - val_loss: 1.3024 - val_accuracy: 0.6257\n",
      "Epoch 35/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.3008 - accuracy: 0.6247 - val_loss: 1.2988 - val_accuracy: 0.6277\n",
      "Epoch 36/300\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.2969 - accuracy: 0.6270 - val_loss: 1.2949 - val_accuracy: 0.6263\n",
      "Epoch 37/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.2933 - accuracy: 0.6274 - val_loss: 1.2910 - val_accuracy: 0.6275\n",
      "Epoch 38/300\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.2896 - accuracy: 0.6277 - val_loss: 1.2874 - val_accuracy: 0.6313\n",
      "Epoch 39/300\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.2858 - accuracy: 0.6300 - val_loss: 1.2836 - val_accuracy: 0.6299\n",
      "Epoch 40/300\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.2821 - accuracy: 0.6300 - val_loss: 1.2800 - val_accuracy: 0.6313\n",
      "Epoch 41/300\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.2784 - accuracy: 0.6309 - val_loss: 1.2765 - val_accuracy: 0.6322\n",
      "Epoch 42/300\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.2748 - accuracy: 0.6321 - val_loss: 1.2729 - val_accuracy: 0.6332\n",
      "Epoch 43/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.2713 - accuracy: 0.6337 - val_loss: 1.2693 - val_accuracy: 0.6335\n",
      "Epoch 44/300\n",
      "42000/42000 [==============================] - 8s 185us/step - loss: 1.2677 - accuracy: 0.6338 - val_loss: 1.2658 - val_accuracy: 0.6345\n",
      "Epoch 45/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.2641 - accuracy: 0.6354 - val_loss: 1.2624 - val_accuracy: 0.6357\n",
      "Epoch 46/300\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.2607 - accuracy: 0.6359 - val_loss: 1.2589 - val_accuracy: 0.6359\n",
      "Epoch 47/300\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.2572 - accuracy: 0.6370 - val_loss: 1.2555 - val_accuracy: 0.6388\n",
      "Epoch 48/300\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.2538 - accuracy: 0.6380 - val_loss: 1.2521 - val_accuracy: 0.6395\n",
      "Epoch 49/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.2504 - accuracy: 0.6387 - val_loss: 1.2485 - val_accuracy: 0.6398\n",
      "Epoch 50/300\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.2471 - accuracy: 0.6394 - val_loss: 1.2453 - val_accuracy: 0.6411\n",
      "Epoch 51/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.2437 - accuracy: 0.6408 - val_loss: 1.2419 - val_accuracy: 0.6414\n",
      "Epoch 52/300\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.2404 - accuracy: 0.6417 - val_loss: 1.2387 - val_accuracy: 0.6425\n",
      "Epoch 53/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.2370 - accuracy: 0.6423 - val_loss: 1.2354 - val_accuracy: 0.6439\n",
      "Epoch 54/300\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.2338 - accuracy: 0.6435 - val_loss: 1.2323 - val_accuracy: 0.6435\n",
      "Epoch 55/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 1.2305 - accuracy: 0.6436 - val_loss: 1.2292 - val_accuracy: 0.6443\n",
      "Epoch 56/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.2274 - accuracy: 0.6450 - val_loss: 1.2259 - val_accuracy: 0.6451\n",
      "Epoch 57/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.2241 - accuracy: 0.6459 - val_loss: 1.2230 - val_accuracy: 0.6446\n",
      "Epoch 58/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.2211 - accuracy: 0.6458 - val_loss: 1.2197 - val_accuracy: 0.6468\n",
      "Epoch 59/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 1.2179 - accuracy: 0.6468 - val_loss: 1.2166 - val_accuracy: 0.6474\n",
      "Epoch 60/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.2149 - accuracy: 0.6477 - val_loss: 1.2136 - val_accuracy: 0.6488\n",
      "Epoch 61/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 1.2118 - accuracy: 0.6487 - val_loss: 1.2105 - val_accuracy: 0.6500\n",
      "Epoch 62/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.2088 - accuracy: 0.6498 - val_loss: 1.2075 - val_accuracy: 0.6490\n",
      "Epoch 63/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 1.2058 - accuracy: 0.6503 - val_loss: 1.2047 - val_accuracy: 0.6499\n",
      "Epoch 64/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.2029 - accuracy: 0.6506 - val_loss: 1.2017 - val_accuracy: 0.6517\n",
      "Epoch 65/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 1.1999 - accuracy: 0.6519 - val_loss: 1.1987 - val_accuracy: 0.6516\n",
      "Epoch 66/300\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.1969 - accuracy: 0.6516 - val_loss: 1.1960 - val_accuracy: 0.6535\n",
      "Epoch 67/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 1.1941 - accuracy: 0.6538 - val_loss: 1.1930 - val_accuracy: 0.6529\n",
      "Epoch 68/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1912 - accuracy: 0.6540 - val_loss: 1.1902 - val_accuracy: 0.6537\n",
      "Epoch 69/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 1.1884 - accuracy: 0.6546 - val_loss: 1.1874 - val_accuracy: 0.6549\n",
      "Epoch 70/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1855 - accuracy: 0.6548 - val_loss: 1.1849 - val_accuracy: 0.6539\n",
      "Epoch 71/300\n",
      "42000/42000 [==============================] - 7s 170us/step - loss: 1.1828 - accuracy: 0.6555 - val_loss: 1.1818 - val_accuracy: 0.6561\n",
      "Epoch 72/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1801 - accuracy: 0.6565 - val_loss: 1.1791 - val_accuracy: 0.6562\n",
      "Epoch 73/300\n",
      "42000/42000 [==============================] - 7s 170us/step - loss: 1.1774 - accuracy: 0.6573 - val_loss: 1.1766 - val_accuracy: 0.6579\n",
      "Epoch 74/300\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.1746 - accuracy: 0.6582 - val_loss: 1.1738 - val_accuracy: 0.6578\n",
      "Epoch 75/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 1.1719 - accuracy: 0.6585 - val_loss: 1.1711 - val_accuracy: 0.6583\n",
      "Epoch 76/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 1.1693 - accuracy: 0.6593 - val_loss: 1.1685 - val_accuracy: 0.6594\n",
      "Epoch 77/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 1.1666 - accuracy: 0.6603 - val_loss: 1.1660 - val_accuracy: 0.6602\n",
      "Epoch 78/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1640 - accuracy: 0.6609 - val_loss: 1.1635 - val_accuracy: 0.6601\n",
      "Epoch 79/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 1.1614 - accuracy: 0.6612 - val_loss: 1.1608 - val_accuracy: 0.6614\n",
      "Epoch 80/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.1588 - accuracy: 0.6630 - val_loss: 1.1584 - val_accuracy: 0.6618\n",
      "Epoch 81/300\n",
      "42000/42000 [==============================] - 7s 170us/step - loss: 1.1562 - accuracy: 0.6629 - val_loss: 1.1557 - val_accuracy: 0.6626\n",
      "Epoch 82/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.1537 - accuracy: 0.6632 - val_loss: 1.1533 - val_accuracy: 0.6636\n",
      "Epoch 83/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 1.1513 - accuracy: 0.6644 - val_loss: 1.1508 - val_accuracy: 0.6639\n",
      "Epoch 84/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1488 - accuracy: 0.6648 - val_loss: 1.1484 - val_accuracy: 0.6645\n",
      "Epoch 85/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 1.1464 - accuracy: 0.6649 - val_loss: 1.1459 - val_accuracy: 0.6657\n",
      "Epoch 86/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1439 - accuracy: 0.6661 - val_loss: 1.1435 - val_accuracy: 0.6662\n",
      "Epoch 87/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 1.1415 - accuracy: 0.6671 - val_loss: 1.1412 - val_accuracy: 0.6669\n",
      "Epoch 88/300\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 1.1391 - accuracy: 0.6679 - val_loss: 1.1388 - val_accuracy: 0.6670\n",
      "Epoch 89/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1368 - accuracy: 0.6686 - val_loss: 1.1364 - val_accuracy: 0.6671\n",
      "Epoch 90/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.1344 - accuracy: 0.6686 - val_loss: 1.1344 - val_accuracy: 0.6693\n",
      "Epoch 91/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 1.1320 - accuracy: 0.6695 - val_loss: 1.1319 - val_accuracy: 0.6695\n",
      "Epoch 92/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.1298 - accuracy: 0.6704 - val_loss: 1.1296 - val_accuracy: 0.6696\n",
      "Epoch 93/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 1.1275 - accuracy: 0.6709 - val_loss: 1.1273 - val_accuracy: 0.6705\n",
      "Epoch 94/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.1252 - accuracy: 0.6708 - val_loss: 1.1253 - val_accuracy: 0.6706\n",
      "Epoch 95/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 1.1230 - accuracy: 0.6723 - val_loss: 1.1229 - val_accuracy: 0.6715\n",
      "Epoch 96/300\n",
      "42000/42000 [==============================] - 8s 191us/step - loss: 1.1207 - accuracy: 0.6720 - val_loss: 1.1207 - val_accuracy: 0.6719\n",
      "Epoch 97/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 1.1185 - accuracy: 0.6732 - val_loss: 1.1187 - val_accuracy: 0.6725\n",
      "Epoch 98/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 1.1163 - accuracy: 0.6742 - val_loss: 1.1164 - val_accuracy: 0.6737\n",
      "Epoch 99/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.1141 - accuracy: 0.6744 - val_loss: 1.1144 - val_accuracy: 0.6733\n",
      "Epoch 100/300\n",
      "42000/42000 [==============================] - 7s 179us/step - loss: 1.1120 - accuracy: 0.6751 - val_loss: 1.1122 - val_accuracy: 0.6737\n",
      "Epoch 101/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 1.1098 - accuracy: 0.6748 - val_loss: 1.1100 - val_accuracy: 0.6745\n",
      "Epoch 102/300\n",
      "42000/42000 [==============================] - 7s 179us/step - loss: 1.1077 - accuracy: 0.6757 - val_loss: 1.1079 - val_accuracy: 0.6748\n",
      "Epoch 103/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 1.1056 - accuracy: 0.6760 - val_loss: 1.1058 - val_accuracy: 0.6755\n",
      "Epoch 104/300\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 1.1035 - accuracy: 0.6777 - val_loss: 1.1038 - val_accuracy: 0.6762\n",
      "Epoch 105/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 1.1014 - accuracy: 0.6774 - val_loss: 1.1018 - val_accuracy: 0.6767\n",
      "Epoch 106/300\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.0994 - accuracy: 0.6779 - val_loss: 1.0998 - val_accuracy: 0.6769\n",
      "Epoch 107/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.0974 - accuracy: 0.6785 - val_loss: 1.0978 - val_accuracy: 0.6784\n",
      "Epoch 108/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.0954 - accuracy: 0.6788 - val_loss: 1.0958 - val_accuracy: 0.6776\n",
      "Epoch 109/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 7s 172us/step - loss: 1.0933 - accuracy: 0.6789 - val_loss: 1.0938 - val_accuracy: 0.6787\n",
      "Epoch 110/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 1.0913 - accuracy: 0.6799 - val_loss: 1.0919 - val_accuracy: 0.6796\n",
      "Epoch 111/300\n",
      "42000/42000 [==============================] - 7s 169us/step - loss: 1.0894 - accuracy: 0.6805 - val_loss: 1.0900 - val_accuracy: 0.6797\n",
      "Epoch 112/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.0875 - accuracy: 0.6808 - val_loss: 1.0880 - val_accuracy: 0.6800\n",
      "Epoch 113/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.0855 - accuracy: 0.6816 - val_loss: 1.0861 - val_accuracy: 0.6806\n",
      "Epoch 114/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 1.0836 - accuracy: 0.6816 - val_loss: 1.0842 - val_accuracy: 0.6811\n",
      "Epoch 115/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 1.0816 - accuracy: 0.6832 - val_loss: 1.0824 - val_accuracy: 0.6815\n",
      "Epoch 116/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 1.0798 - accuracy: 0.6826 - val_loss: 1.0805 - val_accuracy: 0.6821\n",
      "Epoch 117/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 1.0779 - accuracy: 0.6833 - val_loss: 1.0787 - val_accuracy: 0.6826\n",
      "Epoch 118/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.0759 - accuracy: 0.6840 - val_loss: 1.0771 - val_accuracy: 0.6817\n",
      "Epoch 119/300\n",
      "42000/42000 [==============================] - 7s 170us/step - loss: 1.0742 - accuracy: 0.6838 - val_loss: 1.0751 - val_accuracy: 0.6834\n",
      "Epoch 120/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.0724 - accuracy: 0.6847 - val_loss: 1.0733 - val_accuracy: 0.6834\n",
      "Epoch 121/300\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 1.0705 - accuracy: 0.6855 - val_loss: 1.0714 - val_accuracy: 0.6841\n",
      "Epoch 122/300\n",
      "42000/42000 [==============================] - 8s 187us/step - loss: 1.0687 - accuracy: 0.6859 - val_loss: 1.0697 - val_accuracy: 0.6851\n",
      "Epoch 123/300\n",
      "42000/42000 [==============================] - 8s 200us/step - loss: 1.0669 - accuracy: 0.6860 - val_loss: 1.0680 - val_accuracy: 0.6854\n",
      "Epoch 124/300\n",
      "42000/42000 [==============================] - 8s 201us/step - loss: 1.0651 - accuracy: 0.6864 - val_loss: 1.0664 - val_accuracy: 0.6851\n",
      "Epoch 125/300\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 1.0634 - accuracy: 0.6867 - val_loss: 1.0644 - val_accuracy: 0.6864\n",
      "Epoch 126/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 1.0615 - accuracy: 0.6873 - val_loss: 1.0630 - val_accuracy: 0.6862\n",
      "Epoch 127/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.0599 - accuracy: 0.6877 - val_loss: 1.0611 - val_accuracy: 0.6876\n",
      "Epoch 128/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 1.0582 - accuracy: 0.6886 - val_loss: 1.0594 - val_accuracy: 0.6867\n",
      "Epoch 129/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.0564 - accuracy: 0.6883 - val_loss: 1.0577 - val_accuracy: 0.6877\n",
      "Epoch 130/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 1.0548 - accuracy: 0.6894 - val_loss: 1.0560 - val_accuracy: 0.6882\n",
      "Epoch 131/300\n",
      "42000/42000 [==============================] - 8s 182us/step - loss: 1.0531 - accuracy: 0.6898 - val_loss: 1.0544 - val_accuracy: 0.6890\n",
      "Epoch 132/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 1.0513 - accuracy: 0.6899 - val_loss: 1.0527 - val_accuracy: 0.6894\n",
      "Epoch 133/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.0497 - accuracy: 0.6906 - val_loss: 1.0512 - val_accuracy: 0.6898\n",
      "Epoch 134/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 1.0480 - accuracy: 0.6911 - val_loss: 1.0497 - val_accuracy: 0.6898\n",
      "Epoch 135/300\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.0463 - accuracy: 0.6919 - val_loss: 1.0479 - val_accuracy: 0.6904\n",
      "Epoch 136/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 1.0448 - accuracy: 0.6925 - val_loss: 1.0464 - val_accuracy: 0.6906\n",
      "Epoch 137/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.0431 - accuracy: 0.6923 - val_loss: 1.0448 - val_accuracy: 0.6915\n",
      "Epoch 138/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 1.0415 - accuracy: 0.6926 - val_loss: 1.0433 - val_accuracy: 0.6917\n",
      "Epoch 139/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.0400 - accuracy: 0.6933 - val_loss: 1.0417 - val_accuracy: 0.6916\n",
      "Epoch 140/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 1.0384 - accuracy: 0.6938 - val_loss: 1.0401 - val_accuracy: 0.6927\n",
      "Epoch 141/300\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.0368 - accuracy: 0.6938 - val_loss: 1.0386 - val_accuracy: 0.6932\n",
      "Epoch 142/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 1.0352 - accuracy: 0.6946 - val_loss: 1.0372 - val_accuracy: 0.6929\n",
      "Epoch 143/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.0338 - accuracy: 0.6948 - val_loss: 1.0355 - val_accuracy: 0.6936\n",
      "Epoch 144/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 1.0321 - accuracy: 0.6949 - val_loss: 1.0340 - val_accuracy: 0.6942\n",
      "Epoch 145/300\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.0307 - accuracy: 0.6960 - val_loss: 1.0326 - val_accuracy: 0.6944\n",
      "Epoch 146/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 1.0291 - accuracy: 0.6958 - val_loss: 1.0312 - val_accuracy: 0.6949\n",
      "Epoch 147/300\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.0276 - accuracy: 0.6970 - val_loss: 1.0297 - val_accuracy: 0.6951\n",
      "Epoch 148/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 1.0262 - accuracy: 0.6967 - val_loss: 1.0281 - val_accuracy: 0.6957\n",
      "Epoch 149/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.0246 - accuracy: 0.6974 - val_loss: 1.0268 - val_accuracy: 0.6965\n",
      "Epoch 150/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 1.0232 - accuracy: 0.6985 - val_loss: 1.0253 - val_accuracy: 0.6956\n",
      "Epoch 151/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.0218 - accuracy: 0.6979 - val_loss: 1.0238 - val_accuracy: 0.6966\n",
      "Epoch 152/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 1.0203 - accuracy: 0.6979 - val_loss: 1.0224 - val_accuracy: 0.6969\n",
      "Epoch 153/300\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 1.0188 - accuracy: 0.6988 - val_loss: 1.0211 - val_accuracy: 0.6977\n",
      "Epoch 154/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 1.0174 - accuracy: 0.6994 - val_loss: 1.0197 - val_accuracy: 0.6981\n",
      "Epoch 155/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 1.0159 - accuracy: 0.6997 - val_loss: 1.0184 - val_accuracy: 0.6979\n",
      "Epoch 156/300\n",
      "42000/42000 [==============================] - 9s 203us/step - loss: 1.0146 - accuracy: 0.7001 - val_loss: 1.0169 - val_accuracy: 0.6987\n",
      "Epoch 157/300\n",
      "42000/42000 [==============================] - 8s 198us/step - loss: 1.0132 - accuracy: 0.7008 - val_loss: 1.0156 - val_accuracy: 0.6988\n",
      "Epoch 158/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 1.0117 - accuracy: 0.7011 - val_loss: 1.0142 - val_accuracy: 0.6996\n",
      "Epoch 159/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 1.0104 - accuracy: 0.7011 - val_loss: 1.0130 - val_accuracy: 0.7005\n",
      "Epoch 160/300\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.0089 - accuracy: 0.7020 - val_loss: 1.0118 - val_accuracy: 0.7001\n",
      "Epoch 161/300\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 1.0077 - accuracy: 0.7013 - val_loss: 1.0102 - val_accuracy: 0.7004\n",
      "Epoch 162/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 1.0062 - accuracy: 0.7020 - val_loss: 1.0089 - val_accuracy: 0.7010\n",
      "Epoch 163/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 1.0048 - accuracy: 0.7031 - val_loss: 1.0076 - val_accuracy: 0.7007\n",
      "Epoch 164/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 1.0036 - accuracy: 0.7024 - val_loss: 1.0062 - val_accuracy: 0.7012\n",
      "Epoch 165/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 1.0022 - accuracy: 0.7031 - val_loss: 1.0049 - val_accuracy: 0.7020\n",
      "Epoch 166/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 1.0008 - accuracy: 0.7036 - val_loss: 1.0036 - val_accuracy: 0.7020\n",
      "Epoch 167/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.9995 - accuracy: 0.7044 - val_loss: 1.0025 - val_accuracy: 0.7016\n",
      "Epoch 168/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 0.9983 - accuracy: 0.7034 - val_loss: 1.0011 - val_accuracy: 0.7031\n",
      "Epoch 169/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.9970 - accuracy: 0.7047 - val_loss: 0.9998 - val_accuracy: 0.7033\n",
      "Epoch 170/300\n",
      "42000/42000 [==============================] - 7s 169us/step - loss: 0.9957 - accuracy: 0.7048 - val_loss: 0.9985 - val_accuracy: 0.7034\n",
      "Epoch 171/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 0.9944 - accuracy: 0.7058 - val_loss: 0.9973 - val_accuracy: 0.7035\n",
      "Epoch 172/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 0.9931 - accuracy: 0.7059 - val_loss: 0.9963 - val_accuracy: 0.7038\n",
      "Epoch 173/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 0.9919 - accuracy: 0.7061 - val_loss: 0.9948 - val_accuracy: 0.7042\n",
      "Epoch 174/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.9906 - accuracy: 0.7064 - val_loss: 0.9936 - val_accuracy: 0.7046\n",
      "Epoch 175/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 0.9893 - accuracy: 0.7064 - val_loss: 0.9924 - val_accuracy: 0.7048\n",
      "Epoch 176/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 0.9880 - accuracy: 0.7067 - val_loss: 0.9913 - val_accuracy: 0.7053\n",
      "Epoch 177/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.9868 - accuracy: 0.7073 - val_loss: 0.9900 - val_accuracy: 0.7051\n",
      "Epoch 178/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 0.9856 - accuracy: 0.7074 - val_loss: 0.9889 - val_accuracy: 0.7054\n",
      "Epoch 179/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.9845 - accuracy: 0.7079 - val_loss: 0.9877 - val_accuracy: 0.7062\n",
      "Epoch 180/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 0.9832 - accuracy: 0.7078 - val_loss: 0.9864 - val_accuracy: 0.7060\n",
      "Epoch 181/300\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.9820 - accuracy: 0.7079 - val_loss: 0.9853 - val_accuracy: 0.7064\n",
      "Epoch 182/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.9808 - accuracy: 0.7084 - val_loss: 0.9841 - val_accuracy: 0.7068\n",
      "Epoch 183/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.9796 - accuracy: 0.7090 - val_loss: 0.9830 - val_accuracy: 0.7065\n",
      "Epoch 184/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 0.9784 - accuracy: 0.7087 - val_loss: 0.9817 - val_accuracy: 0.7075\n",
      "Epoch 185/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 0.9772 - accuracy: 0.7090 - val_loss: 0.9807 - val_accuracy: 0.7076\n",
      "Epoch 186/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 0.9760 - accuracy: 0.7099 - val_loss: 0.9795 - val_accuracy: 0.7080\n",
      "Epoch 187/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.9749 - accuracy: 0.7103 - val_loss: 0.9784 - val_accuracy: 0.7079\n",
      "Epoch 188/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 0.9737 - accuracy: 0.7105 - val_loss: 0.9773 - val_accuracy: 0.7085\n",
      "Epoch 189/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 0.9726 - accuracy: 0.7106 - val_loss: 0.9762 - val_accuracy: 0.7087\n",
      "Epoch 190/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.9714 - accuracy: 0.7108 - val_loss: 0.9750 - val_accuracy: 0.7088\n",
      "Epoch 191/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 0.9702 - accuracy: 0.7110 - val_loss: 0.9739 - val_accuracy: 0.7089\n",
      "Epoch 192/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 0.9691 - accuracy: 0.7116 - val_loss: 0.9728 - val_accuracy: 0.7092\n",
      "Epoch 193/300\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.9679 - accuracy: 0.7116 - val_loss: 0.9719 - val_accuracy: 0.7093\n",
      "Epoch 194/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 0.9669 - accuracy: 0.7121 - val_loss: 0.9707 - val_accuracy: 0.7098\n",
      "Epoch 195/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.9657 - accuracy: 0.7127 - val_loss: 0.9696 - val_accuracy: 0.7104\n",
      "Epoch 196/300\n",
      "42000/42000 [==============================] - 7s 169us/step - loss: 0.9646 - accuracy: 0.7123 - val_loss: 0.9685 - val_accuracy: 0.7108\n",
      "Epoch 197/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.9635 - accuracy: 0.7130 - val_loss: 0.9674 - val_accuracy: 0.7103\n",
      "Epoch 198/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 0.9624 - accuracy: 0.7128 - val_loss: 0.9663 - val_accuracy: 0.7107\n",
      "Epoch 199/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.9613 - accuracy: 0.7131 - val_loss: 0.9653 - val_accuracy: 0.7114\n",
      "Epoch 200/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 0.9601 - accuracy: 0.7132 - val_loss: 0.9645 - val_accuracy: 0.7104\n",
      "Epoch 201/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.9592 - accuracy: 0.7138 - val_loss: 0.9632 - val_accuracy: 0.7114\n",
      "Epoch 202/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 0.9580 - accuracy: 0.7143 - val_loss: 0.9623 - val_accuracy: 0.7117\n",
      "Epoch 203/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.9570 - accuracy: 0.7145 - val_loss: 0.9613 - val_accuracy: 0.7119\n",
      "Epoch 204/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 0.9559 - accuracy: 0.7143 - val_loss: 0.9601 - val_accuracy: 0.7123\n",
      "Epoch 205/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.9548 - accuracy: 0.7147 - val_loss: 0.9592 - val_accuracy: 0.7124\n",
      "Epoch 206/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 0.9538 - accuracy: 0.7148 - val_loss: 0.9581 - val_accuracy: 0.7128\n",
      "Epoch 207/300\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.9527 - accuracy: 0.7155 - val_loss: 0.9572 - val_accuracy: 0.7131\n",
      "Epoch 208/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 0.9517 - accuracy: 0.7159 - val_loss: 0.9561 - val_accuracy: 0.7132\n",
      "Epoch 209/300\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.9507 - accuracy: 0.7159 - val_loss: 0.9551 - val_accuracy: 0.7136\n",
      "Epoch 210/300\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 0.9496 - accuracy: 0.7167 - val_loss: 0.9541 - val_accuracy: 0.7138\n",
      "Epoch 211/300\n",
      "42000/42000 [==============================] - 8s 193us/step - loss: 0.9486 - accuracy: 0.7164 - val_loss: 0.9531 - val_accuracy: 0.7142\n",
      "Epoch 212/300\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 0.9476 - accuracy: 0.7170 - val_loss: 0.9521 - val_accuracy: 0.7144\n",
      "Epoch 213/300\n",
      "42000/42000 [==============================] - 8s 190us/step - loss: 0.9465 - accuracy: 0.7174 - val_loss: 0.9511 - val_accuracy: 0.7144\n",
      "Epoch 214/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.9454 - accuracy: 0.7171 - val_loss: 0.9503 - val_accuracy: 0.7155\n",
      "Epoch 215/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 0.9445 - accuracy: 0.7182 - val_loss: 0.9490 - val_accuracy: 0.7149\n",
      "Epoch 216/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 0.9435 - accuracy: 0.7182 - val_loss: 0.9481 - val_accuracy: 0.7152\n",
      "Epoch 217/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42000/42000 [==============================] - 7s 174us/step - loss: 0.9425 - accuracy: 0.7185 - val_loss: 0.9473 - val_accuracy: 0.7155\n",
      "Epoch 218/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 0.9415 - accuracy: 0.7186 - val_loss: 0.9463 - val_accuracy: 0.7158\n",
      "Epoch 219/300\n",
      "42000/42000 [==============================] - 7s 170us/step - loss: 0.9404 - accuracy: 0.7192 - val_loss: 0.9455 - val_accuracy: 0.7157\n",
      "Epoch 220/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.9395 - accuracy: 0.7187 - val_loss: 0.9444 - val_accuracy: 0.7167\n",
      "Epoch 221/300\n",
      "42000/42000 [==============================] - 7s 169us/step - loss: 0.9385 - accuracy: 0.7190 - val_loss: 0.9434 - val_accuracy: 0.7169\n",
      "Epoch 222/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.9375 - accuracy: 0.7196 - val_loss: 0.9424 - val_accuracy: 0.7171\n",
      "Epoch 223/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 0.9365 - accuracy: 0.7201 - val_loss: 0.9416 - val_accuracy: 0.7167\n",
      "Epoch 224/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.9356 - accuracy: 0.7198 - val_loss: 0.9406 - val_accuracy: 0.7171\n",
      "Epoch 225/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 0.9347 - accuracy: 0.7199 - val_loss: 0.9397 - val_accuracy: 0.7179\n",
      "Epoch 226/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 0.9336 - accuracy: 0.7204 - val_loss: 0.9390 - val_accuracy: 0.7182\n",
      "Epoch 227/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 0.9327 - accuracy: 0.7205 - val_loss: 0.9378 - val_accuracy: 0.7183\n",
      "Epoch 228/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.9318 - accuracy: 0.7209 - val_loss: 0.9370 - val_accuracy: 0.7182\n",
      "Epoch 229/300\n",
      "42000/42000 [==============================] - 7s 170us/step - loss: 0.9309 - accuracy: 0.7216 - val_loss: 0.9359 - val_accuracy: 0.7183\n",
      "Epoch 230/300\n",
      "42000/42000 [==============================] - 7s 179us/step - loss: 0.9299 - accuracy: 0.7210 - val_loss: 0.9352 - val_accuracy: 0.7195\n",
      "Epoch 231/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 0.9290 - accuracy: 0.7221 - val_loss: 0.9341 - val_accuracy: 0.7188\n",
      "Epoch 232/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 0.9280 - accuracy: 0.7220 - val_loss: 0.9333 - val_accuracy: 0.7191\n",
      "Epoch 233/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 0.9271 - accuracy: 0.7221 - val_loss: 0.9324 - val_accuracy: 0.7200\n",
      "Epoch 234/300\n",
      "42000/42000 [==============================] - 8s 184us/step - loss: 0.9262 - accuracy: 0.7224 - val_loss: 0.9314 - val_accuracy: 0.7201\n",
      "Epoch 235/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 0.9252 - accuracy: 0.7228 - val_loss: 0.9307 - val_accuracy: 0.7204\n",
      "Epoch 236/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.9243 - accuracy: 0.7230 - val_loss: 0.9297 - val_accuracy: 0.7206\n",
      "Epoch 237/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 0.9234 - accuracy: 0.7235 - val_loss: 0.9288 - val_accuracy: 0.7210\n",
      "Epoch 238/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 0.9225 - accuracy: 0.7234 - val_loss: 0.9280 - val_accuracy: 0.7205\n",
      "Epoch 239/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 0.9216 - accuracy: 0.7238 - val_loss: 0.9271 - val_accuracy: 0.7210\n",
      "Epoch 240/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.9206 - accuracy: 0.7242 - val_loss: 0.9263 - val_accuracy: 0.7214\n",
      "Epoch 241/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 0.9197 - accuracy: 0.7242 - val_loss: 0.9254 - val_accuracy: 0.7218\n",
      "Epoch 242/300\n",
      "42000/42000 [==============================] - 8s 188us/step - loss: 0.9188 - accuracy: 0.7241 - val_loss: 0.9245 - val_accuracy: 0.7216\n",
      "Epoch 243/300\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 0.9180 - accuracy: 0.7246 - val_loss: 0.9236 - val_accuracy: 0.7224\n",
      "Epoch 244/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.9171 - accuracy: 0.7252 - val_loss: 0.9228 - val_accuracy: 0.7222\n",
      "Epoch 245/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 0.9162 - accuracy: 0.7249 - val_loss: 0.9220 - val_accuracy: 0.7231\n",
      "Epoch 246/300\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.9154 - accuracy: 0.7255 - val_loss: 0.9211 - val_accuracy: 0.7230\n",
      "Epoch 247/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 0.9144 - accuracy: 0.7258 - val_loss: 0.9202 - val_accuracy: 0.7227\n",
      "Epoch 248/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.9136 - accuracy: 0.7257 - val_loss: 0.9194 - val_accuracy: 0.7232\n",
      "Epoch 249/300\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 0.9127 - accuracy: 0.7260 - val_loss: 0.9186 - val_accuracy: 0.7238\n",
      "Epoch 250/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.9118 - accuracy: 0.7264 - val_loss: 0.9178 - val_accuracy: 0.7237\n",
      "Epoch 251/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 0.9110 - accuracy: 0.7262 - val_loss: 0.9168 - val_accuracy: 0.7240\n",
      "Epoch 252/300\n",
      "42000/42000 [==============================] - 8s 180us/step - loss: 0.9101 - accuracy: 0.7260 - val_loss: 0.9161 - val_accuracy: 0.7243\n",
      "Epoch 253/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 0.9092 - accuracy: 0.7272 - val_loss: 0.9153 - val_accuracy: 0.7243\n",
      "Epoch 254/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 0.9083 - accuracy: 0.7270 - val_loss: 0.9145 - val_accuracy: 0.7241\n",
      "Epoch 255/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 0.9075 - accuracy: 0.7274 - val_loss: 0.9137 - val_accuracy: 0.7243\n",
      "Epoch 256/300\n",
      "42000/42000 [==============================] - 8s 183us/step - loss: 0.9067 - accuracy: 0.7279 - val_loss: 0.9127 - val_accuracy: 0.7251\n",
      "Epoch 257/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 0.9058 - accuracy: 0.7277 - val_loss: 0.9120 - val_accuracy: 0.7250\n",
      "Epoch 258/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.9050 - accuracy: 0.7276 - val_loss: 0.9111 - val_accuracy: 0.7251\n",
      "Epoch 259/300\n",
      "42000/42000 [==============================] - 7s 174us/step - loss: 0.9042 - accuracy: 0.7275 - val_loss: 0.9103 - val_accuracy: 0.7252\n",
      "Epoch 260/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.9033 - accuracy: 0.7279 - val_loss: 0.9095 - val_accuracy: 0.7258\n",
      "Epoch 261/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 0.9025 - accuracy: 0.7282 - val_loss: 0.9088 - val_accuracy: 0.7258\n",
      "Epoch 262/300\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.9017 - accuracy: 0.7284 - val_loss: 0.9080 - val_accuracy: 0.7257\n",
      "Epoch 263/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 0.9008 - accuracy: 0.7287 - val_loss: 0.9071 - val_accuracy: 0.7260\n",
      "Epoch 264/300\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.8999 - accuracy: 0.7288 - val_loss: 0.9064 - val_accuracy: 0.7263\n",
      "Epoch 265/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 0.8991 - accuracy: 0.7298 - val_loss: 0.9056 - val_accuracy: 0.7268\n",
      "Epoch 266/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.8983 - accuracy: 0.7294 - val_loss: 0.9049 - val_accuracy: 0.7268\n",
      "Epoch 267/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 0.8976 - accuracy: 0.7300 - val_loss: 0.9041 - val_accuracy: 0.7269\n",
      "Epoch 268/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.8967 - accuracy: 0.7293 - val_loss: 0.9033 - val_accuracy: 0.7274\n",
      "Epoch 269/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 0.8959 - accuracy: 0.7295 - val_loss: 0.9025 - val_accuracy: 0.7269\n",
      "Epoch 270/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.8951 - accuracy: 0.7298 - val_loss: 0.9017 - val_accuracy: 0.7277\n",
      "Epoch 271/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 0.8942 - accuracy: 0.7303 - val_loss: 0.9009 - val_accuracy: 0.7276\n",
      "Epoch 272/300\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.8935 - accuracy: 0.7308 - val_loss: 0.9000 - val_accuracy: 0.7281\n",
      "Epoch 273/300\n",
      "42000/42000 [==============================] - 7s 170us/step - loss: 0.8927 - accuracy: 0.7308 - val_loss: 0.8994 - val_accuracy: 0.7284\n",
      "Epoch 274/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.8919 - accuracy: 0.7308 - val_loss: 0.8986 - val_accuracy: 0.7283\n",
      "Epoch 275/300\n",
      "42000/42000 [==============================] - 7s 170us/step - loss: 0.8911 - accuracy: 0.7307 - val_loss: 0.8978 - val_accuracy: 0.7283\n",
      "Epoch 276/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.8903 - accuracy: 0.7314 - val_loss: 0.8971 - val_accuracy: 0.7291\n",
      "Epoch 277/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 0.8895 - accuracy: 0.7316 - val_loss: 0.8964 - val_accuracy: 0.7292\n",
      "Epoch 278/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 0.8887 - accuracy: 0.7314 - val_loss: 0.8955 - val_accuracy: 0.7290\n",
      "Epoch 279/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 0.8879 - accuracy: 0.7319 - val_loss: 0.8948 - val_accuracy: 0.7286\n",
      "Epoch 280/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.8871 - accuracy: 0.7321 - val_loss: 0.8941 - val_accuracy: 0.7296\n",
      "Epoch 281/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 0.8864 - accuracy: 0.7321 - val_loss: 0.8933 - val_accuracy: 0.7295\n",
      "Epoch 282/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.8856 - accuracy: 0.7327 - val_loss: 0.8926 - val_accuracy: 0.7300\n",
      "Epoch 283/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 0.8848 - accuracy: 0.7326 - val_loss: 0.8918 - val_accuracy: 0.7303\n",
      "Epoch 284/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.8839 - accuracy: 0.7329 - val_loss: 0.8912 - val_accuracy: 0.7304\n",
      "Epoch 285/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 0.8833 - accuracy: 0.7327 - val_loss: 0.8904 - val_accuracy: 0.7307\n",
      "Epoch 286/300\n",
      "42000/42000 [==============================] - 7s 176us/step - loss: 0.8825 - accuracy: 0.7336 - val_loss: 0.8896 - val_accuracy: 0.7308\n",
      "Epoch 287/300\n",
      "42000/42000 [==============================] - 7s 173us/step - loss: 0.8817 - accuracy: 0.7340 - val_loss: 0.8889 - val_accuracy: 0.7308\n",
      "Epoch 288/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 0.8809 - accuracy: 0.7338 - val_loss: 0.8882 - val_accuracy: 0.7312\n",
      "Epoch 289/300\n",
      "42000/42000 [==============================] - 9s 203us/step - loss: 0.8803 - accuracy: 0.7339 - val_loss: 0.8875 - val_accuracy: 0.7316\n",
      "Epoch 290/300\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 0.8795 - accuracy: 0.7342 - val_loss: 0.8867 - val_accuracy: 0.7315\n",
      "Epoch 291/300\n",
      "42000/42000 [==============================] - 8s 189us/step - loss: 0.8787 - accuracy: 0.7341 - val_loss: 0.8860 - val_accuracy: 0.7316\n",
      "Epoch 292/300\n",
      "42000/42000 [==============================] - 7s 178us/step - loss: 0.8778 - accuracy: 0.7344 - val_loss: 0.8855 - val_accuracy: 0.7318\n",
      "Epoch 293/300\n",
      "42000/42000 [==============================] - 7s 172us/step - loss: 0.8771 - accuracy: 0.7354 - val_loss: 0.8846 - val_accuracy: 0.7320\n",
      "Epoch 294/300\n",
      "42000/42000 [==============================] - 8s 181us/step - loss: 0.8765 - accuracy: 0.7355 - val_loss: 0.8838 - val_accuracy: 0.7325\n",
      "Epoch 295/300\n",
      "42000/42000 [==============================] - 7s 171us/step - loss: 0.8756 - accuracy: 0.7356 - val_loss: 0.8832 - val_accuracy: 0.7326\n",
      "Epoch 296/300\n",
      "42000/42000 [==============================] - 7s 175us/step - loss: 0.8749 - accuracy: 0.7353 - val_loss: 0.8825 - val_accuracy: 0.7327\n",
      "Epoch 297/300\n",
      "42000/42000 [==============================] - 7s 170us/step - loss: 0.8742 - accuracy: 0.7363 - val_loss: 0.8817 - val_accuracy: 0.7325\n",
      "Epoch 298/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.8735 - accuracy: 0.7361 - val_loss: 0.8809 - val_accuracy: 0.7333\n",
      "Epoch 299/300\n",
      "42000/42000 [==============================] - 7s 177us/step - loss: 0.8727 - accuracy: 0.7359 - val_loss: 0.8803 - val_accuracy: 0.7335\n",
      "Epoch 300/300\n",
      "42000/42000 [==============================] - 8s 179us/step - loss: 0.8720 - accuracy: 0.7365 - val_loss: 0.8797 - val_accuracy: 0.7339\n"
     ]
    }
   ],
   "source": [
    "# now fit the model\n",
    "model9_keras_result_1=model9_keras.fit(X_train_new, y_train_new,           \n",
    "          validation_data=(X_val_new,y_val_new),\n",
    "          epochs=300,batch_size=500,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = model2_keras.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plotModelTrainingValidationLossAccuracy(model9_keras_result_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "18000/18000 [==============================] - 1s 35us/step\n",
      "test acc: 0.7262222170829773\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model  on test data\n",
    "\n",
    "evaluateModel(model9_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets use BatchNormalizaiton, this will help in better training the model and also avoid overfitting ( avoids of vanishing gradient problem too)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets add the regularization techniques for better accuracy and avoiding overfitting of model on training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lets Use BatchNormalization \n",
    "this will help to reduce vanishing gradient problem and also control the jumping of weight parameters ( this will also make sure training model is not overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import it first\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regularized = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelInitializer='he_uniform'\n",
    "activation='relu'\n",
    "learningRate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_regularized.add(Dense(units=512,kernel_initializer=kernelInitializer,input_shape=(1024,)))\n",
    "model_regularized.add(BatchNormalization())\n",
    "model_regularized.add(Activation(activation))\n",
    "\n",
    "model_regularized.add(Dense(256, kernel_initializer=kernelInitializer))\n",
    "model_regularized.add(BatchNormalization())\n",
    "\n",
    "model_regularized.add(Activation(activation))\n",
    "\n",
    "model_regularized.add(Dense(10, kernel_initializer=kernelInitializer)) \n",
    "model_regularized.add(BatchNormalization())\n",
    "model_regularized.add(Activation('softmax'))\n",
    "\n",
    "optimizer = optimizers.adam(learning_rate=learningRate)\n",
    "\n",
    "model_regularized.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/100\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.3762 - accuracy: 0.8819 - val_loss: 0.4158 - val_accuracy: 0.8736\n",
      "Epoch 2/100\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.3426 - accuracy: 0.8922 - val_loss: 0.3966 - val_accuracy: 0.8789\n",
      "Epoch 3/100\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.3268 - accuracy: 0.8964 - val_loss: 0.4068 - val_accuracy: 0.8787\n",
      "Epoch 4/100\n",
      "42000/42000 [==============================] - 9s 223us/step - loss: 0.3206 - accuracy: 0.8974 - val_loss: 0.3694 - val_accuracy: 0.8883\n",
      "Epoch 5/100\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.2976 - accuracy: 0.9046 - val_loss: 0.4490 - val_accuracy: 0.8635\n",
      "Epoch 6/100\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 0.2878 - accuracy: 0.9097 - val_loss: 0.3979 - val_accuracy: 0.8807\n",
      "Epoch 7/100\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.2716 - accuracy: 0.9145 - val_loss: 0.3892 - val_accuracy: 0.8861\n",
      "Epoch 8/100\n",
      "42000/42000 [==============================] - 9s 226us/step - loss: 0.2460 - accuracy: 0.9225 - val_loss: 0.3281 - val_accuracy: 0.9056\n",
      "Epoch 9/100\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.2405 - accuracy: 0.9233 - val_loss: 0.4032 - val_accuracy: 0.8832\n",
      "Epoch 10/100\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.2294 - accuracy: 0.9272 - val_loss: 0.3491 - val_accuracy: 0.8989\n",
      "Epoch 11/100\n",
      "42000/42000 [==============================] - 9s 222us/step - loss: 0.2248 - accuracy: 0.9272 - val_loss: 0.2948 - val_accuracy: 0.9183\n",
      "Epoch 12/100\n",
      "42000/42000 [==============================] - 10s 227us/step - loss: 0.1978 - accuracy: 0.9362 - val_loss: 0.3349 - val_accuracy: 0.9045\n",
      "Epoch 13/100\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.1960 - accuracy: 0.9386 - val_loss: 0.3231 - val_accuracy: 0.9081\n",
      "Epoch 14/100\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.1970 - accuracy: 0.9361 - val_loss: 0.3295 - val_accuracy: 0.9078\n",
      "Epoch 15/100\n",
      "42000/42000 [==============================] - 9s 225us/step - loss: 0.1861 - accuracy: 0.9398 - val_loss: 0.3155 - val_accuracy: 0.9118\n",
      "Epoch 16/100\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.1818 - accuracy: 0.9403 - val_loss: 0.2961 - val_accuracy: 0.9192\n",
      "Epoch 17/100\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.1604 - accuracy: 0.9483 - val_loss: 0.3296 - val_accuracy: 0.9104\n",
      "Epoch 18/100\n",
      "42000/42000 [==============================] - 9s 222us/step - loss: 0.1681 - accuracy: 0.9466 - val_loss: 0.3271 - val_accuracy: 0.9126\n",
      "Epoch 19/100\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.1546 - accuracy: 0.9499 - val_loss: 0.3070 - val_accuracy: 0.9192\n",
      "Epoch 20/100\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 0.1481 - accuracy: 0.9515 - val_loss: 0.2815 - val_accuracy: 0.9263\n",
      "Epoch 21/100\n",
      "42000/42000 [==============================] - 11s 250us/step - loss: 0.1276 - accuracy: 0.9591 - val_loss: 0.2753 - val_accuracy: 0.9304\n",
      "Epoch 22/100\n",
      "42000/42000 [==============================] - 10s 235us/step - loss: 0.1333 - accuracy: 0.9568 - val_loss: 0.3154 - val_accuracy: 0.9182\n",
      "Epoch 23/100\n",
      "42000/42000 [==============================] - 10s 238us/step - loss: 0.1337 - accuracy: 0.9564 - val_loss: 0.2927 - val_accuracy: 0.9273\n",
      "Epoch 24/100\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.1173 - accuracy: 0.9627 - val_loss: 0.2906 - val_accuracy: 0.9279\n",
      "Epoch 25/100\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.1124 - accuracy: 0.9638 - val_loss: 0.2598 - val_accuracy: 0.9382\n",
      "Epoch 26/100\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.1164 - accuracy: 0.9627 - val_loss: 0.2965 - val_accuracy: 0.9275\n",
      "Epoch 27/100\n",
      "42000/42000 [==============================] - 9s 222us/step - loss: 0.1237 - accuracy: 0.9595 - val_loss: 0.3056 - val_accuracy: 0.9254\n",
      "Epoch 28/100\n",
      "42000/42000 [==============================] - 9s 209us/step - loss: 0.1237 - accuracy: 0.9602 - val_loss: 0.3208 - val_accuracy: 0.9220\n",
      "Epoch 29/100\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.1133 - accuracy: 0.9621 - val_loss: 0.3085 - val_accuracy: 0.9247\n",
      "Epoch 30/100\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 0.1067 - accuracy: 0.9652 - val_loss: 0.2756 - val_accuracy: 0.9353\n",
      "Epoch 31/100\n",
      "42000/42000 [==============================] - 11s 272us/step - loss: 0.0955 - accuracy: 0.9691 - val_loss: 0.2744 - val_accuracy: 0.9359\n",
      "Epoch 32/100\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 0.0971 - accuracy: 0.9690 - val_loss: 0.2624 - val_accuracy: 0.9412\n",
      "Epoch 33/100\n",
      "42000/42000 [==============================] - 10s 245us/step - loss: 0.0920 - accuracy: 0.9705 - val_loss: 0.2829 - val_accuracy: 0.9357\n",
      "Epoch 34/100\n",
      "42000/42000 [==============================] - 11s 255us/step - loss: 0.0798 - accuracy: 0.9744 - val_loss: 0.2806 - val_accuracy: 0.9363\n",
      "Epoch 35/100\n",
      "42000/42000 [==============================] - 10s 245us/step - loss: 0.0836 - accuracy: 0.9732 - val_loss: 0.2877 - val_accuracy: 0.9335\n",
      "Epoch 36/100\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 0.0900 - accuracy: 0.9703 - val_loss: 0.3000 - val_accuracy: 0.9333\n",
      "Epoch 37/100\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0810 - accuracy: 0.9734 - val_loss: 0.3293 - val_accuracy: 0.9252\n",
      "Epoch 38/100\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 0.0838 - accuracy: 0.9720 - val_loss: 0.3025 - val_accuracy: 0.9327\n",
      "Epoch 39/100\n",
      "42000/42000 [==============================] - 8s 200us/step - loss: 0.0786 - accuracy: 0.9744 - val_loss: 0.2750 - val_accuracy: 0.9409\n",
      "Epoch 40/100\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0783 - accuracy: 0.9750 - val_loss: 0.3217 - val_accuracy: 0.9285\n",
      "Epoch 41/100\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0777 - accuracy: 0.9750 - val_loss: 0.2902 - val_accuracy: 0.9370\n",
      "Epoch 42/100\n",
      "42000/42000 [==============================] - 9s 207us/step - loss: 0.0771 - accuracy: 0.9754 - val_loss: 0.3042 - val_accuracy: 0.9318\n",
      "Epoch 43/100\n",
      "42000/42000 [==============================] - 9s 222us/step - loss: 0.0701 - accuracy: 0.9778 - val_loss: 0.3258 - val_accuracy: 0.9290\n",
      "Epoch 44/100\n",
      "42000/42000 [==============================] - 9s 209us/step - loss: 0.0715 - accuracy: 0.9765 - val_loss: 0.3109 - val_accuracy: 0.9340\n",
      "Epoch 45/100\n",
      "42000/42000 [==============================] - 9s 223us/step - loss: 0.0764 - accuracy: 0.9745 - val_loss: 0.2872 - val_accuracy: 0.9394\n",
      "Epoch 46/100\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 0.0734 - accuracy: 0.9751 - val_loss: 0.3050 - val_accuracy: 0.9333\n",
      "Epoch 47/100\n",
      "42000/42000 [==============================] - 9s 218us/step - loss: 0.0600 - accuracy: 0.9813 - val_loss: 0.2968 - val_accuracy: 0.9379\n",
      "Epoch 48/100\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.0682 - accuracy: 0.9777 - val_loss: 0.3005 - val_accuracy: 0.9363\n",
      "Epoch 49/100\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0615 - accuracy: 0.9797 - val_loss: 0.3135 - val_accuracy: 0.9341\n",
      "Epoch 50/100\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 0.0608 - accuracy: 0.9806 - val_loss: 0.3109 - val_accuracy: 0.9354\n",
      "Epoch 51/100\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0583 - accuracy: 0.9818 - val_loss: 0.3157 - val_accuracy: 0.9347\n",
      "Epoch 52/100\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.0517 - accuracy: 0.9836 - val_loss: 0.3100 - val_accuracy: 0.9403\n",
      "Epoch 53/100\n",
      "42000/42000 [==============================] - 9s 222us/step - loss: 0.0583 - accuracy: 0.9807 - val_loss: 0.3122 - val_accuracy: 0.9372\n",
      "Epoch 54/100\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 0.0610 - accuracy: 0.9797 - val_loss: 0.2934 - val_accuracy: 0.9423\n",
      "Epoch 55/100\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0586 - accuracy: 0.9807 - val_loss: 0.3141 - val_accuracy: 0.9374\n",
      "Epoch 56/100\n",
      "42000/42000 [==============================] - 9s 212us/step - loss: 0.0684 - accuracy: 0.9776 - val_loss: 0.3143 - val_accuracy: 0.9378\n",
      "Epoch 57/100\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0611 - accuracy: 0.9797 - val_loss: 0.3173 - val_accuracy: 0.9362\n",
      "Epoch 58/100\n",
      "42000/42000 [==============================] - 9s 209us/step - loss: 0.0532 - accuracy: 0.9823 - val_loss: 0.2955 - val_accuracy: 0.9429\n",
      "Epoch 59/100\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.0451 - accuracy: 0.9853 - val_loss: 0.3050 - val_accuracy: 0.9400\n",
      "Epoch 60/100\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 0.0560 - accuracy: 0.9820 - val_loss: 0.3090 - val_accuracy: 0.9393\n",
      "Epoch 61/100\n",
      "42000/42000 [==============================] - 9s 210us/step - loss: 0.0556 - accuracy: 0.9817 - val_loss: 0.3094 - val_accuracy: 0.9406\n",
      "Epoch 62/100\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0537 - accuracy: 0.9816 - val_loss: 0.3128 - val_accuracy: 0.9389\n",
      "Epoch 63/100\n",
      "42000/42000 [==============================] - 9s 211us/step - loss: 0.0578 - accuracy: 0.9807 - val_loss: 0.3069 - val_accuracy: 0.9399\n",
      "Epoch 64/100\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0553 - accuracy: 0.9812 - val_loss: 0.3124 - val_accuracy: 0.9397\n",
      "Epoch 65/100\n",
      "42000/42000 [==============================] - 9s 209us/step - loss: 0.0577 - accuracy: 0.9813 - val_loss: 0.3005 - val_accuracy: 0.9418\n",
      "Epoch 66/100\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 0.0550 - accuracy: 0.9813 - val_loss: 0.3589 - val_accuracy: 0.9302\n",
      "Epoch 67/100\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0555 - accuracy: 0.9822 - val_loss: 0.3224 - val_accuracy: 0.9399\n",
      "Epoch 68/100\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0457 - accuracy: 0.9850 - val_loss: 0.3243 - val_accuracy: 0.9392\n",
      "Epoch 69/100\n",
      "42000/42000 [==============================] - 9s 218us/step - loss: 0.0444 - accuracy: 0.9853 - val_loss: 0.2925 - val_accuracy: 0.9469\n",
      "Epoch 70/100\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0419 - accuracy: 0.9861 - val_loss: 0.3285 - val_accuracy: 0.9389\n",
      "Epoch 71/100\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0472 - accuracy: 0.9842 - val_loss: 0.3108 - val_accuracy: 0.9442\n",
      "Epoch 72/100\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0487 - accuracy: 0.9834 - val_loss: 0.3220 - val_accuracy: 0.9406\n",
      "Epoch 73/100\n",
      "42000/42000 [==============================] - 9s 225us/step - loss: 0.0366 - accuracy: 0.9887 - val_loss: 0.3104 - val_accuracy: 0.9449\n",
      "Epoch 74/100\n",
      "42000/42000 [==============================] - 10s 238us/step - loss: 0.0381 - accuracy: 0.9873 - val_loss: 0.3310 - val_accuracy: 0.9384\n",
      "Epoch 75/100\n",
      "42000/42000 [==============================] - 10s 241us/step - loss: 0.0501 - accuracy: 0.9836 - val_loss: 0.3374 - val_accuracy: 0.9382\n",
      "Epoch 76/100\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 0.0614 - accuracy: 0.9799 - val_loss: 0.3458 - val_accuracy: 0.9387\n",
      "Epoch 77/100\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0465 - accuracy: 0.9849 - val_loss: 0.3169 - val_accuracy: 0.9426\n",
      "Epoch 78/100\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0352 - accuracy: 0.9885 - val_loss: 0.2994 - val_accuracy: 0.9467\n",
      "Epoch 79/100\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0278 - accuracy: 0.9914 - val_loss: 0.3280 - val_accuracy: 0.9417\n",
      "Epoch 80/100\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 0.0394 - accuracy: 0.9872 - val_loss: 0.3160 - val_accuracy: 0.9443\n",
      "Epoch 81/100\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0337 - accuracy: 0.9892 - val_loss: 0.3203 - val_accuracy: 0.9442\n",
      "Epoch 82/100\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 0.0354 - accuracy: 0.9884 - val_loss: 0.3247 - val_accuracy: 0.9439\n",
      "Epoch 83/100\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 0.0452 - accuracy: 0.9853 - val_loss: 0.3385 - val_accuracy: 0.9406\n",
      "Epoch 84/100\n",
      "42000/42000 [==============================] - 9s 207us/step - loss: 0.0476 - accuracy: 0.9842 - val_loss: 0.3554 - val_accuracy: 0.9381\n",
      "Epoch 85/100\n",
      "42000/42000 [==============================] - 9s 218us/step - loss: 0.0575 - accuracy: 0.9809 - val_loss: 0.3601 - val_accuracy: 0.9359\n",
      "Epoch 86/100\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 0.0503 - accuracy: 0.9831 - val_loss: 0.3261 - val_accuracy: 0.9423\n",
      "Epoch 87/100\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0367 - accuracy: 0.9877 - val_loss: 0.3332 - val_accuracy: 0.9425\n",
      "Epoch 88/100\n",
      "42000/42000 [==============================] - 9s 216us/step - loss: 0.0372 - accuracy: 0.9878 - val_loss: 0.3180 - val_accuracy: 0.9455\n",
      "Epoch 89/100\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0309 - accuracy: 0.9900 - val_loss: 0.3160 - val_accuracy: 0.9465\n",
      "Epoch 90/100\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0345 - accuracy: 0.9892 - val_loss: 0.3082 - val_accuracy: 0.9473\n",
      "Epoch 91/100\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0289 - accuracy: 0.9905 - val_loss: 0.3172 - val_accuracy: 0.9464\n",
      "Epoch 92/100\n",
      "42000/42000 [==============================] - 9s 214us/step - loss: 0.0306 - accuracy: 0.9901 - val_loss: 0.3292 - val_accuracy: 0.9427\n",
      "Epoch 93/100\n",
      "42000/42000 [==============================] - 9s 215us/step - loss: 0.0455 - accuracy: 0.9847 - val_loss: 0.3452 - val_accuracy: 0.9406\n",
      "Epoch 94/100\n",
      "42000/42000 [==============================] - 10s 226us/step - loss: 0.0381 - accuracy: 0.9871 - val_loss: 0.3332 - val_accuracy: 0.9436\n",
      "Epoch 95/100\n",
      "42000/42000 [==============================] - 9s 221us/step - loss: 0.0448 - accuracy: 0.9847 - val_loss: 0.3396 - val_accuracy: 0.9421\n",
      "Epoch 96/100\n",
      "42000/42000 [==============================] - 9s 219us/step - loss: 0.0413 - accuracy: 0.9858 - val_loss: 0.3322 - val_accuracy: 0.9447\n",
      "Epoch 97/100\n",
      "42000/42000 [==============================] - 9s 217us/step - loss: 0.0316 - accuracy: 0.9900 - val_loss: 0.3216 - val_accuracy: 0.9479\n",
      "Epoch 98/100\n",
      "42000/42000 [==============================] - 9s 224us/step - loss: 0.0354 - accuracy: 0.9886 - val_loss: 0.3417 - val_accuracy: 0.9418\n",
      "Epoch 99/100\n",
      "42000/42000 [==============================] - 9s 220us/step - loss: 0.0347 - accuracy: 0.9886 - val_loss: 0.3528 - val_accuracy: 0.9413\n",
      "Epoch 100/100\n",
      "42000/42000 [==============================] - 9s 213us/step - loss: 0.0465 - accuracy: 0.9846 - val_loss: 0.3590 - val_accuracy: 0.9410\n"
     ]
    }
   ],
   "source": [
    "# now fit the model\n",
    "model_regularized_result=model_regularized.fit(X_train_new, y_train_new,           \n",
    "          validation_data=(X_val_new,y_val_new),\n",
    "          epochs=100,batch_size=500,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that Training Accuracy is 98% and Validation Accuracy is 94%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now Lets include use dropout regularization too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import it first\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regularized_1 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernelInitializer='he_uniform'\n",
    "activation='relu'\n",
    "learningRate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_regularized_1.add(Dense(units=512,kernel_initializer=kernelInitializer,input_shape=(1024,)))\n",
    "model_regularized_1.add(BatchNormalization())\n",
    "model_regularized_1.add(Activation(activation))\n",
    "model_regularized_1.add(Dropout(0.2))\n",
    "\n",
    "model_regularized_1.add(Dense(256, kernel_initializer=kernelInitializer))\n",
    "model_regularized_1.add(BatchNormalization())\n",
    "model_regularized_1.add(Activation(activation))\n",
    "model_regularized_1.add(Dropout(0.2))\n",
    "\n",
    "model_regularized_1.add(Dense(10, kernel_initializer=kernelInitializer)) \n",
    "model_regularized_1.add(Activation('softmax'))\n",
    "\n",
    "optimizer = optimizers.adam(learning_rate=learningRate)\n",
    "\n",
    "model_regularized_1.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 60000 samples\n",
      "Epoch 1/300\n",
      "42000/42000 [==============================] - 12s 287us/step - loss: 1.8239 - accuracy: 0.3802 - val_loss: 1.2533 - val_accuracy: 0.6039\n",
      "Epoch 2/300\n",
      "42000/42000 [==============================] - 12s 284us/step - loss: 1.0720 - accuracy: 0.6564 - val_loss: 0.8486 - val_accuracy: 0.7360\n",
      "Epoch 3/300\n",
      "42000/42000 [==============================] - 12s 291us/step - loss: 0.8876 - accuracy: 0.7208 - val_loss: 0.7462 - val_accuracy: 0.7718\n",
      "Epoch 4/300\n",
      "42000/42000 [==============================] - 10s 245us/step - loss: 0.7795 - accuracy: 0.7531 - val_loss: 0.6565 - val_accuracy: 0.7975\n",
      "Epoch 5/300\n",
      "42000/42000 [==============================] - 10s 241us/step - loss: 0.7085 - accuracy: 0.7765 - val_loss: 0.5888 - val_accuracy: 0.8208\n",
      "Epoch 6/300\n",
      "42000/42000 [==============================] - 10s 233us/step - loss: 0.6762 - accuracy: 0.7854 - val_loss: 0.5602 - val_accuracy: 0.8281\n",
      "Epoch 7/300\n",
      "42000/42000 [==============================] - 10s 229us/step - loss: 0.6403 - accuracy: 0.7956 - val_loss: 0.5582 - val_accuracy: 0.8253\n",
      "Epoch 8/300\n",
      "42000/42000 [==============================] - 10s 238us/step - loss: 0.5994 - accuracy: 0.8080 - val_loss: 0.5258 - val_accuracy: 0.8366\n",
      "Epoch 9/300\n",
      "42000/42000 [==============================] - 10s 232us/step - loss: 0.5830 - accuracy: 0.8141 - val_loss: 0.4812 - val_accuracy: 0.8518\n",
      "Epoch 10/300\n",
      "42000/42000 [==============================] - 10s 241us/step - loss: 0.5602 - accuracy: 0.8204 - val_loss: 0.4870 - val_accuracy: 0.8512\n",
      "Epoch 11/300\n",
      "42000/42000 [==============================] - 10s 240us/step - loss: 0.5473 - accuracy: 0.8249 - val_loss: 0.4615 - val_accuracy: 0.8597\n",
      "Epoch 12/300\n",
      "42000/42000 [==============================] - 10s 239us/step - loss: 0.5162 - accuracy: 0.8347 - val_loss: 0.4398 - val_accuracy: 0.8663\n",
      "Epoch 13/300\n",
      "42000/42000 [==============================] - 10s 241us/step - loss: 0.5029 - accuracy: 0.8394 - val_loss: 0.4413 - val_accuracy: 0.8639\n",
      "Epoch 14/300\n",
      "42000/42000 [==============================] - 10s 240us/step - loss: 0.4935 - accuracy: 0.8421 - val_loss: 0.4031 - val_accuracy: 0.8813\n",
      "Epoch 15/300\n",
      "42000/42000 [==============================] - 10s 240us/step - loss: 0.4723 - accuracy: 0.8480 - val_loss: 0.3967 - val_accuracy: 0.8814\n",
      "Epoch 16/300\n",
      "42000/42000 [==============================] - 11s 266us/step - loss: 0.4719 - accuracy: 0.8482 - val_loss: 0.3923 - val_accuracy: 0.8807\n",
      "Epoch 17/300\n",
      "42000/42000 [==============================] - 11s 262us/step - loss: 0.4577 - accuracy: 0.8538 - val_loss: 0.3805 - val_accuracy: 0.8857\n",
      "Epoch 18/300\n",
      "42000/42000 [==============================] - 11s 264us/step - loss: 0.4464 - accuracy: 0.8561 - val_loss: 0.3899 - val_accuracy: 0.8838\n",
      "Epoch 19/300\n",
      "42000/42000 [==============================] - 11s 270us/step - loss: 0.4415 - accuracy: 0.8571 - val_loss: 0.4053 - val_accuracy: 0.8742\n",
      "Epoch 20/300\n",
      "42000/42000 [==============================] - 11s 264us/step - loss: 0.4298 - accuracy: 0.8613 - val_loss: 0.3783 - val_accuracy: 0.8860\n",
      "Epoch 21/300\n",
      "42000/42000 [==============================] - 11s 256us/step - loss: 0.4250 - accuracy: 0.8609 - val_loss: 0.3844 - val_accuracy: 0.8824\n",
      "Epoch 22/300\n",
      "42000/42000 [==============================] - 11s 263us/step - loss: 0.4064 - accuracy: 0.8672 - val_loss: 0.3479 - val_accuracy: 0.8962\n",
      "Epoch 23/300\n",
      "42000/42000 [==============================] - 11s 251us/step - loss: 0.4018 - accuracy: 0.8694 - val_loss: 0.3504 - val_accuracy: 0.8944\n",
      "Epoch 24/300\n",
      "42000/42000 [==============================] - 11s 257us/step - loss: 0.4015 - accuracy: 0.8689 - val_loss: 0.3565 - val_accuracy: 0.8963\n",
      "Epoch 25/300\n",
      "42000/42000 [==============================] - 11s 263us/step - loss: 0.3957 - accuracy: 0.8716 - val_loss: 0.3472 - val_accuracy: 0.8971\n",
      "Epoch 26/300\n",
      "42000/42000 [==============================] - 11s 255us/step - loss: 0.3882 - accuracy: 0.8752 - val_loss: 0.3428 - val_accuracy: 0.8985\n",
      "Epoch 27/300\n",
      "42000/42000 [==============================] - 11s 259us/step - loss: 0.3829 - accuracy: 0.8758 - val_loss: 0.3361 - val_accuracy: 0.9013\n",
      "Epoch 28/300\n",
      "42000/42000 [==============================] - 11s 252us/step - loss: 0.3724 - accuracy: 0.8775 - val_loss: 0.3416 - val_accuracy: 0.8982\n",
      "Epoch 29/300\n",
      "42000/42000 [==============================] - 11s 251us/step - loss: 0.3645 - accuracy: 0.8812 - val_loss: 0.3281 - val_accuracy: 0.9020\n",
      "Epoch 30/300\n",
      "42000/42000 [==============================] - 11s 260us/step - loss: 0.3693 - accuracy: 0.8797 - val_loss: 0.3098 - val_accuracy: 0.9091\n",
      "Epoch 31/300\n",
      "42000/42000 [==============================] - 11s 260us/step - loss: 0.3582 - accuracy: 0.8808 - val_loss: 0.3128 - val_accuracy: 0.9081\n",
      "Epoch 32/300\n",
      "42000/42000 [==============================] - 13s 299us/step - loss: 0.3654 - accuracy: 0.8809 - val_loss: 0.3216 - val_accuracy: 0.9054\n",
      "Epoch 33/300\n",
      "42000/42000 [==============================] - 12s 283us/step - loss: 0.3493 - accuracy: 0.8840 - val_loss: 0.3087 - val_accuracy: 0.9107\n",
      "Epoch 34/300\n",
      "42000/42000 [==============================] - 12s 289us/step - loss: 0.3472 - accuracy: 0.8851 - val_loss: 0.3084 - val_accuracy: 0.9101\n",
      "Epoch 35/300\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 0.3420 - accuracy: 0.8868 - val_loss: 0.3059 - val_accuracy: 0.9118\n",
      "Epoch 36/300\n",
      "42000/42000 [==============================] - 11s 255us/step - loss: 0.3360 - accuracy: 0.8901 - val_loss: 0.3098 - val_accuracy: 0.9119\n",
      "Epoch 37/300\n",
      "42000/42000 [==============================] - 10s 247us/step - loss: 0.3255 - accuracy: 0.8935 - val_loss: 0.2831 - val_accuracy: 0.9203\n",
      "Epoch 38/300\n",
      "42000/42000 [==============================] - 10s 242us/step - loss: 0.3206 - accuracy: 0.8952 - val_loss: 0.3016 - val_accuracy: 0.9139\n",
      "Epoch 39/300\n",
      "42000/42000 [==============================] - 10s 240us/step - loss: 0.3204 - accuracy: 0.8941 - val_loss: 0.3010 - val_accuracy: 0.9134\n",
      "Epoch 40/300\n",
      "42000/42000 [==============================] - 10s 237us/step - loss: 0.3212 - accuracy: 0.8935 - val_loss: 0.2813 - val_accuracy: 0.9218\n",
      "Epoch 41/300\n",
      "42000/42000 [==============================] - 10s 234us/step - loss: 0.3110 - accuracy: 0.8981 - val_loss: 0.2868 - val_accuracy: 0.9182\n",
      "Epoch 42/300\n",
      "42000/42000 [==============================] - 10s 243us/step - loss: 0.3095 - accuracy: 0.8986 - val_loss: 0.2880 - val_accuracy: 0.9182\n",
      "Epoch 43/300\n",
      "42000/42000 [==============================] - 10s 235us/step - loss: 0.3141 - accuracy: 0.8953 - val_loss: 0.2996 - val_accuracy: 0.9144\n",
      "Epoch 44/300\n",
      "42000/42000 [==============================] - 10s 243us/step - loss: 0.3078 - accuracy: 0.8981 - val_loss: 0.2863 - val_accuracy: 0.9175\n",
      "Epoch 45/300\n",
      "42000/42000 [==============================] - 10s 248us/step - loss: 0.2950 - accuracy: 0.9020 - val_loss: 0.2681 - val_accuracy: 0.9259\n",
      "Epoch 46/300\n",
      "42000/42000 [==============================] - 10s 240us/step - loss: 0.2916 - accuracy: 0.9023 - val_loss: 0.2685 - val_accuracy: 0.9255\n",
      "Epoch 47/300\n",
      "42000/42000 [==============================] - 10s 242us/step - loss: 0.2976 - accuracy: 0.9012 - val_loss: 0.2735 - val_accuracy: 0.9232\n",
      "Epoch 48/300\n",
      "42000/42000 [==============================] - 10s 249us/step - loss: 0.2939 - accuracy: 0.9022 - val_loss: 0.2700 - val_accuracy: 0.9234\n",
      "Epoch 49/300\n",
      "42000/42000 [==============================] - 10s 243us/step - loss: 0.2853 - accuracy: 0.9049 - val_loss: 0.2592 - val_accuracy: 0.9287\n",
      "Epoch 50/300\n",
      "42000/42000 [==============================] - 10s 243us/step - loss: 0.2865 - accuracy: 0.9042 - val_loss: 0.2773 - val_accuracy: 0.9215\n",
      "Epoch 51/300\n",
      "42000/42000 [==============================] - 10s 247us/step - loss: 0.2837 - accuracy: 0.9054 - val_loss: 0.2723 - val_accuracy: 0.9244\n",
      "Epoch 52/300\n",
      "42000/42000 [==============================] - 10s 243us/step - loss: 0.2818 - accuracy: 0.9069 - val_loss: 0.2564 - val_accuracy: 0.9305\n",
      "Epoch 53/300\n",
      "42000/42000 [==============================] - 10s 245us/step - loss: 0.2770 - accuracy: 0.9070 - val_loss: 0.2745 - val_accuracy: 0.9225\n",
      "Epoch 54/300\n",
      "42000/42000 [==============================] - 10s 242us/step - loss: 0.2755 - accuracy: 0.9074 - val_loss: 0.2589 - val_accuracy: 0.9298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300\n",
      "42000/42000 [==============================] - 10s 238us/step - loss: 0.2603 - accuracy: 0.9128 - val_loss: 0.2504 - val_accuracy: 0.9330\n",
      "Epoch 56/300\n",
      "42000/42000 [==============================] - 10s 240us/step - loss: 0.2734 - accuracy: 0.9078 - val_loss: 0.2549 - val_accuracy: 0.9308\n",
      "Epoch 57/300\n",
      "42000/42000 [==============================] - 10s 239us/step - loss: 0.2630 - accuracy: 0.9113 - val_loss: 0.2644 - val_accuracy: 0.9283\n",
      "Epoch 58/300\n",
      "42000/42000 [==============================] - 10s 238us/step - loss: 0.2603 - accuracy: 0.9139 - val_loss: 0.2456 - val_accuracy: 0.9344\n",
      "Epoch 59/300\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 0.2582 - accuracy: 0.9143 - val_loss: 0.2488 - val_accuracy: 0.9342\n",
      "Epoch 60/300\n",
      "42000/42000 [==============================] - 10s 249us/step - loss: 0.2546 - accuracy: 0.9144 - val_loss: 0.2518 - val_accuracy: 0.9333\n",
      "Epoch 61/300\n",
      "42000/42000 [==============================] - 11s 265us/step - loss: 0.2558 - accuracy: 0.9155 - val_loss: 0.2683 - val_accuracy: 0.9266\n",
      "Epoch 62/300\n",
      "42000/42000 [==============================] - 10s 240us/step - loss: 0.2539 - accuracy: 0.9158 - val_loss: 0.2457 - val_accuracy: 0.9348\n",
      "Epoch 63/300\n",
      "42000/42000 [==============================] - 10s 240us/step - loss: 0.2462 - accuracy: 0.9182 - val_loss: 0.2466 - val_accuracy: 0.9342\n",
      "Epoch 64/300\n",
      "42000/42000 [==============================] - 10s 240us/step - loss: 0.2419 - accuracy: 0.9184 - val_loss: 0.2433 - val_accuracy: 0.9369\n",
      "Epoch 65/300\n",
      "42000/42000 [==============================] - 10s 243us/step - loss: 0.2455 - accuracy: 0.9177 - val_loss: 0.2453 - val_accuracy: 0.9345\n",
      "Epoch 66/300\n",
      "42000/42000 [==============================] - 10s 242us/step - loss: 0.2501 - accuracy: 0.9173 - val_loss: 0.2451 - val_accuracy: 0.9351\n",
      "Epoch 67/300\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 0.2428 - accuracy: 0.9185 - val_loss: 0.2437 - val_accuracy: 0.9357\n",
      "Epoch 68/300\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 0.2453 - accuracy: 0.9165 - val_loss: 0.2374 - val_accuracy: 0.9380\n",
      "Epoch 69/300\n",
      "42000/42000 [==============================] - 10s 239us/step - loss: 0.2393 - accuracy: 0.9197 - val_loss: 0.2474 - val_accuracy: 0.9343\n",
      "Epoch 70/300\n",
      "42000/42000 [==============================] - 10s 242us/step - loss: 0.2443 - accuracy: 0.9189 - val_loss: 0.2512 - val_accuracy: 0.9331\n",
      "Epoch 71/300\n",
      "42000/42000 [==============================] - 10s 242us/step - loss: 0.2348 - accuracy: 0.9217 - val_loss: 0.2377 - val_accuracy: 0.9378\n",
      "Epoch 72/300\n",
      "42000/42000 [==============================] - 10s 242us/step - loss: 0.2343 - accuracy: 0.9213 - val_loss: 0.2380 - val_accuracy: 0.9381\n",
      "Epoch 73/300\n",
      "42000/42000 [==============================] - 10s 247us/step - loss: 0.2317 - accuracy: 0.9213 - val_loss: 0.2667 - val_accuracy: 0.9300\n",
      "Epoch 74/300\n",
      "42000/42000 [==============================] - 10s 248us/step - loss: 0.2343 - accuracy: 0.9223 - val_loss: 0.2422 - val_accuracy: 0.9363\n",
      "Epoch 75/300\n",
      "42000/42000 [==============================] - 10s 237us/step - loss: 0.2235 - accuracy: 0.9247 - val_loss: 0.2393 - val_accuracy: 0.9377\n",
      "Epoch 76/300\n",
      "42000/42000 [==============================] - 10s 244us/step - loss: 0.2227 - accuracy: 0.9261 - val_loss: 0.2299 - val_accuracy: 0.9408\n",
      "Epoch 77/300\n",
      "42000/42000 [==============================] - 10s 249us/step - loss: 0.2270 - accuracy: 0.9241 - val_loss: 0.2268 - val_accuracy: 0.9415\n",
      "Epoch 78/300\n",
      "42000/42000 [==============================] - 10s 245us/step - loss: 0.2192 - accuracy: 0.9268 - val_loss: 0.2446 - val_accuracy: 0.9375\n",
      "Epoch 79/300\n",
      "42000/42000 [==============================] - 11s 251us/step - loss: 0.2174 - accuracy: 0.9278 - val_loss: 0.2434 - val_accuracy: 0.9365\n",
      "Epoch 80/300\n",
      "42000/42000 [==============================] - 12s 275us/step - loss: 0.2338 - accuracy: 0.9227 - val_loss: 0.2533 - val_accuracy: 0.9325\n",
      "Epoch 81/300\n",
      "42000/42000 [==============================] - 13s 304us/step - loss: 0.2210 - accuracy: 0.9251 - val_loss: 0.2316 - val_accuracy: 0.9414\n",
      "Epoch 82/300\n",
      "42000/42000 [==============================] - 13s 313us/step - loss: 0.2213 - accuracy: 0.9260 - val_loss: 0.2358 - val_accuracy: 0.9391\n",
      "Epoch 83/300\n",
      "42000/42000 [==============================] - 13s 316us/step - loss: 0.2156 - accuracy: 0.9275 - val_loss: 0.2395 - val_accuracy: 0.9389\n",
      "Epoch 84/300\n",
      "42000/42000 [==============================] - 13s 313us/step - loss: 0.2133 - accuracy: 0.9274 - val_loss: 0.2385 - val_accuracy: 0.9380\n",
      "Epoch 85/300\n",
      "42000/42000 [==============================] - 13s 308us/step - loss: 0.2089 - accuracy: 0.9301 - val_loss: 0.2327 - val_accuracy: 0.9413\n",
      "Epoch 86/300\n",
      "42000/42000 [==============================] - 13s 304us/step - loss: 0.2141 - accuracy: 0.9267 - val_loss: 0.2331 - val_accuracy: 0.9412\n",
      "Epoch 87/300\n",
      "42000/42000 [==============================] - 13s 308us/step - loss: 0.2112 - accuracy: 0.9278 - val_loss: 0.2364 - val_accuracy: 0.9407\n",
      "Epoch 88/300\n",
      "42000/42000 [==============================] - 13s 300us/step - loss: 0.2050 - accuracy: 0.9311 - val_loss: 0.2288 - val_accuracy: 0.9420\n",
      "Epoch 89/300\n",
      "42000/42000 [==============================] - 13s 310us/step - loss: 0.2014 - accuracy: 0.9315 - val_loss: 0.2276 - val_accuracy: 0.9448\n",
      "Epoch 90/300\n",
      "42000/42000 [==============================] - 13s 303us/step - loss: 0.2028 - accuracy: 0.9311 - val_loss: 0.2237 - val_accuracy: 0.9450\n",
      "Epoch 91/300\n",
      "42000/42000 [==============================] - 13s 304us/step - loss: 0.1998 - accuracy: 0.9322 - val_loss: 0.2194 - val_accuracy: 0.9465\n",
      "Epoch 92/300\n",
      "42000/42000 [==============================] - 13s 316us/step - loss: 0.1972 - accuracy: 0.9340 - val_loss: 0.2174 - val_accuracy: 0.9476\n",
      "Epoch 93/300\n",
      "42000/42000 [==============================] - 11s 273us/step - loss: 0.1986 - accuracy: 0.9320 - val_loss: 0.2304 - val_accuracy: 0.9425\n",
      "Epoch 94/300\n",
      "42000/42000 [==============================] - 11s 263us/step - loss: 0.2062 - accuracy: 0.9305 - val_loss: 0.2270 - val_accuracy: 0.9444\n",
      "Epoch 95/300\n",
      "42000/42000 [==============================] - 11s 256us/step - loss: 0.1973 - accuracy: 0.9333 - val_loss: 0.2237 - val_accuracy: 0.9452\n",
      "Epoch 96/300\n",
      "42000/42000 [==============================] - 11s 258us/step - loss: 0.1964 - accuracy: 0.9338 - val_loss: 0.2315 - val_accuracy: 0.9431\n",
      "Epoch 97/300\n",
      "42000/42000 [==============================] - 11s 258us/step - loss: 0.1989 - accuracy: 0.9331 - val_loss: 0.2258 - val_accuracy: 0.9454\n",
      "Epoch 98/300\n",
      "42000/42000 [==============================] - 11s 250us/step - loss: 0.1912 - accuracy: 0.9351 - val_loss: 0.2375 - val_accuracy: 0.9404\n",
      "Epoch 99/300\n",
      "42000/42000 [==============================] - 11s 256us/step - loss: 0.1948 - accuracy: 0.9333 - val_loss: 0.2213 - val_accuracy: 0.9479\n",
      "Epoch 100/300\n",
      "42000/42000 [==============================] - 11s 252us/step - loss: 0.1869 - accuracy: 0.9368 - val_loss: 0.2212 - val_accuracy: 0.9462\n",
      "Epoch 101/300\n",
      "42000/42000 [==============================] - 10s 248us/step - loss: 0.1909 - accuracy: 0.9361 - val_loss: 0.2460 - val_accuracy: 0.9382\n",
      "Epoch 102/300\n",
      "42000/42000 [==============================] - 11s 267us/step - loss: 0.1939 - accuracy: 0.9348 - val_loss: 0.2195 - val_accuracy: 0.9475\n",
      "Epoch 103/300\n",
      "42000/42000 [==============================] - 11s 252us/step - loss: 0.1869 - accuracy: 0.9370 - val_loss: 0.2318 - val_accuracy: 0.9445\n",
      "Epoch 104/300\n",
      "42000/42000 [==============================] - 10s 249us/step - loss: 0.1869 - accuracy: 0.9362 - val_loss: 0.2200 - val_accuracy: 0.9475\n",
      "Epoch 105/300\n",
      "42000/42000 [==============================] - 11s 266us/step - loss: 0.1950 - accuracy: 0.9341 - val_loss: 0.2279 - val_accuracy: 0.9447\n",
      "Epoch 106/300\n",
      "42000/42000 [==============================] - 11s 269us/step - loss: 0.1852 - accuracy: 0.9375 - val_loss: 0.2152 - val_accuracy: 0.9492\n",
      "Epoch 107/300\n",
      "42000/42000 [==============================] - 10s 229us/step - loss: 0.1851 - accuracy: 0.9374 - val_loss: 0.2289 - val_accuracy: 0.9454\n",
      "Epoch 108/300\n",
      "42000/42000 [==============================] - 10s 238us/step - loss: 0.1836 - accuracy: 0.9388 - val_loss: 0.2265 - val_accuracy: 0.9462\n",
      "Epoch 109/300\n",
      "42000/42000 [==============================] - 10s 237us/step - loss: 0.1844 - accuracy: 0.9385 - val_loss: 0.2283 - val_accuracy: 0.9459\n",
      "Epoch 110/300\n",
      "42000/42000 [==============================] - 10s 238us/step - loss: 0.1830 - accuracy: 0.9379 - val_loss: 0.2373 - val_accuracy: 0.9419\n",
      "Epoch 111/300\n",
      "42000/42000 [==============================] - 10s 238us/step - loss: 0.1747 - accuracy: 0.9405 - val_loss: 0.2282 - val_accuracy: 0.9462\n",
      "Epoch 112/300\n",
      "42000/42000 [==============================] - 10s 235us/step - loss: 0.1773 - accuracy: 0.9401 - val_loss: 0.2287 - val_accuracy: 0.9460\n",
      "Epoch 113/300\n",
      "42000/42000 [==============================] - 10s 241us/step - loss: 0.1802 - accuracy: 0.9396 - val_loss: 0.2179 - val_accuracy: 0.9487\n",
      "Epoch 114/300\n",
      "42000/42000 [==============================] - 12s 278us/step - loss: 0.1798 - accuracy: 0.9398 - val_loss: 0.2171 - val_accuracy: 0.9478\n",
      "Epoch 115/300\n",
      "42000/42000 [==============================] - 12s 286us/step - loss: 0.1761 - accuracy: 0.9404 - val_loss: 0.2206 - val_accuracy: 0.9481\n",
      "Epoch 116/300\n",
      "42000/42000 [==============================] - 10s 249us/step - loss: 0.1723 - accuracy: 0.9423 - val_loss: 0.2127 - val_accuracy: 0.9513\n",
      "Epoch 117/300\n",
      "42000/42000 [==============================] - 10s 236us/step - loss: 0.1764 - accuracy: 0.9401 - val_loss: 0.2191 - val_accuracy: 0.9495\n",
      "Epoch 118/300\n",
      "42000/42000 [==============================] - 10s 243us/step - loss: 0.1706 - accuracy: 0.9418 - val_loss: 0.2191 - val_accuracy: 0.9494\n",
      "Epoch 119/300\n",
      "42000/42000 [==============================] - 10s 241us/step - loss: 0.1686 - accuracy: 0.9433 - val_loss: 0.2255 - val_accuracy: 0.9478\n",
      "Epoch 120/300\n",
      "42000/42000 [==============================] - 10s 242us/step - loss: 0.1760 - accuracy: 0.9399 - val_loss: 0.2179 - val_accuracy: 0.9492\n",
      "Epoch 121/300\n",
      "42000/42000 [==============================] - 10s 247us/step - loss: 0.1685 - accuracy: 0.9426 - val_loss: 0.2266 - val_accuracy: 0.9464\n",
      "Epoch 122/300\n",
      "42000/42000 [==============================] - 10s 247us/step - loss: 0.1684 - accuracy: 0.9432 - val_loss: 0.2141 - val_accuracy: 0.9509\n",
      "Epoch 123/300\n",
      "42000/42000 [==============================] - 10s 245us/step - loss: 0.1646 - accuracy: 0.9442 - val_loss: 0.2172 - val_accuracy: 0.9492\n",
      "Epoch 124/300\n",
      "42000/42000 [==============================] - 10s 247us/step - loss: 0.1657 - accuracy: 0.9422 - val_loss: 0.2209 - val_accuracy: 0.9485\n",
      "Epoch 125/300\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 0.1663 - accuracy: 0.9449 - val_loss: 0.2206 - val_accuracy: 0.9500\n",
      "Epoch 126/300\n",
      "42000/42000 [==============================] - 11s 256us/step - loss: 0.1643 - accuracy: 0.9454 - val_loss: 0.2311 - val_accuracy: 0.9469\n",
      "Epoch 127/300\n",
      "42000/42000 [==============================] - 10s 249us/step - loss: 0.1782 - accuracy: 0.9400 - val_loss: 0.2261 - val_accuracy: 0.9468\n",
      "Epoch 128/300\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 0.1629 - accuracy: 0.9453 - val_loss: 0.2263 - val_accuracy: 0.9471\n",
      "Epoch 129/300\n",
      "42000/42000 [==============================] - 10s 247us/step - loss: 0.1667 - accuracy: 0.9442 - val_loss: 0.2182 - val_accuracy: 0.9501\n",
      "Epoch 130/300\n",
      "42000/42000 [==============================] - 11s 251us/step - loss: 0.1561 - accuracy: 0.9469 - val_loss: 0.2226 - val_accuracy: 0.9496\n",
      "Epoch 131/300\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 0.1618 - accuracy: 0.9443 - val_loss: 0.2176 - val_accuracy: 0.9513\n",
      "Epoch 132/300\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 0.1717 - accuracy: 0.9430 - val_loss: 0.2323 - val_accuracy: 0.9461\n",
      "Epoch 133/300\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 0.1563 - accuracy: 0.9476 - val_loss: 0.2326 - val_accuracy: 0.9467\n",
      "Epoch 134/300\n",
      "42000/42000 [==============================] - 10s 247us/step - loss: 0.1620 - accuracy: 0.9460 - val_loss: 0.2209 - val_accuracy: 0.9499\n",
      "Epoch 135/300\n",
      "42000/42000 [==============================] - 10s 236us/step - loss: 0.1586 - accuracy: 0.9457 - val_loss: 0.2171 - val_accuracy: 0.9503\n",
      "Epoch 136/300\n",
      "42000/42000 [==============================] - 10s 245us/step - loss: 0.1641 - accuracy: 0.9445 - val_loss: 0.2206 - val_accuracy: 0.9506\n",
      "Epoch 137/300\n",
      "42000/42000 [==============================] - 10s 248us/step - loss: 0.1590 - accuracy: 0.9466 - val_loss: 0.2304 - val_accuracy: 0.9460\n",
      "Epoch 138/300\n",
      "42000/42000 [==============================] - 10s 244us/step - loss: 0.1626 - accuracy: 0.9441 - val_loss: 0.2221 - val_accuracy: 0.9503\n",
      "Epoch 139/300\n",
      "42000/42000 [==============================] - 11s 257us/step - loss: 0.1529 - accuracy: 0.9482 - val_loss: 0.2175 - val_accuracy: 0.9522\n",
      "Epoch 140/300\n",
      "42000/42000 [==============================] - 11s 260us/step - loss: 0.1561 - accuracy: 0.9465 - val_loss: 0.2154 - val_accuracy: 0.9526\n",
      "Epoch 141/300\n",
      "42000/42000 [==============================] - 11s 262us/step - loss: 0.1534 - accuracy: 0.9488 - val_loss: 0.2153 - val_accuracy: 0.9521\n",
      "Epoch 142/300\n",
      "42000/42000 [==============================] - 11s 265us/step - loss: 0.1534 - accuracy: 0.9490 - val_loss: 0.2221 - val_accuracy: 0.9485\n",
      "Epoch 143/300\n",
      "42000/42000 [==============================] - 11s 264us/step - loss: 0.1649 - accuracy: 0.9442 - val_loss: 0.2155 - val_accuracy: 0.9518\n",
      "Epoch 144/300\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 0.1577 - accuracy: 0.9469 - val_loss: 0.2153 - val_accuracy: 0.9518\n",
      "Epoch 145/300\n",
      "42000/42000 [==============================] - 10s 244us/step - loss: 0.1558 - accuracy: 0.9478 - val_loss: 0.2210 - val_accuracy: 0.9510\n",
      "Epoch 146/300\n",
      "42000/42000 [==============================] - 10s 240us/step - loss: 0.1525 - accuracy: 0.9485 - val_loss: 0.2270 - val_accuracy: 0.9485\n",
      "Epoch 147/300\n",
      "42000/42000 [==============================] - 10s 241us/step - loss: 0.1536 - accuracy: 0.9492 - val_loss: 0.2337 - val_accuracy: 0.9475\n",
      "Epoch 148/300\n",
      "42000/42000 [==============================] - 10s 238us/step - loss: 0.1549 - accuracy: 0.9489 - val_loss: 0.2150 - val_accuracy: 0.9524\n",
      "Epoch 149/300\n",
      "42000/42000 [==============================] - 10s 237us/step - loss: 0.1539 - accuracy: 0.9480 - val_loss: 0.2107 - val_accuracy: 0.9536\n",
      "Epoch 150/300\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 0.1570 - accuracy: 0.9478 - val_loss: 0.2140 - val_accuracy: 0.9529\n",
      "Epoch 151/300\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 0.1522 - accuracy: 0.9494 - val_loss: 0.2141 - val_accuracy: 0.9532\n",
      "Epoch 152/300\n",
      "42000/42000 [==============================] - 11s 264us/step - loss: 0.1493 - accuracy: 0.9496 - val_loss: 0.2171 - val_accuracy: 0.9517\n",
      "Epoch 153/300\n",
      "42000/42000 [==============================] - 10s 243us/step - loss: 0.1508 - accuracy: 0.9485 - val_loss: 0.2187 - val_accuracy: 0.9514\n",
      "Epoch 154/300\n",
      "42000/42000 [==============================] - 10s 244us/step - loss: 0.1493 - accuracy: 0.9501 - val_loss: 0.2105 - val_accuracy: 0.9538\n",
      "Epoch 155/300\n",
      "42000/42000 [==============================] - 10s 243us/step - loss: 0.1456 - accuracy: 0.9504 - val_loss: 0.2132 - val_accuracy: 0.9533\n",
      "Epoch 156/300\n",
      "42000/42000 [==============================] - 10s 245us/step - loss: 0.1462 - accuracy: 0.9511 - val_loss: 0.2203 - val_accuracy: 0.9502\n",
      "Epoch 157/300\n",
      "42000/42000 [==============================] - 10s 247us/step - loss: 0.1453 - accuracy: 0.9507 - val_loss: 0.2104 - val_accuracy: 0.9540\n",
      "Epoch 158/300\n",
      "42000/42000 [==============================] - 12s 281us/step - loss: 0.1436 - accuracy: 0.9524 - val_loss: 0.2124 - val_accuracy: 0.9545\n",
      "Epoch 159/300\n",
      "42000/42000 [==============================] - 10s 248us/step - loss: 0.1520 - accuracy: 0.9482 - val_loss: 0.2214 - val_accuracy: 0.9521\n",
      "Epoch 160/300\n",
      "42000/42000 [==============================] - 11s 265us/step - loss: 0.1424 - accuracy: 0.9520 - val_loss: 0.2210 - val_accuracy: 0.9529\n",
      "Epoch 161/300\n",
      "42000/42000 [==============================] - 11s 257us/step - loss: 0.1442 - accuracy: 0.9516 - val_loss: 0.2153 - val_accuracy: 0.9532\n",
      "Epoch 162/300\n",
      "42000/42000 [==============================] - 10s 247us/step - loss: 0.1452 - accuracy: 0.9511 - val_loss: 0.2259 - val_accuracy: 0.9499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/300\n",
      "42000/42000 [==============================] - 10s 242us/step - loss: 0.1394 - accuracy: 0.9540 - val_loss: 0.2228 - val_accuracy: 0.9511\n",
      "Epoch 164/300\n",
      "42000/42000 [==============================] - 10s 243us/step - loss: 0.1421 - accuracy: 0.9505 - val_loss: 0.2240 - val_accuracy: 0.9499\n",
      "Epoch 165/300\n",
      "42000/42000 [==============================] - 10s 244us/step - loss: 0.1485 - accuracy: 0.9498 - val_loss: 0.2326 - val_accuracy: 0.9485\n",
      "Epoch 166/300\n",
      "42000/42000 [==============================] - 11s 251us/step - loss: 0.1423 - accuracy: 0.9532 - val_loss: 0.2189 - val_accuracy: 0.9516\n",
      "Epoch 167/300\n",
      "42000/42000 [==============================] - 11s 254us/step - loss: 0.1461 - accuracy: 0.9503 - val_loss: 0.2170 - val_accuracy: 0.9536\n",
      "Epoch 168/300\n",
      "42000/42000 [==============================] - 10s 244us/step - loss: 0.1455 - accuracy: 0.9509 - val_loss: 0.2181 - val_accuracy: 0.9521\n",
      "Epoch 169/300\n",
      "42000/42000 [==============================] - 11s 251us/step - loss: 0.1416 - accuracy: 0.9524 - val_loss: 0.2196 - val_accuracy: 0.9519\n",
      "Epoch 170/300\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 0.1432 - accuracy: 0.9525 - val_loss: 0.2122 - val_accuracy: 0.9548\n",
      "Epoch 171/300\n",
      "42000/42000 [==============================] - 10s 245us/step - loss: 0.1388 - accuracy: 0.9522 - val_loss: 0.2258 - val_accuracy: 0.9522\n",
      "Epoch 172/300\n",
      "42000/42000 [==============================] - 10s 244us/step - loss: 0.1436 - accuracy: 0.9520 - val_loss: 0.2228 - val_accuracy: 0.9520\n",
      "Epoch 173/300\n",
      "42000/42000 [==============================] - 10s 241us/step - loss: 0.1436 - accuracy: 0.9521 - val_loss: 0.2222 - val_accuracy: 0.9522\n",
      "Epoch 174/300\n",
      "42000/42000 [==============================] - 10s 245us/step - loss: 0.1374 - accuracy: 0.9534 - val_loss: 0.2200 - val_accuracy: 0.9528\n",
      "Epoch 175/300\n",
      "42000/42000 [==============================] - 10s 245us/step - loss: 0.1336 - accuracy: 0.9549 - val_loss: 0.2149 - val_accuracy: 0.9543\n",
      "Epoch 176/300\n",
      "42000/42000 [==============================] - 10s 244us/step - loss: 0.1396 - accuracy: 0.9539 - val_loss: 0.2157 - val_accuracy: 0.9536\n",
      "Epoch 177/300\n",
      "42000/42000 [==============================] - 10s 244us/step - loss: 0.1321 - accuracy: 0.9556 - val_loss: 0.2123 - val_accuracy: 0.9559\n",
      "Epoch 178/300\n",
      "42000/42000 [==============================] - 10s 239us/step - loss: 0.1331 - accuracy: 0.9554 - val_loss: 0.2177 - val_accuracy: 0.9549\n",
      "Epoch 179/300\n",
      "42000/42000 [==============================] - 10s 241us/step - loss: 0.1361 - accuracy: 0.9546 - val_loss: 0.2165 - val_accuracy: 0.9548\n",
      "Epoch 180/300\n",
      "42000/42000 [==============================] - 10s 243us/step - loss: 0.1359 - accuracy: 0.9538 - val_loss: 0.2188 - val_accuracy: 0.9547\n",
      "Epoch 181/300\n",
      "42000/42000 [==============================] - 10s 240us/step - loss: 0.1436 - accuracy: 0.9511 - val_loss: 0.2161 - val_accuracy: 0.9542\n",
      "Epoch 182/300\n",
      "42000/42000 [==============================] - 10s 242us/step - loss: 0.1327 - accuracy: 0.9552 - val_loss: 0.2197 - val_accuracy: 0.9538\n",
      "Epoch 183/300\n",
      "42000/42000 [==============================] - 10s 238us/step - loss: 0.1368 - accuracy: 0.9541 - val_loss: 0.2184 - val_accuracy: 0.9536\n",
      "Epoch 184/300\n",
      "42000/42000 [==============================] - 10s 240us/step - loss: 0.1338 - accuracy: 0.9554 - val_loss: 0.2126 - val_accuracy: 0.9559\n",
      "Epoch 185/300\n",
      "42000/42000 [==============================] - 10s 244us/step - loss: 0.1277 - accuracy: 0.9574 - val_loss: 0.2174 - val_accuracy: 0.9549\n",
      "Epoch 186/300\n",
      "42000/42000 [==============================] - 12s 289us/step - loss: 0.1384 - accuracy: 0.9528 - val_loss: 0.2276 - val_accuracy: 0.9507\n",
      "Epoch 187/300\n",
      "42000/42000 [==============================] - 12s 274us/step - loss: 0.1369 - accuracy: 0.9549 - val_loss: 0.2116 - val_accuracy: 0.9548\n",
      "Epoch 188/300\n",
      "42000/42000 [==============================] - 11s 263us/step - loss: 0.1316 - accuracy: 0.9561 - val_loss: 0.2135 - val_accuracy: 0.9556\n",
      "Epoch 189/300\n",
      "42000/42000 [==============================] - 11s 264us/step - loss: 0.1361 - accuracy: 0.9543 - val_loss: 0.2295 - val_accuracy: 0.9521\n",
      "Epoch 190/300\n",
      "42000/42000 [==============================] - 11s 256us/step - loss: 0.1329 - accuracy: 0.9547 - val_loss: 0.2180 - val_accuracy: 0.9548\n",
      "Epoch 191/300\n",
      "42000/42000 [==============================] - 11s 255us/step - loss: 0.1319 - accuracy: 0.9555 - val_loss: 0.2201 - val_accuracy: 0.9537\n",
      "Epoch 192/300\n",
      "42000/42000 [==============================] - 11s 256us/step - loss: 0.1314 - accuracy: 0.9562 - val_loss: 0.2120 - val_accuracy: 0.9569\n",
      "Epoch 193/300\n",
      "42000/42000 [==============================] - 11s 262us/step - loss: 0.1290 - accuracy: 0.9571 - val_loss: 0.2164 - val_accuracy: 0.9558\n",
      "Epoch 194/300\n",
      "42000/42000 [==============================] - 11s 264us/step - loss: 0.1344 - accuracy: 0.9555 - val_loss: 0.2165 - val_accuracy: 0.9546\n",
      "Epoch 195/300\n",
      "42000/42000 [==============================] - 11s 258us/step - loss: 0.1274 - accuracy: 0.9581 - val_loss: 0.2228 - val_accuracy: 0.9522\n",
      "Epoch 196/300\n",
      "42000/42000 [==============================] - 11s 262us/step - loss: 0.1338 - accuracy: 0.9546 - val_loss: 0.2141 - val_accuracy: 0.9556\n",
      "Epoch 197/300\n",
      "42000/42000 [==============================] - 11s 262us/step - loss: 0.1290 - accuracy: 0.9566 - val_loss: 0.2238 - val_accuracy: 0.9525\n",
      "Epoch 198/300\n",
      "42000/42000 [==============================] - 11s 269us/step - loss: 0.1273 - accuracy: 0.9584 - val_loss: 0.2210 - val_accuracy: 0.9531\n",
      "Epoch 199/300\n",
      "42000/42000 [==============================] - 11s 259us/step - loss: 0.1288 - accuracy: 0.9568 - val_loss: 0.2177 - val_accuracy: 0.9552\n",
      "Epoch 200/300\n",
      "42000/42000 [==============================] - 11s 261us/step - loss: 0.1258 - accuracy: 0.9579 - val_loss: 0.2182 - val_accuracy: 0.9540\n",
      "Epoch 201/300\n",
      "42000/42000 [==============================] - 11s 261us/step - loss: 0.1217 - accuracy: 0.9592 - val_loss: 0.2155 - val_accuracy: 0.9546\n",
      "Epoch 202/300\n",
      "42000/42000 [==============================] - 11s 254us/step - loss: 0.1226 - accuracy: 0.9591 - val_loss: 0.2159 - val_accuracy: 0.9554\n",
      "Epoch 203/300\n",
      "42000/42000 [==============================] - 11s 256us/step - loss: 0.1308 - accuracy: 0.9563 - val_loss: 0.2398 - val_accuracy: 0.9470\n",
      "Epoch 204/300\n",
      "42000/42000 [==============================] - 11s 257us/step - loss: 0.1327 - accuracy: 0.9561 - val_loss: 0.2262 - val_accuracy: 0.9534\n",
      "Epoch 205/300\n",
      "42000/42000 [==============================] - 11s 256us/step - loss: 0.1280 - accuracy: 0.9564 - val_loss: 0.2194 - val_accuracy: 0.9554\n",
      "Epoch 206/300\n",
      "42000/42000 [==============================] - 11s 265us/step - loss: 0.1276 - accuracy: 0.9576 - val_loss: 0.2238 - val_accuracy: 0.9522\n",
      "Epoch 207/300\n",
      "42000/42000 [==============================] - 12s 278us/step - loss: 0.1276 - accuracy: 0.9572 - val_loss: 0.2216 - val_accuracy: 0.9534\n",
      "Epoch 208/300\n",
      "42000/42000 [==============================] - 12s 284us/step - loss: 0.1244 - accuracy: 0.9591 - val_loss: 0.2128 - val_accuracy: 0.9561\n",
      "Epoch 209/300\n",
      "42000/42000 [==============================] - 12s 276us/step - loss: 0.1225 - accuracy: 0.9583 - val_loss: 0.2155 - val_accuracy: 0.9556\n",
      "Epoch 210/300\n",
      "42000/42000 [==============================] - 11s 266us/step - loss: 0.1250 - accuracy: 0.9570 - val_loss: 0.2153 - val_accuracy: 0.9554\n",
      "Epoch 211/300\n",
      "42000/42000 [==============================] - 11s 264us/step - loss: 0.1231 - accuracy: 0.9591 - val_loss: 0.2211 - val_accuracy: 0.9536\n",
      "Epoch 212/300\n",
      "42000/42000 [==============================] - 11s 262us/step - loss: 0.1223 - accuracy: 0.9583 - val_loss: 0.2195 - val_accuracy: 0.9535\n",
      "Epoch 213/300\n",
      "42000/42000 [==============================] - 11s 264us/step - loss: 0.1259 - accuracy: 0.9579 - val_loss: 0.2194 - val_accuracy: 0.9546\n",
      "Epoch 214/300\n",
      "42000/42000 [==============================] - 11s 262us/step - loss: 0.1259 - accuracy: 0.9582 - val_loss: 0.2145 - val_accuracy: 0.9561\n",
      "Epoch 215/300\n",
      "42000/42000 [==============================] - 11s 258us/step - loss: 0.1224 - accuracy: 0.9590 - val_loss: 0.2226 - val_accuracy: 0.9544\n",
      "Epoch 216/300\n",
      "42000/42000 [==============================] - 11s 253us/step - loss: 0.1198 - accuracy: 0.9590 - val_loss: 0.2195 - val_accuracy: 0.9560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/300\n",
      "42000/42000 [==============================] - 10s 250us/step - loss: 0.1269 - accuracy: 0.9578 - val_loss: 0.2154 - val_accuracy: 0.9557\n",
      "Epoch 218/300\n",
      "42000/42000 [==============================] - 10s 246us/step - loss: 0.1172 - accuracy: 0.9607 - val_loss: 0.2228 - val_accuracy: 0.9546\n",
      "Epoch 219/300\n",
      "42000/42000 [==============================] - 11s 254us/step - loss: 0.1281 - accuracy: 0.9577 - val_loss: 0.2168 - val_accuracy: 0.9548\n",
      "Epoch 220/300\n",
      "42000/42000 [==============================] - 11s 253us/step - loss: 0.1233 - accuracy: 0.9595 - val_loss: 0.2114 - val_accuracy: 0.9573\n",
      "Epoch 221/300\n",
      "42000/42000 [==============================] - 10s 249us/step - loss: 0.1189 - accuracy: 0.9601 - val_loss: 0.2136 - val_accuracy: 0.9561\n",
      "Epoch 222/300\n",
      "42000/42000 [==============================] - 11s 255us/step - loss: 0.1206 - accuracy: 0.9599 - val_loss: 0.2146 - val_accuracy: 0.9566\n",
      "Epoch 223/300\n",
      "42000/42000 [==============================] - 11s 250us/step - loss: 0.1269 - accuracy: 0.9577 - val_loss: 0.2149 - val_accuracy: 0.9560\n",
      "Epoch 224/300\n",
      "42000/42000 [==============================] - 10s 250us/step - loss: 0.1188 - accuracy: 0.9599 - val_loss: 0.2175 - val_accuracy: 0.9557\n",
      "Epoch 225/300\n",
      "42000/42000 [==============================] - 11s 254us/step - loss: 0.1220 - accuracy: 0.9595 - val_loss: 0.2194 - val_accuracy: 0.9553\n",
      "Epoch 226/300\n",
      "42000/42000 [==============================] - 11s 260us/step - loss: 0.1228 - accuracy: 0.9593 - val_loss: 0.2256 - val_accuracy: 0.9531\n",
      "Epoch 227/300\n",
      "42000/42000 [==============================] - 11s 264us/step - loss: 0.1227 - accuracy: 0.9595 - val_loss: 0.2118 - val_accuracy: 0.9568\n",
      "Epoch 228/300\n",
      "42000/42000 [==============================] - 11s 256us/step - loss: 0.1186 - accuracy: 0.9607 - val_loss: 0.2241 - val_accuracy: 0.9542\n",
      "Epoch 229/300\n",
      "42000/42000 [==============================] - 11s 262us/step - loss: 0.1203 - accuracy: 0.9594 - val_loss: 0.2164 - val_accuracy: 0.9562\n",
      "Epoch 230/300\n",
      "42000/42000 [==============================] - 11s 256us/step - loss: 0.1222 - accuracy: 0.9606 - val_loss: 0.2220 - val_accuracy: 0.9538\n",
      "Epoch 231/300\n",
      "42000/42000 [==============================] - 10s 250us/step - loss: 0.1237 - accuracy: 0.9583 - val_loss: 0.2233 - val_accuracy: 0.9531\n",
      "Epoch 232/300\n",
      "42000/42000 [==============================] - 11s 251us/step - loss: 0.1196 - accuracy: 0.9596 - val_loss: 0.2133 - val_accuracy: 0.9573\n",
      "Epoch 233/300\n",
      "42000/42000 [==============================] - 11s 255us/step - loss: 0.1152 - accuracy: 0.9606 - val_loss: 0.2204 - val_accuracy: 0.9552\n",
      "Epoch 234/300\n",
      "42000/42000 [==============================] - 11s 253us/step - loss: 0.1197 - accuracy: 0.9599 - val_loss: 0.2206 - val_accuracy: 0.9557\n",
      "Epoch 235/300\n",
      "42000/42000 [==============================] - 10s 247us/step - loss: 0.1184 - accuracy: 0.9617 - val_loss: 0.2179 - val_accuracy: 0.9568\n",
      "Epoch 236/300\n",
      "42000/42000 [==============================] - 11s 255us/step - loss: 0.1152 - accuracy: 0.9617 - val_loss: 0.2116 - val_accuracy: 0.9578\n",
      "Epoch 237/300\n",
      "42000/42000 [==============================] - 11s 256us/step - loss: 0.1146 - accuracy: 0.9613 - val_loss: 0.2157 - val_accuracy: 0.9567\n",
      "Epoch 238/300\n",
      "42000/42000 [==============================] - 11s 255us/step - loss: 0.1117 - accuracy: 0.9636 - val_loss: 0.2160 - val_accuracy: 0.9567\n",
      "Epoch 239/300\n",
      "42000/42000 [==============================] - 11s 258us/step - loss: 0.1167 - accuracy: 0.9611 - val_loss: 0.2241 - val_accuracy: 0.9549\n",
      "Epoch 240/300\n",
      "42000/42000 [==============================] - 11s 259us/step - loss: 0.1202 - accuracy: 0.9598 - val_loss: 0.2118 - val_accuracy: 0.9580\n",
      "Epoch 241/300\n",
      "42000/42000 [==============================] - 11s 257us/step - loss: 0.1194 - accuracy: 0.9609 - val_loss: 0.2160 - val_accuracy: 0.9569\n",
      "Epoch 242/300\n",
      "42000/42000 [==============================] - 11s 261us/step - loss: 0.1097 - accuracy: 0.9628 - val_loss: 0.2104 - val_accuracy: 0.9589\n",
      "Epoch 243/300\n",
      "42000/42000 [==============================] - 13s 314us/step - loss: 0.1082 - accuracy: 0.9653 - val_loss: 0.2216 - val_accuracy: 0.9554\n",
      "Epoch 244/300\n",
      "42000/42000 [==============================] - 12s 294us/step - loss: 0.1162 - accuracy: 0.9610 - val_loss: 0.2211 - val_accuracy: 0.9562\n",
      "Epoch 245/300\n",
      "42000/42000 [==============================] - 11s 258us/step - loss: 0.1091 - accuracy: 0.9638 - val_loss: 0.2125 - val_accuracy: 0.9581\n",
      "Epoch 246/300\n",
      "42000/42000 [==============================] - 11s 250us/step - loss: 0.1129 - accuracy: 0.9626 - val_loss: 0.2220 - val_accuracy: 0.9558\n",
      "Epoch 247/300\n",
      "42000/42000 [==============================] - 11s 257us/step - loss: 0.1142 - accuracy: 0.9629 - val_loss: 0.2262 - val_accuracy: 0.9547\n",
      "Epoch 248/300\n",
      "42000/42000 [==============================] - 11s 254us/step - loss: 0.1159 - accuracy: 0.9606 - val_loss: 0.2273 - val_accuracy: 0.9539\n",
      "Epoch 249/300\n",
      "42000/42000 [==============================] - 11s 259us/step - loss: 0.1151 - accuracy: 0.9609 - val_loss: 0.2143 - val_accuracy: 0.9576\n",
      "Epoch 250/300\n",
      "42000/42000 [==============================] - 11s 265us/step - loss: 0.1085 - accuracy: 0.9646 - val_loss: 0.2207 - val_accuracy: 0.9553\n",
      "Epoch 251/300\n",
      "42000/42000 [==============================] - 11s 259us/step - loss: 0.1117 - accuracy: 0.9625 - val_loss: 0.2139 - val_accuracy: 0.9569\n",
      "Epoch 252/300\n",
      "42000/42000 [==============================] - 10s 248us/step - loss: 0.1140 - accuracy: 0.9618 - val_loss: 0.2188 - val_accuracy: 0.9567\n",
      "Epoch 253/300\n",
      "42000/42000 [==============================] - 11s 251us/step - loss: 0.1126 - accuracy: 0.9620 - val_loss: 0.2220 - val_accuracy: 0.9556\n",
      "Epoch 254/300\n",
      "42000/42000 [==============================] - 11s 255us/step - loss: 0.1099 - accuracy: 0.9627 - val_loss: 0.2153 - val_accuracy: 0.9567\n",
      "Epoch 255/300\n",
      "42000/42000 [==============================] - 11s 263us/step - loss: 0.1108 - accuracy: 0.9636 - val_loss: 0.2218 - val_accuracy: 0.9564\n",
      "Epoch 256/300\n",
      "42000/42000 [==============================] - 11s 257us/step - loss: 0.1098 - accuracy: 0.9634 - val_loss: 0.2218 - val_accuracy: 0.9573\n",
      "Epoch 257/300\n",
      "42000/42000 [==============================] - 11s 259us/step - loss: 0.1105 - accuracy: 0.9632 - val_loss: 0.2196 - val_accuracy: 0.9573\n",
      "Epoch 258/300\n",
      "42000/42000 [==============================] - 11s 256us/step - loss: 0.1062 - accuracy: 0.9645 - val_loss: 0.2213 - val_accuracy: 0.9563\n",
      "Epoch 259/300\n",
      "42000/42000 [==============================] - 11s 259us/step - loss: 0.1121 - accuracy: 0.9629 - val_loss: 0.2269 - val_accuracy: 0.9560\n",
      "Epoch 260/300\n",
      "42000/42000 [==============================] - 11s 258us/step - loss: 0.1197 - accuracy: 0.9609 - val_loss: 0.2221 - val_accuracy: 0.9560\n",
      "Epoch 261/300\n",
      "42000/42000 [==============================] - 11s 267us/step - loss: 0.1109 - accuracy: 0.9632 - val_loss: 0.2210 - val_accuracy: 0.9566\n",
      "Epoch 262/300\n",
      "42000/42000 [==============================] - 11s 261us/step - loss: 0.1182 - accuracy: 0.9604 - val_loss: 0.2259 - val_accuracy: 0.9550\n",
      "Epoch 263/300\n",
      "42000/42000 [==============================] - 11s 258us/step - loss: 0.1069 - accuracy: 0.9643 - val_loss: 0.2183 - val_accuracy: 0.9577\n",
      "Epoch 264/300\n",
      "42000/42000 [==============================] - 11s 270us/step - loss: 0.1095 - accuracy: 0.9634 - val_loss: 0.2250 - val_accuracy: 0.9566\n",
      "Epoch 265/300\n",
      "42000/42000 [==============================] - 11s 263us/step - loss: 0.1128 - accuracy: 0.9618 - val_loss: 0.2217 - val_accuracy: 0.9568\n",
      "Epoch 266/300\n",
      "42000/42000 [==============================] - 11s 268us/step - loss: 0.1107 - accuracy: 0.9634 - val_loss: 0.2202 - val_accuracy: 0.9579\n",
      "Epoch 267/300\n",
      "42000/42000 [==============================] - 11s 259us/step - loss: 0.1070 - accuracy: 0.9648 - val_loss: 0.2192 - val_accuracy: 0.9564\n",
      "Epoch 268/300\n",
      "42000/42000 [==============================] - 11s 251us/step - loss: 0.1108 - accuracy: 0.9633 - val_loss: 0.2227 - val_accuracy: 0.9563\n",
      "Epoch 269/300\n",
      "42000/42000 [==============================] - 11s 259us/step - loss: 0.1079 - accuracy: 0.9637 - val_loss: 0.2280 - val_accuracy: 0.9560\n",
      "Epoch 270/300\n",
      "42000/42000 [==============================] - 11s 259us/step - loss: 0.1115 - accuracy: 0.9626 - val_loss: 0.2170 - val_accuracy: 0.9575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 271/300\n",
      "42000/42000 [==============================] - 11s 265us/step - loss: 0.1091 - accuracy: 0.9634 - val_loss: 0.2159 - val_accuracy: 0.9581\n",
      "Epoch 272/300\n",
      "42000/42000 [==============================] - 11s 257us/step - loss: 0.1079 - accuracy: 0.9642 - val_loss: 0.2162 - val_accuracy: 0.9580\n",
      "Epoch 273/300\n",
      "42000/42000 [==============================] - 11s 253us/step - loss: 0.1086 - accuracy: 0.9634 - val_loss: 0.2233 - val_accuracy: 0.9556\n",
      "Epoch 274/300\n",
      "42000/42000 [==============================] - 10s 249us/step - loss: 0.1067 - accuracy: 0.9634 - val_loss: 0.2185 - val_accuracy: 0.9581\n",
      "Epoch 275/300\n",
      "42000/42000 [==============================] - 11s 255us/step - loss: 0.1113 - accuracy: 0.9634 - val_loss: 0.2197 - val_accuracy: 0.9577\n",
      "Epoch 276/300\n",
      "42000/42000 [==============================] - 11s 254us/step - loss: 0.1098 - accuracy: 0.9631 - val_loss: 0.2212 - val_accuracy: 0.9564\n",
      "Epoch 277/300\n",
      "42000/42000 [==============================] - 10s 250us/step - loss: 0.1101 - accuracy: 0.9634 - val_loss: 0.2206 - val_accuracy: 0.9566\n",
      "Epoch 278/300\n",
      "42000/42000 [==============================] - 11s 252us/step - loss: 0.1073 - accuracy: 0.9636 - val_loss: 0.2204 - val_accuracy: 0.9573\n",
      "Epoch 279/300\n",
      "42000/42000 [==============================] - 11s 251us/step - loss: 0.1023 - accuracy: 0.9656 - val_loss: 0.2161 - val_accuracy: 0.9582\n",
      "Epoch 280/300\n",
      "42000/42000 [==============================] - 11s 255us/step - loss: 0.1052 - accuracy: 0.9652 - val_loss: 0.2163 - val_accuracy: 0.9579\n",
      "Epoch 281/300\n",
      "42000/42000 [==============================] - 11s 254us/step - loss: 0.1059 - accuracy: 0.9648 - val_loss: 0.2238 - val_accuracy: 0.9564\n",
      "Epoch 282/300\n",
      "42000/42000 [==============================] - 11s 254us/step - loss: 0.1007 - accuracy: 0.9667 - val_loss: 0.2189 - val_accuracy: 0.9582\n",
      "Epoch 283/300\n",
      "42000/42000 [==============================] - 11s 257us/step - loss: 0.1094 - accuracy: 0.9631 - val_loss: 0.2211 - val_accuracy: 0.9579\n",
      "Epoch 284/300\n",
      "42000/42000 [==============================] - 11s 254us/step - loss: 0.1115 - accuracy: 0.9632 - val_loss: 0.2241 - val_accuracy: 0.9571\n",
      "Epoch 285/300\n",
      "42000/42000 [==============================] - 11s 257us/step - loss: 0.1088 - accuracy: 0.9634 - val_loss: 0.2225 - val_accuracy: 0.9566\n",
      "Epoch 286/300\n",
      "42000/42000 [==============================] - 11s 260us/step - loss: 0.1058 - accuracy: 0.9654 - val_loss: 0.2205 - val_accuracy: 0.9576\n",
      "Epoch 287/300\n",
      "42000/42000 [==============================] - 11s 257us/step - loss: 0.1037 - accuracy: 0.9648 - val_loss: 0.2225 - val_accuracy: 0.9565\n",
      "Epoch 288/300\n",
      "42000/42000 [==============================] - 11s 258us/step - loss: 0.1077 - accuracy: 0.9646 - val_loss: 0.2169 - val_accuracy: 0.9591\n",
      "Epoch 289/300\n",
      "42000/42000 [==============================] - 11s 262us/step - loss: 0.1095 - accuracy: 0.9640 - val_loss: 0.2167 - val_accuracy: 0.9576\n",
      "Epoch 290/300\n",
      "42000/42000 [==============================] - 11s 255us/step - loss: 0.1061 - accuracy: 0.9648 - val_loss: 0.2177 - val_accuracy: 0.9582\n",
      "Epoch 291/300\n",
      "42000/42000 [==============================] - 11s 272us/step - loss: 0.1047 - accuracy: 0.9652 - val_loss: 0.2205 - val_accuracy: 0.9579\n",
      "Epoch 292/300\n",
      "42000/42000 [==============================] - 11s 268us/step - loss: 0.1008 - accuracy: 0.9665 - val_loss: 0.2193 - val_accuracy: 0.9583\n",
      "Epoch 293/300\n",
      "42000/42000 [==============================] - 11s 261us/step - loss: 0.1032 - accuracy: 0.9660 - val_loss: 0.2203 - val_accuracy: 0.9582\n",
      "Epoch 294/300\n",
      "42000/42000 [==============================] - 11s 258us/step - loss: 0.1090 - accuracy: 0.9642 - val_loss: 0.2203 - val_accuracy: 0.9571\n",
      "Epoch 295/300\n",
      "42000/42000 [==============================] - 11s 263us/step - loss: 0.1090 - accuracy: 0.9636 - val_loss: 0.2264 - val_accuracy: 0.9556\n",
      "Epoch 296/300\n",
      "42000/42000 [==============================] - 11s 255us/step - loss: 0.1003 - accuracy: 0.9661 - val_loss: 0.2187 - val_accuracy: 0.9587\n",
      "Epoch 297/300\n",
      "42000/42000 [==============================] - 10s 249us/step - loss: 0.1025 - accuracy: 0.9658 - val_loss: 0.2202 - val_accuracy: 0.9572\n",
      "Epoch 298/300\n",
      "42000/42000 [==============================] - 11s 261us/step - loss: 0.1003 - accuracy: 0.9663 - val_loss: 0.2260 - val_accuracy: 0.9575\n",
      "Epoch 299/300\n",
      "42000/42000 [==============================] - 11s 256us/step - loss: 0.1009 - accuracy: 0.9670 - val_loss: 0.2243 - val_accuracy: 0.9577\n",
      "Epoch 300/300\n",
      "42000/42000 [==============================] - 11s 253us/step - loss: 0.1019 - accuracy: 0.9656 - val_loss: 0.2179 - val_accuracy: 0.9594\n"
     ]
    }
   ],
   "source": [
    "# now fit the model\n",
    "model_regularized_1_result=model_regularized_1.fit(X_train_new, y_train_new,           \n",
    "          validation_data=(X_val_new,y_val_new),\n",
    "          epochs=300,batch_size=500,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that Training Accuracy is 96% and Validation Accuracy is 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets apply these models on Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "18000/18000 [==============================] - 1s 63us/step\n",
      "test acc: 0.8190555572509766\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model  model2_keras\n",
    "\n",
    "evaluateModel(model2_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy of 81% using model model2_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "18000/18000 [==============================] - 1s 44us/step\n",
      "test acc: 0.5775555372238159\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model  model9_keras\n",
    "\n",
    "evaluateModel(model9_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy of 57% using model model9_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "18000/18000 [==============================] - 1s 58us/step\n",
      "test acc: 0.8358333110809326\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model  model_regularized ( Batch normalization)\n",
    "\n",
    "evaluateModel(model_regularized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Accuracy of 83% using model model_regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Evaluate on test data\n",
      "18000/18000 [==============================] - 1s 56us/step\n",
      "test acc: 0.8716111183166504\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model  model_regularized ( Batch normalization+ Dropout)\n",
    "\n",
    "evaluateModel(model_regularized_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Accuracy of 87% using model model_regularized_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So the best model is model_regularized_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict the output\n",
    "\n",
    "y_pred=model_regularized_1.predict(X_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets get the max probality of the output\n",
    "y_pred_new = np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.8716111111111111\n",
      "Confustion Matrix \n",
      " \n",
      "[[1631   27   13   19   16    8   21   14   21   44]\n",
      " [  20 1630   17   34   39   16   17   31   13   11]\n",
      " [   7   31 1595   41   20   12    6   37   22   32]\n",
      " [  11   24   26 1470   11   76   19   24   35   23]\n",
      " [  14   52   16   27 1619   12   20   12   15   25]\n",
      " [  10   12    8   89    8 1511   56    5   41   28]\n",
      " [  36   17   16   24   29   61 1560   12   66   11]\n",
      " [  10   66   43   23   14   14    7 1607    8   16]\n",
      " [  21   30   20   46   14   35   61    9 1525   51]\n",
      " [  42   28   23   47   18   36   10   16   43 1541]]\n",
      "classification Report \n",
      " \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.90      1814\n",
      "           1       0.85      0.89      0.87      1828\n",
      "           2       0.90      0.88      0.89      1803\n",
      "           3       0.81      0.86      0.83      1719\n",
      "           4       0.91      0.89      0.90      1812\n",
      "           5       0.85      0.85      0.85      1768\n",
      "           6       0.88      0.85      0.86      1832\n",
      "           7       0.91      0.89      0.90      1808\n",
      "           8       0.85      0.84      0.85      1812\n",
      "           9       0.86      0.85      0.86      1804\n",
      "\n",
      "    accuracy                           0.87     18000\n",
      "   macro avg       0.87      0.87      0.87     18000\n",
      "weighted avg       0.87      0.87      0.87     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lets print the metric and report\n",
    "\n",
    "print('Accuracy score %s'%(metrics.accuracy_score(y_test,y_pred_new)))\n",
    "print('Confustion Matrix \\n \\n%s'%(confusion_matrix(y_test,y_pred_new)))\n",
    "print('classification Report \\n \\n%s'%(classification_report(y_test,y_pred_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from the classification report we can see that final model as performed good in predicting correct class as correct and incorrect class as incorrect with percentage of 87\n",
    "- from precision we can see that it has performed good in predicting numbers 0,2,4,7 as correct values with more than 90% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So the best model that returing highest accuracy is 87% ( model with regularization techniqes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Load , understand and process the DataSet\n",
    "> - Dataset was available in .h5 file format and we had to use the h5py library to load the file\n",
    "> - On extracting the data from file we found out that the data was divided into 3 sets , Trainset, ValSet and TestSet (with its corresponding X and Y feature)\n",
    "> - We also noticed that each feature dataset was 3-D data ex : X_train was having shape (42000, 32, 32) and target dataset of 1-D :ex y_train (42000,)\n",
    "> - As part of Data Preprocessing we first converted all the list of dataset to numpyArray\n",
    "> - Since we had 3-D data we rehsaped them to 2d like #Reshaping the data ( n * 32 * 32) to (n * 1024) as all model accept 2-D data\n",
    "> - target variables y_* holds the classification value like values ranging from 0 to 9, we had to convert them to binary values, we used keras to_categorical to convert the same  ex : y_train (42000,) to (42000,10)\n",
    "> - When then noramlized the data using StandardScalar to make sure all the dataset is in same scale\n",
    "\n",
    "- K-Nearest Neighbor(KNN) classifier Apporach\n",
    "\n",
    "> - With KNN aproach we noticed that validatoin accuracy was not that great, we got around 57% ( executed on google colab, it took close to 14 hours)\n",
    "\n",
    "- Deep Learning Technique ( Neural Network Classifier)\n",
    "> - We used Keras to build the network, started by importing all the required libraries\n",
    "> - Since our target variable was multiclass classification we used softmax activation on output layer\n",
    "> - We first built multiple network using training set and validated it on validation set, however execution on Testset is done at the end\n",
    "> - We build multiple Netowrks ( used 10 Epochs and batch_size 50)\n",
    "    - Basic Neural Network -1 , With this Training Accuracy is 91% and Validation Accuracy is 88% \n",
    "        - Hidden node -1\n",
    "        - Activation funtion - Relu\n",
    "        - kernelinitializer - uniform\n",
    "        - Optimizer - Adam\n",
    "    - Neural Network -2 , With this Training Accuracy is 91% and Validation Accuracy is 89%\n",
    "        - Hidden node -2\n",
    "        - Activation funtion - Relu\n",
    "        - kernelinitializer - uniform\n",
    "        - Optimizer - Adam , with learning rate 0.001\n",
    "    - Neural Network -3 , With this Training Accuracy is 10% and Validation Accuracy is 10%\n",
    "        - Hidden node -2\n",
    "        - Activation funtion - Relu\n",
    "        - kernelinitializer - uniform\n",
    "        - Optimizer - Adam ,with learning rate 0.01\n",
    "    - Neural Network -4 , With this Training Accuracy is 89% and Validation Accuracy is 87%\n",
    "        - Hidden node -2\n",
    "        - Activation funtion - Relu\n",
    "        - kernelinitializer - uniform\n",
    "        - Optimizer - SGD with learning rate 0.01\n",
    "    - Neural Network -5 , With this Training Accuracy is 63% and Validation Accuracy is 63%\n",
    "        - Hidden node -2\n",
    "        - Activation funtion - Relu\n",
    "        - kernelinitializer - uniform\n",
    "        - Optimizer - SGD with learning rate 0.001\n",
    "    - Neural Network -6 , With this Training Accuracy is 90% and Validation Accuracy is 89%\n",
    "        - Hidden node -2\n",
    "        - Activation funtion - glorot_uniform ( Xavier)\n",
    "        - kernelinitializer - uniform\n",
    "        - Optimizer - SGD with learning rate 0.01\n",
    "    - Neural Network -7 , With this Training Accuracy is 87% and Validation Accuracy is 75%\n",
    "        - Hidden node -2\n",
    "        - Activation funtion - glorot_uniform ( Xavier)\n",
    "        - kernelinitializer - uniform\n",
    "        - Optimizer - SGD with learning rate 0.001\n",
    "    - Neural Network -8 , With this Training Accuracy is 91% and Validation Accuracy is 90%\n",
    "        - Hidden node -2\n",
    "        - Activation funtion - relu\n",
    "        - kernelinitializer - he_uniform\n",
    "        - Optimizer - SGD with learning rate 0.01\n",
    "    - Neural Network -9 , With this Training Accuracy is 55% and Validation Accuracy is 57%\n",
    "        - Hidden node -2\n",
    "        - Activation funtion - sigmoid\n",
    "        - kernelinitializer - he_uniform\n",
    "        - Optimizer - Adam with learning rate 0.01\n",
    "> - We noticed that  Neural Network -2 with Neural Network -9 were best among the onces we build with Train/Val accuracy close to 90%\n",
    "\n",
    "- Improving the performance\n",
    "> - we then used GridSearchCV to find the optimal values for batch_size and epochs for better accuracy, as per it we found best values as ----> {'batch_size': 500, 'epochs': 300}\n",
    "> - We use the models Neural Network -2 with Neural Network -9 and improve them to get better accuracy\n",
    "> - Stochastic gradient, cross-entropy\n",
    "> - We used regularization techniques to improve out training model ( so that model is not overfit) and performs better on validation/test model\n",
    "> - Then we applyed batch normalizatoin on each of the identified Neural network , this will help to reduce vanishing gradient problem and also control the jumping of weight parameters ( this will also make sure training model is not overfit), with this we got Training Accuracy as 98% and Validation Accuracy as 94%\n",
    "> - Then used the dropout to see if this improves the accuracy.\n",
    "> - After all this we got a accuracy of 96% on training and 95% validation dataset\n",
    "\n",
    "- Applying model on Test DataSet to find how it behaves :)\n",
    "> - On applying the good models on test data set we found out below results\n",
    "> - Test Accuracy of 81% using model model2_keras\n",
    "> - Test Accuracy of 57% using model model9_keras\n",
    "> - Test Accuracy of 83% using model model_regularized\n",
    "> - Test Accuracy of 87% using model model_regularized_1\n",
    "> - So the best model that giving accuracy of 87% is model_regularized_1 where we have adam optimizer, BatchNormalization and dropbout\n",
    "\n",
    "- Final Statement\n",
    "> - On going through different models we noticed that the Neural network does better job in predicting the results whihc gave us 87% accuracy on test dataset where as KNN gave us around 50% accuracy\n",
    "\n",
    "- NOTE: KNN model executed from google colab and screenshots are attached"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
